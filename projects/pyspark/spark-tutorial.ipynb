{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Spark Tutorial](https://www.youtube.com/watch?v=5RosqOeJrrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# `local[*]` means use all available cores on the local machine.\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"spark-intro\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://3e6699c11a7b:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark-intro</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fc63f7ecc50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the number of driver cores.\n",
    "\n",
    "spark.sparkContext.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|employee_id|department_id|         name|age|gender|salary|          hire_date|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01 00:00:00|\n",
      "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15 00:00:00|\n",
      "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01 00:00:00|\n",
      "|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30 00:00:00|\n",
      "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01 00:00:00|\n",
      "|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01 00:00:00|\n",
      "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15 00:00:00|\n",
      "|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01 00:00:00|\n",
      "|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01 00:00:00|\n",
      "|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01 00:00:00|\n",
      "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01 00:00:00|\n",
      "|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15 00:00:00|\n",
      "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01 00:00:00|\n",
      "|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01 00:00:00|\n",
      "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30 00:00:00|\n",
      "|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01 00:00:00|\n",
      "|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15 00:00:00|\n",
      "|         18|          104|    Nancy Liu| 29|Female| 50000|2017-06-01 00:00:00|\n",
      "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01 00:00:00|\n",
      "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01 00:00:00|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "\n",
      "Number of rows in the Employees DataFrame: 20\n",
      "Number of partitions in the Employees DataFrame: 1\n"
     ]
    }
   ],
   "source": [
    "# Read csv into dataframe\n",
    "\n",
    "emp = spark.read.csv('data/emp.csv', header=True, inferSchema=True)\n",
    "emp.show()\n",
    "\n",
    "print(f\"Number of rows in the Employees DataFrame: {emp.count()}\")\n",
    "print(f\"Number of partitions in the Employees DataFrame: {emp.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- department_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- hire_date: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema of the dataframe\n",
    "\n",
    "emp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Increase the number of partitions to 10\n",
    "# (use `coalesce()` to decrease the number of partitions)\n",
    "\n",
    "emp_re = emp.repartition(10)\n",
    "emp_re.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|employee_id|department_id|         name|age|gender|salary|          hire_date|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15 00:00:00|\n",
      "|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15 00:00:00|\n",
      "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01 00:00:00|\n",
      "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01 00:00:00|\n",
      "|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01 00:00:00|\n",
      "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30 00:00:00|\n",
      "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01 00:00:00|\n",
      "|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01 00:00:00|\n",
      "|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15 00:00:00|\n",
      "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01 00:00:00|\n",
      "|         18|          104|    Nancy Liu| 29|Female| 50000|2017-06-01 00:00:00|\n",
      "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15 00:00:00|\n",
      "|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01 00:00:00|\n",
      "|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01 00:00:00|\n",
      "|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01 00:00:00|\n",
      "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01 00:00:00|\n",
      "|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01 00:00:00|\n",
      "|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01 00:00:00|\n",
      "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01 00:00:00|\n",
      "|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30 00:00:00|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "\n",
      "Number of rows in the Employees (Repartitioned) DataFrame: 20\n",
      "Number of partitions in the Employees (Repartitioned) DataFrame: 10\n"
     ]
    }
   ],
   "source": [
    "emp_re.show()\n",
    "\n",
    "print(f\"Number of rows in the Employees (Repartitioned) DataFrame: {emp_re.count()}\")\n",
    "print(f\"Number of partitions in the Employees (Repartitioned) DataFrame: {emp_re.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the Cities-DataFrame: 2349391\n",
      "Number of partitions in the Cities-DataFrame: 19\n"
     ]
    }
   ],
   "source": [
    "# Load another (much larger) csv into a new dataframe and check its properties.\n",
    "\n",
    "cities = spark.read.csv('data/cities.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Print the rows and partitions of the dataframe\n",
    "print(f\"Number of rows in the Cities-DataFrame: {cities.count()}\")\n",
    "print(f\"Number of partitions in the Cities-DataFrame: {cities.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Transformations 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('employee_id', IntegerType(), True), StructField('department_id', IntegerType(), True), StructField('name', StringType(), True), StructField('age', IntegerType(), True), StructField('gender', StringType(), True), StructField('salary', IntegerType(), True), StructField('hire_date', TimestampType(), True)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|employee_id|department_id|         name|age|gender|salary|          hire_date|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01 00:00:00|\n",
      "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15 00:00:00|\n",
      "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01 00:00:00|\n",
      "|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30 00:00:00|\n",
      "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01 00:00:00|\n",
      "|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01 00:00:00|\n",
      "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15 00:00:00|\n",
      "|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01 00:00:00|\n",
      "|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01 00:00:00|\n",
      "|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01 00:00:00|\n",
      "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01 00:00:00|\n",
      "|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15 00:00:00|\n",
      "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01 00:00:00|\n",
      "|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01 00:00:00|\n",
      "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30 00:00:00|\n",
      "|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01 00:00:00|\n",
      "|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15 00:00:00|\n",
      "|         18|          104|    Nancy Liu| 29|Female| 50000|2017-06-01 00:00:00|\n",
      "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01 00:00:00|\n",
      "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01 00:00:00|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('name', StringType(), True), StructField('age', IntegerType(), True)])\n",
      "StructType([StructField('name', StringType(), True), StructField('age', IntegerType(), True)])\n"
     ]
    }
   ],
   "source": [
    "# Creating a manual schema in Spark\n",
    "from pyspark.sql.types import _parse_datatype_string\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# ! IMPLICT INFERENCE\n",
    "# Spark can infer the schema from a string\n",
    "schema_string = \"name string, age int\"\n",
    "print(_parse_datatype_string(schema_string))\n",
    "\n",
    "# ! EXPLICIT INFERENCE\n",
    "# Template: StructType([StructField(name, dataType, nullable?)]) \n",
    "schema_spark = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "print(schema_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---+------+\n",
      "|employee_id|         name|age|salary|\n",
      "+-----------+-------------+---+------+\n",
      "|          1|     John Doe| 30| 50000|\n",
      "|          2|   Jane Smith| 25| 45000|\n",
      "|          3|    Bob Brown| 35| 55000|\n",
      "|          4|    Alice Lee| 28| 48000|\n",
      "|          5|    Jack Chan| 40| 60000|\n",
      "|          6|    Jill Wong| 32| 52000|\n",
      "|          7|James Johnson| 42| 70000|\n",
      "|          8|     Kate Kim| 29| 51000|\n",
      "|          9|      Tom Tan| 33| 58000|\n",
      "|         10|     Lisa Lee| 27| 47000|\n",
      "|         11|   David Park| 38| 65000|\n",
      "|         12|   Susan Chen| 31| 54000|\n",
      "|         13|    Brian Kim| 45| 75000|\n",
      "|         14|    Emily Lee| 26| 46000|\n",
      "|         15|  Michael Lee| 37| 63000|\n",
      "|         16|  Kelly Zhang| 30| 49000|\n",
      "|         17|  George Wang| 34| 57000|\n",
      "|         18|    Nancy Liu| 29| 50000|\n",
      "|         19|  Steven Chen| 36| 62000|\n",
      "|         20|    Grace Kim| 32| 53000|\n",
      "+-----------+-------------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Columns and Expressions\n",
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "# ? col(\"name\") == expr(\"name\"), since both are Column objects and hence treated as same.\n",
    "# select employee_id, name, age, salary from emp\n",
    "\n",
    "emp_filtered = emp.select(col(\"employee_id\"), expr(\"name\"), emp.age, emp.salary)    # ! TRANSFORMATION\n",
    "emp_filtered.show() # ! ACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+---+------+\n",
      "|emp_id|         name|age|salary|\n",
      "+------+-------------+---+------+\n",
      "|     1|     John Doe| 30| 50000|\n",
      "|     2|   Jane Smith| 25| 45000|\n",
      "|     3|    Bob Brown| 35| 55000|\n",
      "|     4|    Alice Lee| 28| 48000|\n",
      "|     5|    Jack Chan| 40| 60000|\n",
      "|     6|    Jill Wong| 32| 52000|\n",
      "|     7|James Johnson| 42| 70000|\n",
      "|     8|     Kate Kim| 29| 51000|\n",
      "|     9|      Tom Tan| 33| 58000|\n",
      "|    10|     Lisa Lee| 27| 47000|\n",
      "|    11|   David Park| 38| 65000|\n",
      "|    12|   Susan Chen| 31| 54000|\n",
      "|    13|    Brian Kim| 45| 75000|\n",
      "|    14|    Emily Lee| 26| 46000|\n",
      "|    15|  Michael Lee| 37| 63000|\n",
      "|    16|  Kelly Zhang| 30| 49000|\n",
      "|    17|  George Wang| 34| 57000|\n",
      "|    18|    Nancy Liu| 29| 50000|\n",
      "|    19|  Steven Chen| 36| 62000|\n",
      "|    20|    Grace Kim| 32| 53000|\n",
      "+------+-------------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_casted = emp_filtered.select(expr(\"employee_id as emp_id\"), emp_filtered.name, expr(\"cast(age as int) as age\"), emp_filtered.salary)\n",
    "emp_casted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_casted.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+---+------+\n",
      "|emp_id|         name|age|salary|\n",
      "+------+-------------+---+------+\n",
      "|     1|     John Doe| 30| 50000|\n",
      "|     2|   Jane Smith| 25| 45000|\n",
      "|     3|    Bob Brown| 35| 55000|\n",
      "|     4|    Alice Lee| 28| 48000|\n",
      "|     5|    Jack Chan| 40| 60000|\n",
      "|     6|    Jill Wong| 32| 52000|\n",
      "|     7|James Johnson| 42| 70000|\n",
      "|     8|     Kate Kim| 29| 51000|\n",
      "|     9|      Tom Tan| 33| 58000|\n",
      "|    10|     Lisa Lee| 27| 47000|\n",
      "|    11|   David Park| 38| 65000|\n",
      "|    12|   Susan Chen| 31| 54000|\n",
      "|    13|    Brian Kim| 45| 75000|\n",
      "|    14|    Emily Lee| 26| 46000|\n",
      "|    15|  Michael Lee| 37| 63000|\n",
      "|    16|  Kelly Zhang| 30| 49000|\n",
      "|    17|  George Wang| 34| 57000|\n",
      "|    18|    Nancy Liu| 29| 50000|\n",
      "|    19|  Steven Chen| 36| 62000|\n",
      "|    20|    Grace Kim| 32| 53000|\n",
      "+------+-------------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_casted_alt = emp_filtered.selectExpr(\"employee_id as emp_id\", \"name\", \"cast(age as int) as age\", \"salary\")\n",
    "emp_casted_alt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+---+------+\n",
      "|emp_id|         name|age|salary|\n",
      "+------+-------------+---+------+\n",
      "|     3|    Bob Brown| 35| 55000|\n",
      "|     5|    Jack Chan| 40| 60000|\n",
      "|     6|    Jill Wong| 32| 52000|\n",
      "|     7|James Johnson| 42| 70000|\n",
      "|     9|      Tom Tan| 33| 58000|\n",
      "|    11|   David Park| 38| 65000|\n",
      "|    12|   Susan Chen| 31| 54000|\n",
      "|    13|    Brian Kim| 45| 75000|\n",
      "|    15|  Michael Lee| 37| 63000|\n",
      "|    17|  George Wang| 34| 57000|\n",
      "|    19|  Steven Chen| 36| 62000|\n",
      "|    20|    Grace Kim| 32| 53000|\n",
      "+------+-------------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter emp_casted based on Age > 30\n",
    "\n",
    "emp_casted.select(\"emp_id\", \"name\", \"age\", \"salary\").where(\"age > 30\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Transformations 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|employee_id|department_id|         name|age|gender|salary|          hire_date|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01 00:00:00|\n",
      "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15 00:00:00|\n",
      "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01 00:00:00|\n",
      "|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30 00:00:00|\n",
      "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01 00:00:00|\n",
      "|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01 00:00:00|\n",
      "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15 00:00:00|\n",
      "|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01 00:00:00|\n",
      "|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01 00:00:00|\n",
      "|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01 00:00:00|\n",
      "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01 00:00:00|\n",
      "|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15 00:00:00|\n",
      "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01 00:00:00|\n",
      "|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01 00:00:00|\n",
      "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30 00:00:00|\n",
      "|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01 00:00:00|\n",
      "|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15 00:00:00|\n",
      "|         18|          104|    Nancy Liu| 29|Female| 50000|2017-06-01 00:00:00|\n",
      "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01 00:00:00|\n",
      "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01 00:00:00|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- department_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- hire_date: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, cast\n",
    "\n",
    "emp.select(\"employee_id\", \"name\", \"age\", col(\"salary\").cast(\"double\")).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---+-------+-------+\n",
      "|employee_id|         name|age| salary|    tax|\n",
      "+-----------+-------------+---+-------+-------+\n",
      "|          1|     John Doe| 30|50000.0|10000.0|\n",
      "|          2|   Jane Smith| 25|45000.0| 9000.0|\n",
      "|          3|    Bob Brown| 35|55000.0|11000.0|\n",
      "|          4|    Alice Lee| 28|48000.0| 9600.0|\n",
      "|          5|    Jack Chan| 40|60000.0|12000.0|\n",
      "|          6|    Jill Wong| 32|52000.0|10400.0|\n",
      "|          7|James Johnson| 42|70000.0|14000.0|\n",
      "|          8|     Kate Kim| 29|51000.0|10200.0|\n",
      "|          9|      Tom Tan| 33|58000.0|11600.0|\n",
      "|         10|     Lisa Lee| 27|47000.0| 9400.0|\n",
      "|         11|   David Park| 38|65000.0|13000.0|\n",
      "|         12|   Susan Chen| 31|54000.0|10800.0|\n",
      "|         13|    Brian Kim| 45|75000.0|15000.0|\n",
      "|         14|    Emily Lee| 26|46000.0| 9200.0|\n",
      "|         15|  Michael Lee| 37|63000.0|12600.0|\n",
      "|         16|  Kelly Zhang| 30|49000.0| 9800.0|\n",
      "|         17|  George Wang| 34|57000.0|11400.0|\n",
      "|         18|    Nancy Liu| 29|50000.0|10000.0|\n",
      "|         19|  Steven Chen| 36|62000.0|12400.0|\n",
      "|         20|    Grace Kim| 32|53000.0|10600.0|\n",
      "+-----------+-------------+---+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adding new columns to the DataFrame\n",
    "\n",
    "emp_casted = emp.select(\"employee_id\", \"name\", \"age\", col(\"salary\").cast(\"double\"))\n",
    "\n",
    "emp_taxed = emp_casted.withColumn(\"tax\", col(\"salary\") * 0.2)\n",
    "emp_taxed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---+-------+-------+---------+---------+\n",
      "|employee_id|         name|age| salary|    tax|columnOne|columnTwo|\n",
      "+-----------+-------------+---+-------+-------+---------+---------+\n",
      "|          1|     John Doe| 30|50000.0|10000.0|        1|      two|\n",
      "|          2|   Jane Smith| 25|45000.0| 9000.0|        1|      two|\n",
      "|          3|    Bob Brown| 35|55000.0|11000.0|        1|      two|\n",
      "|          4|    Alice Lee| 28|48000.0| 9600.0|        1|      two|\n",
      "|          5|    Jack Chan| 40|60000.0|12000.0|        1|      two|\n",
      "|          6|    Jill Wong| 32|52000.0|10400.0|        1|      two|\n",
      "|          7|James Johnson| 42|70000.0|14000.0|        1|      two|\n",
      "|          8|     Kate Kim| 29|51000.0|10200.0|        1|      two|\n",
      "|          9|      Tom Tan| 33|58000.0|11600.0|        1|      two|\n",
      "|         10|     Lisa Lee| 27|47000.0| 9400.0|        1|      two|\n",
      "|         11|   David Park| 38|65000.0|13000.0|        1|      two|\n",
      "|         12|   Susan Chen| 31|54000.0|10800.0|        1|      two|\n",
      "|         13|    Brian Kim| 45|75000.0|15000.0|        1|      two|\n",
      "|         14|    Emily Lee| 26|46000.0| 9200.0|        1|      two|\n",
      "|         15|  Michael Lee| 37|63000.0|12600.0|        1|      two|\n",
      "|         16|  Kelly Zhang| 30|49000.0| 9800.0|        1|      two|\n",
      "|         17|  George Wang| 34|57000.0|11400.0|        1|      two|\n",
      "|         18|    Nancy Liu| 29|50000.0|10000.0|        1|      two|\n",
      "|         19|  Steven Chen| 36|62000.0|12400.0|        1|      two|\n",
      "|         20|    Grace Kim| 32|53000.0|10600.0|        1|      two|\n",
      "+-----------+-------------+---+-------+-------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Literals (Adding a constant to the DataFrame)\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "emp_new_cols = emp_taxed.withColumn(\"columnOne\", lit(1)).withColumn(\"columnTwo\", lit(\"two\"))\n",
    "emp_new_cols.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+---+-------+-------+---------+---------+\n",
      "|emp_id|         name|age| salary|    tax|columnOne|columnTwo|\n",
      "+------+-------------+---+-------+-------+---------+---------+\n",
      "|     1|     John Doe| 30|50000.0|10000.0|        1|      two|\n",
      "|     2|   Jane Smith| 25|45000.0| 9000.0|        1|      two|\n",
      "|     3|    Bob Brown| 35|55000.0|11000.0|        1|      two|\n",
      "|     4|    Alice Lee| 28|48000.0| 9600.0|        1|      two|\n",
      "|     5|    Jack Chan| 40|60000.0|12000.0|        1|      two|\n",
      "|     6|    Jill Wong| 32|52000.0|10400.0|        1|      two|\n",
      "|     7|James Johnson| 42|70000.0|14000.0|        1|      two|\n",
      "|     8|     Kate Kim| 29|51000.0|10200.0|        1|      two|\n",
      "|     9|      Tom Tan| 33|58000.0|11600.0|        1|      two|\n",
      "|    10|     Lisa Lee| 27|47000.0| 9400.0|        1|      two|\n",
      "|    11|   David Park| 38|65000.0|13000.0|        1|      two|\n",
      "|    12|   Susan Chen| 31|54000.0|10800.0|        1|      two|\n",
      "|    13|    Brian Kim| 45|75000.0|15000.0|        1|      two|\n",
      "|    14|    Emily Lee| 26|46000.0| 9200.0|        1|      two|\n",
      "|    15|  Michael Lee| 37|63000.0|12600.0|        1|      two|\n",
      "|    16|  Kelly Zhang| 30|49000.0| 9800.0|        1|      two|\n",
      "|    17|  George Wang| 34|57000.0|11400.0|        1|      two|\n",
      "|    18|    Nancy Liu| 29|50000.0|10000.0|        1|      two|\n",
      "|    19|  Steven Chen| 36|62000.0|12400.0|        1|      two|\n",
      "|    20|    Grace Kim| 32|53000.0|10600.0|        1|      two|\n",
      "+------+-------------+---+-------+-------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_new_cols.withColumnRenamed(\"employee_id\", \"emp_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---+-------+-------+---------+\n",
      "|employee_id|         name|age| salary|    tax|columnOne|\n",
      "+-----------+-------------+---+-------+-------+---------+\n",
      "|          1|     John Doe| 30|50000.0|10000.0|        1|\n",
      "|          2|   Jane Smith| 25|45000.0| 9000.0|        1|\n",
      "|          3|    Bob Brown| 35|55000.0|11000.0|        1|\n",
      "|          4|    Alice Lee| 28|48000.0| 9600.0|        1|\n",
      "|          5|    Jack Chan| 40|60000.0|12000.0|        1|\n",
      "|          6|    Jill Wong| 32|52000.0|10400.0|        1|\n",
      "|          7|James Johnson| 42|70000.0|14000.0|        1|\n",
      "|          8|     Kate Kim| 29|51000.0|10200.0|        1|\n",
      "|          9|      Tom Tan| 33|58000.0|11600.0|        1|\n",
      "|         10|     Lisa Lee| 27|47000.0| 9400.0|        1|\n",
      "|         11|   David Park| 38|65000.0|13000.0|        1|\n",
      "|         12|   Susan Chen| 31|54000.0|10800.0|        1|\n",
      "|         13|    Brian Kim| 45|75000.0|15000.0|        1|\n",
      "|         14|    Emily Lee| 26|46000.0| 9200.0|        1|\n",
      "|         15|  Michael Lee| 37|63000.0|12600.0|        1|\n",
      "|         16|  Kelly Zhang| 30|49000.0| 9800.0|        1|\n",
      "|         17|  George Wang| 34|57000.0|11400.0|        1|\n",
      "|         18|    Nancy Liu| 29|50000.0|10000.0|        1|\n",
      "|         19|  Steven Chen| 36|62000.0|12400.0|        1|\n",
      "|         20|    Grace Kim| 32|53000.0|10600.0|        1|\n",
      "+-----------+-------------+---+-------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dropping columns from the DataFrame\n",
    "\n",
    "emp_new_cols.drop(\"columnTwo\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---+-------+-------+---------+---------+\n",
      "|employee_id|         name|age| salary|    tax|columnOne|columnTwo|\n",
      "+-----------+-------------+---+-------+-------+---------+---------+\n",
      "|          1|     John Doe| 30|50000.0|10000.0|        1|      two|\n",
      "|          2|   Jane Smith| 25|45000.0| 9000.0|        1|      two|\n",
      "|          3|    Bob Brown| 35|55000.0|11000.0|        1|      two|\n",
      "|          4|    Alice Lee| 28|48000.0| 9600.0|        1|      two|\n",
      "|          5|    Jack Chan| 40|60000.0|12000.0|        1|      two|\n",
      "|          6|    Jill Wong| 32|52000.0|10400.0|        1|      two|\n",
      "|          7|James Johnson| 42|70000.0|14000.0|        1|      two|\n",
      "|          8|     Kate Kim| 29|51000.0|10200.0|        1|      two|\n",
      "|          9|      Tom Tan| 33|58000.0|11600.0|        1|      two|\n",
      "|         10|     Lisa Lee| 27|47000.0| 9400.0|        1|      two|\n",
      "|         11|   David Park| 38|65000.0|13000.0|        1|      two|\n",
      "|         12|   Susan Chen| 31|54000.0|10800.0|        1|      two|\n",
      "|         13|    Brian Kim| 45|75000.0|15000.0|        1|      two|\n",
      "|         14|    Emily Lee| 26|46000.0| 9200.0|        1|      two|\n",
      "|         15|  Michael Lee| 37|63000.0|12600.0|        1|      two|\n",
      "|         16|  Kelly Zhang| 30|49000.0| 9800.0|        1|      two|\n",
      "|         17|  George Wang| 34|57000.0|11400.0|        1|      two|\n",
      "|         18|    Nancy Liu| 29|50000.0|10000.0|        1|      two|\n",
      "|         19|  Steven Chen| 36|62000.0|12400.0|        1|      two|\n",
      "|         20|    Grace Kim| 32|53000.0|10600.0|        1|      two|\n",
      "+-----------+-------------+---+-------+-------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_new_cols.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---+-------+-------+\n",
      "|employee_id|         name|age| salary|    tax|\n",
      "+-----------+-------------+---+-------+-------+\n",
      "|          3|    Bob Brown| 35|55000.0|11000.0|\n",
      "|          5|    Jack Chan| 40|60000.0|12000.0|\n",
      "|          6|    Jill Wong| 32|52000.0|10400.0|\n",
      "|          7|James Johnson| 42|70000.0|14000.0|\n",
      "|          8|     Kate Kim| 29|51000.0|10200.0|\n",
      "+-----------+-------------+---+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter DataFrame where tax > 10000, along with LIMIT to 5 rows\n",
    "\n",
    "emp_taxed.where(\"tax > 10000\").limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---+-------+-------+------+\n",
      "|employee_id|         name|age| salary|    tax| bonus|\n",
      "+-----------+-------------+---+-------+-------+------+\n",
      "|          1|     John Doe| 30|50000.0|10000.0|5000.0|\n",
      "|          2|   Jane Smith| 25|45000.0| 9000.0|4500.0|\n",
      "|          3|    Bob Brown| 35|55000.0|11000.0|5500.0|\n",
      "|          4|    Alice Lee| 28|48000.0| 9600.0|4800.0|\n",
      "|          5|    Jack Chan| 40|60000.0|12000.0|6000.0|\n",
      "|          6|    Jill Wong| 32|52000.0|10400.0|5200.0|\n",
      "|          7|James Johnson| 42|70000.0|14000.0|7000.0|\n",
      "|          8|     Kate Kim| 29|51000.0|10200.0|5100.0|\n",
      "|          9|      Tom Tan| 33|58000.0|11600.0|5800.0|\n",
      "|         10|     Lisa Lee| 27|47000.0| 9400.0|4700.0|\n",
      "|         11|   David Park| 38|65000.0|13000.0|6500.0|\n",
      "|         12|   Susan Chen| 31|54000.0|10800.0|5400.0|\n",
      "|         13|    Brian Kim| 45|75000.0|15000.0|7500.0|\n",
      "|         14|    Emily Lee| 26|46000.0| 9200.0|4600.0|\n",
      "|         15|  Michael Lee| 37|63000.0|12600.0|6300.0|\n",
      "|         16|  Kelly Zhang| 30|49000.0| 9800.0|4900.0|\n",
      "|         17|  George Wang| 34|57000.0|11400.0|5700.0|\n",
      "|         18|    Nancy Liu| 29|50000.0|10000.0|5000.0|\n",
      "|         19|  Steven Chen| 36|62000.0|12400.0|6200.0|\n",
      "|         20|    Grace Kim| 32|53000.0|10600.0|5300.0|\n",
      "+-----------+-------------+---+-------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Bonus: Adding multiple columns to the dataframe at once\n",
    "\n",
    "columns = {\n",
    "    'tax': col('salary') * 0.2,\n",
    "    'bonus': col('salary') * 0.1\n",
    "}\n",
    "\n",
    "emp_casted.withColumns(columns).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String and Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://3e6699c11a7b:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark-intro</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fc63f7ecc50>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"String & Dates\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+-------------------+----------+\n",
      "|employee_id|department_id|         name|age|gender|salary|          hire_date|new_gender|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+----------+\n",
      "|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01 00:00:00|         M|\n",
      "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15 00:00:00|         F|\n",
      "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01 00:00:00|         M|\n",
      "|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30 00:00:00|         F|\n",
      "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01 00:00:00|         M|\n",
      "|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01 00:00:00|         F|\n",
      "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15 00:00:00|         M|\n",
      "|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01 00:00:00|         F|\n",
      "|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01 00:00:00|         M|\n",
      "|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01 00:00:00|         F|\n",
      "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01 00:00:00|         M|\n",
      "|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15 00:00:00|         F|\n",
      "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01 00:00:00|         M|\n",
      "|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01 00:00:00|         F|\n",
      "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30 00:00:00|         M|\n",
      "|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01 00:00:00|         F|\n",
      "|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15 00:00:00|         M|\n",
      "|         18|          104|    Nancy Liu| 29|Female| 50000|2017-06-01 00:00:00|         F|\n",
      "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01 00:00:00|         M|\n",
      "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01 00:00:00|         F|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a \"Case\" column to the DataFrame based on conditions\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "emp.withColumn('new_gender', when(col('gender') == 'Male', 'M').when(col('gender') == 'Female', 'F').otherwise(None)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+-------------------+-------------+\n",
      "|employee_id|department_id|         name|age|gender|salary|          hire_date|     new_name|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+-------------+\n",
      "|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01 00:00:00|     Zohn Doe|\n",
      "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15 00:00:00|   Zane Smith|\n",
      "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01 00:00:00|    Bob Brown|\n",
      "|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30 00:00:00|    Alice Lee|\n",
      "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01 00:00:00|    Zack Chan|\n",
      "|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01 00:00:00|    Zill Wong|\n",
      "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15 00:00:00|Zames Zohnson|\n",
      "|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01 00:00:00|     Kate Kim|\n",
      "|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01 00:00:00|      Tom Tan|\n",
      "|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01 00:00:00|     Lisa Lee|\n",
      "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01 00:00:00|   David Park|\n",
      "|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15 00:00:00|   Susan Chen|\n",
      "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01 00:00:00|    Brian Kim|\n",
      "|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01 00:00:00|    Emily Lee|\n",
      "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30 00:00:00|  Michael Lee|\n",
      "|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01 00:00:00|  Kelly Zhang|\n",
      "|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15 00:00:00|  George Wang|\n",
      "|         18|          104|    Nancy Liu| 29|Female| 50000|2017-06-01 00:00:00|    Nancy Liu|\n",
      "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01 00:00:00|  Steven Chen|\n",
      "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01 00:00:00|    Grace Kim|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace in Strings\n",
    "\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "emp.withColumn(\"new_name\", regexp_replace(\"name\", \"J\", \"Z\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- department_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert Timestamp (String) type column to Date type\n",
    "\n",
    "from pyspark.sql.functions import to_date, col\n",
    "\n",
    "emp.withColumn(\"hire_date\", to_date(col(\"hire_date\"), 'yyyy-MM-dd')).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+-------------------+------------+--------------------------+\n",
      "|employee_id|department_id|name         |age|gender|salary|hire_date          |current_date|current_timestamp         |\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+------------+--------------------------+\n",
      "|1          |101          |John Doe     |30 |Male  |50000 |2015-01-01 00:00:00|2025-04-30  |2025-04-30 02:52:20.972595|\n",
      "|2          |101          |Jane Smith   |25 |Female|45000 |2016-02-15 00:00:00|2025-04-30  |2025-04-30 02:52:20.972595|\n",
      "|3          |102          |Bob Brown    |35 |Male  |55000 |2014-05-01 00:00:00|2025-04-30  |2025-04-30 02:52:20.972595|\n",
      "|4          |102          |Alice Lee    |28 |Female|48000 |2017-09-30 00:00:00|2025-04-30  |2025-04-30 02:52:20.972595|\n",
      "|5          |103          |Jack Chan    |40 |Male  |60000 |2013-04-01 00:00:00|2025-04-30  |2025-04-30 02:52:20.972595|\n",
      "|6          |103          |Jill Wong    |32 |Female|52000 |2018-07-01 00:00:00|2025-04-30  |2025-04-30 02:52:20.972595|\n",
      "|7          |101          |James Johnson|42 |Male  |70000 |2012-03-15 00:00:00|2025-04-30  |2025-04-30 02:52:20.972595|\n",
      "|8          |102          |Kate Kim     |29 |Female|51000 |2019-10-01 00:00:00|2025-04-30  |2025-04-30 02:52:20.972595|\n",
      "|9          |103          |Tom Tan      |33 |Male  |58000 |2016-06-01 00:00:00|2025-04-30  |2025-04-30 02:52:20.972595|\n",
      "|10         |104          |Lisa Lee     |27 |Female|47000 |2018-08-01 00:00:00|2025-04-30  |2025-04-30 02:52:20.972595|\n",
      "|11         |104          |David Park   |38 |Male  |65000 |2015-11-01 00:00:00|2025-04-30  |2025-04-30 02:52:20.972595|\n",
      "|12         |105          |Susan Chen   |31 |Female|54000 |2017-02-15 00:00:00|2025-04-30  |2025-04-30 02:52:20.972595|\n",
      "|13         |106          |Brian Kim    |45 |Male  |75000 |2011-07-01 00:00:00|2025-04-30  |2025-04-30 02:52:20.972595|\n",
      "|14         |107          |Emily Lee    |26 |Female|46000 |2019-01-01 00:00:00|2025-04-30  |2025-04-30 02:52:20.972595|\n",
      "|15         |106          |Michael Lee  |37 |Male  |63000 |2014-09-30 00:00:00|2025-04-30  |2025-04-30 02:52:20.972595|\n",
      "|16         |107          |Kelly Zhang  |30 |Female|49000 |2018-04-01 00:00:00|2025-04-30  |2025-04-30 02:52:20.972595|\n",
      "|17         |105          |George Wang  |34 |Male  |57000 |2016-03-15 00:00:00|2025-04-30  |2025-04-30 02:52:20.972595|\n",
      "|18         |104          |Nancy Liu    |29 |Female|50000 |2017-06-01 00:00:00|2025-04-30  |2025-04-30 02:52:20.972595|\n",
      "|19         |103          |Steven Chen  |36 |Male  |62000 |2015-08-01 00:00:00|2025-04-30  |2025-04-30 02:52:20.972595|\n",
      "|20         |102          |Grace Kim    |32 |Female|53000 |2018-11-01 00:00:00|2025-04-30  |2025-04-30 02:52:20.972595|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add Current Date and Timestamp columns\n",
    "\n",
    "from pyspark.sql.functions import current_date, current_timestamp\n",
    "\n",
    "columns = {\n",
    "    \"current_date\": current_date(),\n",
    "    \"current_timestamp\": current_timestamp()\n",
    "}\n",
    "\n",
    "emp.withColumns(columns).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|employee_id|department_id|         name|age|gender|salary|          hire_date|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01 00:00:00|\n",
      "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15 00:00:00|\n",
      "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01 00:00:00|\n",
      "|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30 00:00:00|\n",
      "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01 00:00:00|\n",
      "|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01 00:00:00|\n",
      "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15 00:00:00|\n",
      "|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01 00:00:00|\n",
      "|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01 00:00:00|\n",
      "|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01 00:00:00|\n",
      "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01 00:00:00|\n",
      "|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15 00:00:00|\n",
      "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01 00:00:00|\n",
      "|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01 00:00:00|\n",
      "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30 00:00:00|\n",
      "|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01 00:00:00|\n",
      "|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15 00:00:00|\n",
      "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01 00:00:00|\n",
      "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01 00:00:00|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with null values in a column\n",
    "\n",
    "temp = emp.withColumn(\"gender\", when(col(\"name\") == \"Nancy Liu\", None).otherwise(col(\"gender\")))\n",
    "temp.na.drop().show()\n",
    "\n",
    "del temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+-------+------+-------------------+\n",
      "|employee_id|department_id|         name|age| gender|salary|          hire_date|\n",
      "+-----------+-------------+-------------+---+-------+------+-------------------+\n",
      "|          1|          101|     John Doe| 30|   Male| 50000|2015-01-01 00:00:00|\n",
      "|          2|          101|   Jane Smith| 25| Female| 45000|2016-02-15 00:00:00|\n",
      "|          3|          102|    Bob Brown| 35|   Male| 55000|2014-05-01 00:00:00|\n",
      "|          4|          102|    Alice Lee| 28| Female| 48000|2017-09-30 00:00:00|\n",
      "|          5|          103|    Jack Chan| 40|   Male| 60000|2013-04-01 00:00:00|\n",
      "|          6|          103|    Jill Wong| 32| Female| 52000|2018-07-01 00:00:00|\n",
      "|          7|          101|James Johnson| 42|   Male| 70000|2012-03-15 00:00:00|\n",
      "|          8|          102|     Kate Kim| 29| Female| 51000|2019-10-01 00:00:00|\n",
      "|          9|          103|      Tom Tan| 33|   Male| 58000|2016-06-01 00:00:00|\n",
      "|         10|          104|     Lisa Lee| 27| Female| 47000|2018-08-01 00:00:00|\n",
      "|         11|          104|   David Park| 38|   Male| 65000|2015-11-01 00:00:00|\n",
      "|         12|          105|   Susan Chen| 31| Female| 54000|2017-02-15 00:00:00|\n",
      "|         13|          106|    Brian Kim| 45|   Male| 75000|2011-07-01 00:00:00|\n",
      "|         14|          107|    Emily Lee| 26| Female| 46000|2019-01-01 00:00:00|\n",
      "|         15|          106|  Michael Lee| 37|   Male| 63000|2014-09-30 00:00:00|\n",
      "|         16|          107|  Kelly Zhang| 30| Female| 49000|2018-04-01 00:00:00|\n",
      "|         17|          105|  George Wang| 34|   Male| 57000|2016-03-15 00:00:00|\n",
      "|         18|          104|    Nancy Liu| 29|Unknown| 50000|2017-06-01 00:00:00|\n",
      "|         19|          103|  Steven Chen| 36|   Male| 62000|2015-08-01 00:00:00|\n",
      "|         20|          102|    Grace Kim| 32| Female| 53000|2018-11-01 00:00:00|\n",
      "+-----------+-------------+-------------+---+-------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fix null values with coalesce\n",
    "from pyspark.sql.functions import coalesce, lit\n",
    "\n",
    "temp = emp.withColumn(\"gender\", when(col(\"name\") == \"Nancy Liu\", None).otherwise(col(\"gender\")))\n",
    "\n",
    "temp.withColumn(\"gender\", coalesce(col(\"gender\"), lit(\"Unknown\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+-------------------+---------+\n",
      "|employee_id|department_id|         name|age|gender|salary|          hire_date|hire_year|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+---------+\n",
      "|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01 00:00:00|     2015|\n",
      "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15 00:00:00|     2016|\n",
      "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01 00:00:00|     2014|\n",
      "|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30 00:00:00|     2017|\n",
      "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01 00:00:00|     2013|\n",
      "|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01 00:00:00|     2018|\n",
      "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15 00:00:00|     2012|\n",
      "|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01 00:00:00|     2019|\n",
      "|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01 00:00:00|     2016|\n",
      "|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01 00:00:00|     2018|\n",
      "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01 00:00:00|     2015|\n",
      "|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15 00:00:00|     2017|\n",
      "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01 00:00:00|     2011|\n",
      "|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01 00:00:00|     2019|\n",
      "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30 00:00:00|     2014|\n",
      "|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01 00:00:00|     2018|\n",
      "|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15 00:00:00|     2016|\n",
      "|         18|          104|    Nancy Liu| 29|Female| 50000|2017-06-01 00:00:00|     2017|\n",
      "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01 00:00:00|     2015|\n",
      "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01 00:00:00|     2018|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert date/timestamp into string and extract information from it\n",
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "emp.withColumn(\"hire_year\", date_format(col(\"hire_date\"), \"yyyy\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort, Union & Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: string (nullable = true)\n",
      " |-- department_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      " |-- hire_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert all columns to string type\n",
    "\n",
    "emp_str = emp.select([col(c).cast(\"string\") for c in emp.columns])\n",
    "emp_str.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataframe into 2 parts\n",
    "\n",
    "emp_str1 = emp_str.filter(emp.employee_id < 11)\n",
    "emp_str2 = emp_str.filter(emp.employee_id > 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|employee_id|department_id|         name|age|gender|salary|          hire_date|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01 00:00:00|\n",
      "|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15 00:00:00|\n",
      "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01 00:00:00|\n",
      "|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01 00:00:00|\n",
      "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30 00:00:00|\n",
      "|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01 00:00:00|\n",
      "|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15 00:00:00|\n",
      "|         18|          104|    Nancy Liu| 29|Female| 50000|2017-06-01 00:00:00|\n",
      "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01 00:00:00|\n",
      "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01 00:00:00|\n",
      "|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01 00:00:00|\n",
      "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15 00:00:00|\n",
      "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01 00:00:00|\n",
      "|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30 00:00:00|\n",
      "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01 00:00:00|\n",
      "|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01 00:00:00|\n",
      "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15 00:00:00|\n",
      "|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01 00:00:00|\n",
      "|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01 00:00:00|\n",
      "|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01 00:00:00|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Union and Union All (remove duplicates)\n",
    "# ! The columns must be in the same order and have the same data types\n",
    "# ? UnionByName can used when the column names are different but data types are same\n",
    "\n",
    "emp_str2.union(emp_str1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|employee_id|department_id|         name|age|gender|salary|          hire_date|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01 00:00:00|\n",
      "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15 00:00:00|\n",
      "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01 00:00:00|\n",
      "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30 00:00:00|\n",
      "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01 00:00:00|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|employee_id|department_id|         name|age|gender|salary|          hire_date|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01 00:00:00|\n",
      "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15 00:00:00|\n",
      "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01 00:00:00|\n",
      "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01 00:00:00|\n",
      "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30 00:00:00|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sorting the dataframe\n",
    "\n",
    "from pyspark.sql.functions import asc, desc\n",
    "\n",
    "emp.orderBy(desc(\"salary\")).show(5)\n",
    "\n",
    "emp.orderBy(asc(\"hire_date\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+--------+\n",
      "|department_id|dept_count|dept_pay|\n",
      "+-------------+----------+--------+\n",
      "|          103|         4|  232000|\n",
      "|          102|         4|  207000|\n",
      "|          101|         3|  165000|\n",
      "|          104|         3|  162000|\n",
      "|          106|         2|  138000|\n",
      "|          105|         2|  111000|\n",
      "|          107|         2|   95000|\n",
      "+-------------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregation functions\n",
    "from pyspark.sql.functions import count, sum, avg, max, min\n",
    "\n",
    "emp.groupBy(\"department_id\").agg(count(\"employee_id\").alias(\"dept_count\"), sum(\"salary\").alias(\"dept_pay\")).orderBy(desc(\"dept_pay\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+\n",
      "|department_id|avg_dept_salary|\n",
      "+-------------+---------------+\n",
      "|          101|        55000.0|\n",
      "|          103|        58000.0|\n",
      "|          102|        51750.0|\n",
      "|          105|        55500.0|\n",
      "|          106|        69000.0|\n",
      "|          104|        54000.0|\n",
      "+-------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.groupby(\"department_id\").agg(avg(\"salary\").alias(\"avg_dept_salary\")).where(col(\"avg_dept_salary\") > 50000).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique Data and Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|employee_id|department_id|         name|age|gender|salary|          hire_date|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01 00:00:00|\n",
      "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01 00:00:00|\n",
      "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01 00:00:00|\n",
      "|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01 00:00:00|\n",
      "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01 00:00:00|\n",
      "|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01 00:00:00|\n",
      "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01 00:00:00|\n",
      "|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01 00:00:00|\n",
      "|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15 00:00:00|\n",
      "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15 00:00:00|\n",
      "|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01 00:00:00|\n",
      "|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30 00:00:00|\n",
      "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01 00:00:00|\n",
      "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15 00:00:00|\n",
      "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01 00:00:00|\n",
      "|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15 00:00:00|\n",
      "|         18|          104|    Nancy Liu| 29|Female| 50000|2017-06-01 00:00:00|\n",
      "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30 00:00:00|\n",
      "|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01 00:00:00|\n",
      "|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01 00:00:00|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get unique data from the dataframe\n",
    "\n",
    "emp.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|department_id|\n",
      "+-------------+\n",
      "|          101|\n",
      "|          103|\n",
      "|          107|\n",
      "|          102|\n",
      "|          105|\n",
      "|          106|\n",
      "|          104|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.select('department_id').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+-------------------+---------------+\n",
      "|employee_id|department_id|         name|age|gender|salary|          hire_date|max_dept_salary|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+---------------+\n",
      "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15 00:00:00|          70000|\n",
      "|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01 00:00:00|          70000|\n",
      "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15 00:00:00|          70000|\n",
      "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01 00:00:00|          55000|\n",
      "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01 00:00:00|          55000|\n",
      "|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01 00:00:00|          55000|\n",
      "|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30 00:00:00|          55000|\n",
      "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01 00:00:00|          62000|\n",
      "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01 00:00:00|          62000|\n",
      "|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01 00:00:00|          62000|\n",
      "|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01 00:00:00|          62000|\n",
      "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01 00:00:00|          65000|\n",
      "|         18|          104|    Nancy Liu| 29|Female| 50000|2017-06-01 00:00:00|          65000|\n",
      "|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01 00:00:00|          65000|\n",
      "|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15 00:00:00|          57000|\n",
      "|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15 00:00:00|          57000|\n",
      "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01 00:00:00|          75000|\n",
      "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30 00:00:00|          75000|\n",
      "|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01 00:00:00|          49000|\n",
      "|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01 00:00:00|          49000|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Window functions\n",
    "# ? Allows to compute values based on a \"window\" of rows without collapsing them into a single row, unlike groupBy()\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import max, col, desc\n",
    "\n",
    "window_spec = Window.partitionBy(\"department_id\").orderBy(desc(\"salary\"))\n",
    "max_func = max(col(\"salary\")).over(window_spec)\n",
    "\n",
    "emp.withColumn(\"max_dept_salary\", max_func).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-----------+---+------+------+-------------------+----+\n",
      "|employee_id|department_id|       name|age|gender|salary|          hire_date|rank|\n",
      "+-----------+-------------+-----------+---+------+------+-------------------+----+\n",
      "|          1|          101|   John Doe| 30|  Male| 50000|2015-01-01 00:00:00|   2|\n",
      "|         20|          102|  Grace Kim| 32|Female| 53000|2018-11-01 00:00:00|   2|\n",
      "|          5|          103|  Jack Chan| 40|  Male| 60000|2013-04-01 00:00:00|   2|\n",
      "|         18|          104|  Nancy Liu| 29|Female| 50000|2017-06-01 00:00:00|   2|\n",
      "|         12|          105| Susan Chen| 31|Female| 54000|2017-02-15 00:00:00|   2|\n",
      "|         15|          106|Michael Lee| 37|  Male| 63000|2014-09-30 00:00:00|   2|\n",
      "|         14|          107|  Emily Lee| 26|Female| 46000|2019-01-01 00:00:00|   2|\n",
      "+-----------+-------------+-----------+---+------+------+-------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the 2nd highest salary in each department\n",
    "\n",
    "from pyspark.sql.functions import rank, col\n",
    "\n",
    "rank_spec = Window.partitionBy(\"department_id\").orderBy(desc(\"salary\"))\n",
    "rank_func = rank().over(rank_spec)\n",
    "\n",
    "emp.withColumn(\"rank\", rank_func).filter(col(\"rank\") == 2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joins and Data Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://a9106065823c:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Join & Partition</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f461c1b5300>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 36290)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/local/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/local/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/local/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/spark/python/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/spark/python/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/spark/python/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/spark/python/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Join & Partition\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: string (nullable = true)\n",
      " |-- department_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      " |-- hire_date: string (nullable = true)\n",
      "\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|employee_id|department_id|         name|age|gender|salary|          hire_date|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01 00:00:00|\n",
      "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15 00:00:00|\n",
      "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01 00:00:00|\n",
      "|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30 00:00:00|\n",
      "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01 00:00:00|\n",
      "|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01 00:00:00|\n",
      "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15 00:00:00|\n",
      "|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01 00:00:00|\n",
      "|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01 00:00:00|\n",
      "|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01 00:00:00|\n",
      "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01 00:00:00|\n",
      "|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15 00:00:00|\n",
      "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01 00:00:00|\n",
      "|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01 00:00:00|\n",
      "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30 00:00:00|\n",
      "|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01 00:00:00|\n",
      "|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15 00:00:00|\n",
      "|         18|          104|    Nancy Liu| 29|Female| 50000|2017-06-01 00:00:00|\n",
      "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01 00:00:00|\n",
      "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01 00:00:00|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp = spark.read.csv(\"data/emp.csv\", header=True, inferSchema=True)\n",
    "emp = emp.select([col(c).cast(\"string\") for c in emp.columns])\n",
    "\n",
    "emp.printSchema()\n",
    "emp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- department_id: string (nullable = true)\n",
      " |-- department_name: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- budget: string (nullable = true)\n",
      "\n",
      "+-------------+--------------------+-------+---------+-------+\n",
      "|department_id|     department_name|   city|  country| budget|\n",
      "+-------------+--------------------+-------+---------+-------+\n",
      "|          101|               Sales|    NYC|       US|1000000|\n",
      "|          102|           Marketing|     LA|       US| 900000|\n",
      "|          103|             Finance| London|       UK|1200000|\n",
      "|          104|         Engineering|Beijing|    China|1500000|\n",
      "|          105|     Human Resources|  Tokyo|    Japan| 800000|\n",
      "|          106|Research and Deve...|  Perth|Australia|1100000|\n",
      "|          107|    Customer Service| Sydney|Australia| 950000|\n",
      "+-------------+--------------------+-------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept_data = [\n",
    "    [\"101\", \"Sales\", \"NYC\", \"US\", \"1000000\"],\n",
    "    [\"102\", \"Marketing\", \"LA\", \"US\", \"900000\"],\n",
    "    [\"103\", \"Finance\", \"London\", \"UK\", \"1200000\"],\n",
    "    [\"104\", \"Engineering\", \"Beijing\", \"China\", \"1500000\"],\n",
    "    [\"105\", \"Human Resources\", \"Tokyo\", \"Japan\", \"800000\"],\n",
    "    [\"106\", \"Research and Development\", \"Perth\", \"Australia\", \"1100000\"],\n",
    "    [\"107\", \"Customer Service\", \"Sydney\", \"Australia\", \"950000\"]\n",
    "]\n",
    "\n",
    "dept_schema = \"department_id string, department_name string, city string, country string, budget string\"\n",
    "\n",
    "dept = spark.createDataFrame(data=dept_data, schema=dept_schema)\n",
    "\n",
    "dept.printSchema()\n",
    "dept.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions in Employee dataset: 1\n",
      "Partitions in Department dataset: 24\n"
     ]
    }
   ],
   "source": [
    "print(f\"Partitions in Employee dataset: {emp.rdd.getNumPartitions()}\")\n",
    "print(f\"Partitions in Department dataset: {dept.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp.repartition(4).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dept.repartition(8).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `coalesce` can help *reduce* the number of partitions without reshuffle (shuffling b/w the executors).\n",
    "# It also doesn't guarantee uniform data distribution, while repartition does.\n",
    "\n",
    "emp.coalesce(4).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+-------------------+-------------+\n",
      "|employee_id|department_id|         name|age|gender|salary|          hire_date|partition_num|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+-------------+\n",
      "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01 00:00:00|            0|\n",
      "|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30 00:00:00|            0|\n",
      "|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01 00:00:00|            0|\n",
      "|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01 00:00:00|            0|\n",
      "|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01 00:00:00|            0|\n",
      "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01 00:00:00|            0|\n",
      "|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15 00:00:00|            1|\n",
      "|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15 00:00:00|            1|\n",
      "|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01 00:00:00|            2|\n",
      "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01 00:00:00|            2|\n",
      "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01 00:00:00|            2|\n",
      "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30 00:00:00|            2|\n",
      "|         18|          104|    Nancy Liu| 29|Female| 50000|2017-06-01 00:00:00|            2|\n",
      "|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01 00:00:00|            3|\n",
      "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15 00:00:00|            3|\n",
      "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01 00:00:00|            3|\n",
      "|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01 00:00:00|            3|\n",
      "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15 00:00:00|            3|\n",
      "|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01 00:00:00|            3|\n",
      "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01 00:00:00|            3|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Repartition the Employee dataframe on/using the department_id column\n",
    "\n",
    "emp_part = emp.repartition(4, \"department_id\")\n",
    "emp_part.withColumn(\"partition_num\", spark_partition_id()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+--------------------+------+\n",
      "|         name|department_id|     department_name|salary|\n",
      "+-------------+-------------+--------------------+------+\n",
      "|James Johnson|          101|               Sales| 70000|\n",
      "|   Jane Smith|          101|               Sales| 45000|\n",
      "|     John Doe|          101|               Sales| 50000|\n",
      "|    Grace Kim|          102|           Marketing| 53000|\n",
      "|     Kate Kim|          102|           Marketing| 51000|\n",
      "|    Alice Lee|          102|           Marketing| 48000|\n",
      "|    Bob Brown|          102|           Marketing| 55000|\n",
      "|  Steven Chen|          103|             Finance| 62000|\n",
      "|      Tom Tan|          103|             Finance| 58000|\n",
      "|    Jill Wong|          103|             Finance| 52000|\n",
      "|    Jack Chan|          103|             Finance| 60000|\n",
      "|    Nancy Liu|          104|         Engineering| 50000|\n",
      "|   David Park|          104|         Engineering| 65000|\n",
      "|     Lisa Lee|          104|         Engineering| 47000|\n",
      "|  George Wang|          105|     Human Resources| 57000|\n",
      "|   Susan Chen|          105|     Human Resources| 54000|\n",
      "|  Michael Lee|          106|Research and Deve...| 63000|\n",
      "|    Brian Kim|          106|Research and Deve...| 75000|\n",
      "|  Kelly Zhang|          107|    Customer Service| 49000|\n",
      "|    Emily Lee|          107|    Customer Service| 46000|\n",
      "+-------------+-------------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# (Inner) join the Employee and Department dataframes and show a select set of columns\n",
    "\n",
    "emp.join(dept, how=\"inner\", on=emp.department_id==dept.department_id).select(emp.name, dept.department_id, dept.department_name, emp.salary).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading from CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- department_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- hire_date: timestamp (nullable = true)\n",
      "\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|employee_id|department_id|         name|age|gender|salary|          hire_date|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01 00:00:00|\n",
      "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15 00:00:00|\n",
      "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01 00:00:00|\n",
      "|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30 00:00:00|\n",
      "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01 00:00:00|\n",
      "|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01 00:00:00|\n",
      "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15 00:00:00|\n",
      "|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01 00:00:00|\n",
      "|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01 00:00:00|\n",
      "|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01 00:00:00|\n",
      "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01 00:00:00|\n",
      "|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15 00:00:00|\n",
      "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01 00:00:00|\n",
      "|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01 00:00:00|\n",
      "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30 00:00:00|\n",
      "|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01 00:00:00|\n",
      "|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15 00:00:00|\n",
      "|         18|          104|    Nancy Liu| 29|Female| 50000|2017-06-01 00:00:00|\n",
      "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01 00:00:00|\n",
      "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01 00:00:00|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data/emp.csv\", header=True, inferSchema=True)\n",
    "\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proactively passing Spark the dataframe-schema helps optimise the process by preventing \n",
    "# Spark from having to read any data and infer the header/schema by itself.\n",
    "# (No new \"Job\" was initiated for this transformation)\n",
    "\n",
    "emp_schema = \"employee_id int, department_id int, name string, age int, gender string, salary int, hire_date timestamp\"\n",
    "\n",
    "emp_give_schema = spark.read.csv(\"data/emp.csv\", header=True, schema=emp_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|employee_id|department_id|         name|age|gender|salary|          hire_date|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01 00:00:00|\n",
      "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15 00:00:00|\n",
      "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01 00:00:00|\n",
      "|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30 00:00:00|\n",
      "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01 00:00:00|\n",
      "|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01 00:00:00|\n",
      "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15 00:00:00|\n",
      "|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01 00:00:00|\n",
      "|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01 00:00:00|\n",
      "|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01 00:00:00|\n",
      "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01 00:00:00|\n",
      "|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15 00:00:00|\n",
      "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01 00:00:00|\n",
      "|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01 00:00:00|\n",
      "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30 00:00:00|\n",
      "|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01 00:00:00|\n",
      "|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15 00:00:00|\n",
      "|         18|          104|    Nancy Liu| 29|Female| 50000|2017-06-01 00:00:00|\n",
      "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01 00:00:00|\n",
      "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01 00:00:00|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_give_schema.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+-------------------+--------------------------------------------+\n",
      "|employee_id|department_id|name         |age|gender|salary|hire_date          |_corrupt_record                             |\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+--------------------------------------------+\n",
      "|1          |101          |John Doe     |30 |Male  |50000 |2015-01-01 00:00:00|null                                        |\n",
      "|2          |101          |Jane Smith   |25 |Female|45000 |2016-02-15 00:00:00|null                                        |\n",
      "|3          |102          |Bob Brown    |35 |Male  |55000 |2014-05-01 00:00:00|null                                        |\n",
      "|4          |102          |Alice Lee    |28 |Female|48000 |2017-09-30 00:00:00|null                                        |\n",
      "|5          |103          |Jack Chan    |40 |Male  |60000 |2013-04-01 00:00:00|null                                        |\n",
      "|6          |103          |Jill Wong    |32 |Female|52000 |2018-07-01 00:00:00|null                                        |\n",
      "|7          |101          |James Johnson|42 |Male  |null  |2012-03-15 00:00:00|007,101,James Johnson,42,Male,Low,2012-03-15|\n",
      "|8          |102          |Kate Kim     |29 |Female|51000 |2019-10-01 00:00:00|null                                        |\n",
      "|9          |103          |Tom Tan      |33 |Male  |58000 |2016-06-01 00:00:00|null                                        |\n",
      "|10         |104          |Lisa Lee     |27 |Female|47000 |2018-08-01 00:00:00|null                                        |\n",
      "|11         |104          |David Park   |38 |Male  |65000 |null               |011,104,David Park,38,Male,65000,no date    |\n",
      "|12         |105          |Susan Chen   |31 |Female|54000 |2017-02-15 00:00:00|null                                        |\n",
      "|13         |106          |Brian Kim    |45 |Male  |75000 |2011-07-01 00:00:00|null                                        |\n",
      "|14         |107          |Emily Lee    |26 |Female|46000 |2019-01-01 00:00:00|null                                        |\n",
      "|15         |106          |Michael Lee  |37 |Male  |63000 |2014-09-30 00:00:00|null                                        |\n",
      "|16         |107          |Kelly Zhang  |30 |Female|49000 |2018-04-01 00:00:00|null                                        |\n",
      "|17         |105          |George Wang  |34 |Male  |57000 |2016-03-15 00:00:00|null                                        |\n",
      "|18         |104          |Nancy Liu    |29 |Female|50000 |2017-06-01 00:00:00|null                                        |\n",
      "|19         |103          |Steven Chen  |36 |Male  |62000 |2015-08-01 00:00:00|null                                        |\n",
      "|20         |102          |Grace Kim    |32 |Female|53000 |2018-11-01 00:00:00|null                                        |\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+--------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The `mode` attribute (PERMISSIVE, by default) helps with handling \"bad\" records.\n",
    "\n",
    "emp_bad_schema = \"employee_id int, department_id int, name string, age int, gender string, salary int, hire_date timestamp, _corrupt_record string\"\n",
    "\n",
    "bad_df = spark.read.csv(\"data/emp_new.csv\", header=True, schema=emp_bad_schema)\n",
    "bad_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-----------+---+------+------+-------------------+---------------+\n",
      "|employee_id|department_id|       name|age|gender|salary|          hire_date|_corrupt_record|\n",
      "+-----------+-------------+-----------+---+------+------+-------------------+---------------+\n",
      "|          1|          101|   John Doe| 30|  Male| 50000|2015-01-01 00:00:00|           null|\n",
      "|          2|          101| Jane Smith| 25|Female| 45000|2016-02-15 00:00:00|           null|\n",
      "|          3|          102|  Bob Brown| 35|  Male| 55000|2014-05-01 00:00:00|           null|\n",
      "|          4|          102|  Alice Lee| 28|Female| 48000|2017-09-30 00:00:00|           null|\n",
      "|          5|          103|  Jack Chan| 40|  Male| 60000|2013-04-01 00:00:00|           null|\n",
      "|          6|          103|  Jill Wong| 32|Female| 52000|2018-07-01 00:00:00|           null|\n",
      "|          8|          102|   Kate Kim| 29|Female| 51000|2019-10-01 00:00:00|           null|\n",
      "|          9|          103|    Tom Tan| 33|  Male| 58000|2016-06-01 00:00:00|           null|\n",
      "|         10|          104|   Lisa Lee| 27|Female| 47000|2018-08-01 00:00:00|           null|\n",
      "|         12|          105| Susan Chen| 31|Female| 54000|2017-02-15 00:00:00|           null|\n",
      "|         13|          106|  Brian Kim| 45|  Male| 75000|2011-07-01 00:00:00|           null|\n",
      "|         14|          107|  Emily Lee| 26|Female| 46000|2019-01-01 00:00:00|           null|\n",
      "|         15|          106|Michael Lee| 37|  Male| 63000|2014-09-30 00:00:00|           null|\n",
      "|         16|          107|Kelly Zhang| 30|Female| 49000|2018-04-01 00:00:00|           null|\n",
      "|         17|          105|George Wang| 34|  Male| 57000|2016-03-15 00:00:00|           null|\n",
      "|         18|          104|  Nancy Liu| 29|Female| 50000|2017-06-01 00:00:00|           null|\n",
      "|         19|          103|Steven Chen| 36|  Male| 62000|2015-08-01 00:00:00|           null|\n",
      "|         20|          102|  Grace Kim| 32|Female| 53000|2018-11-01 00:00:00|           null|\n",
      "+-----------+-------------+-----------+---+------+------+-------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ? `DROPMALFORMED` mode drops the corrupt rows in the dataframe\n",
    "# ? `FAILFAST` mode fails as soon as it encounters any corrupt data (Usually used in scenarios involving payments processing)\n",
    "\n",
    "spark.read.format(\"csv\").option(\"header\", True).option(\"mode\", \"DROPMALFORMED\").schema(emp_bad_schema).load(\"data/emp_new.csv\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|employee_id|department_id|         name|age|gender|salary|          hire_date|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01 00:00:00|\n",
      "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15 00:00:00|\n",
      "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01 00:00:00|\n",
      "|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30 00:00:00|\n",
      "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01 00:00:00|\n",
      "|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01 00:00:00|\n",
      "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15 00:00:00|\n",
      "|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01 00:00:00|\n",
      "|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01 00:00:00|\n",
      "|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01 00:00:00|\n",
      "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01 00:00:00|\n",
      "|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15 00:00:00|\n",
      "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01 00:00:00|\n",
      "|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01 00:00:00|\n",
      "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30 00:00:00|\n",
      "|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01 00:00:00|\n",
      "|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15 00:00:00|\n",
      "|         18|          104|    Nancy Liu| 29|Female| 50000|2017-06-01 00:00:00|\n",
      "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01 00:00:00|\n",
      "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01 00:00:00|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pass multiple arguments for Option using dict()\n",
    "\n",
    "_options = {\n",
    "    \"header\": True,\n",
    "    \"inferSchema\": True,\n",
    "    \"mode\": \"PERMISSIVE\",\n",
    "}\n",
    "\n",
    "spark.read.format(\"csv\").options(**_options).load(\"data/emp.csv\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Complex Data Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`Columnar Format` benifits when extracting specific column information since it only needs to read, decompress and process a specific part for the current query. In comparison. `Row Format` needs to read, decompress and process the entire file to determine the result for a particular query.\n",
    "\n",
    "- Row Format: <pre>A001, Dexter, <b>500</b>, A002, Tom, <b>600</b>, A003, Jerry, <b>1000</b></pre>\n",
    "- Column Format: <pre>A001, A002, A003, Dexter, Tom, Jerry, <b>`500, 600, 1000`</b></pre>\n",
    "- Query: Print the salary for all employees\n",
    "<br>\n",
    "<br>\n",
    "<center><img src=\"images/format-comparison.png\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- transacted_at: timestamp (nullable = true)\n",
      " |-- trx_id: integer (nullable = true)\n",
      " |-- retailer_id: integer (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- city_id: integer (nullable = true)\n",
      "\n",
      "+-------------------+----------+-----------+-----------------------------------------------+-------+----------+\n",
      "|transacted_at      |trx_id    |retailer_id|description                                    |amount |city_id   |\n",
      "+-------------------+----------+-----------+-----------------------------------------------+-------+----------+\n",
      "|2017-11-24 19:00:00|1995601912|2077350195 |Walgreen       11-25                           |197.23 |216510442 |\n",
      "|2017-11-24 19:00:00|1734117021|644879053  |unkn    ppd id: 768641     11-26               |8.58   |930259917 |\n",
      "|2017-11-24 19:00:00|1734117022|847200066  |Wal-Mart  ppd id: 555914     Algiers    11-26  |1737.26|1646415505|\n",
      "|2017-11-24 19:00:00|1734117030|1953761884 |Home Depot     ppd id: 265293   11-25          |384.5  |287177635 |\n",
      "|2017-11-24 19:00:00|1734117089|1898522855 |Target        11-25                            |66.33  |1855530529|\n",
      "|2017-11-24 19:00:00|1734117117|997626433  |Sears  ppd id: 856095  Ashgabat                |298.87 |957346984 |\n",
      "|2017-11-24 19:00:00|1734117123|1953761884 |unkn   ppd id: 153174    Little Rock    11-25  |19.55  |45522086  |\n",
      "|2017-11-24 19:00:00|1734117152|1429095612 |Ikea     arc id: 527956  Saint John's   11-26  |9.39   |1268541279|\n",
      "|2017-11-24 19:00:00|1734117153|847200066  |unkn        Kingstown                          |2907.57|1483931123|\n",
      "|2017-11-24 19:00:00|1734117212|1996661856 |unkn    ppd id: 454437   11-24                 |140.38 |336763936 |\n",
      "|2017-11-24 19:00:00|1734117241|486576507  |iTunes                                         |2912.67|1663872965|\n",
      "|2017-11-24 19:00:00|2076947148|847200066  |Wal-Mart         11-24                         |62.83  |1556600840|\n",
      "|2017-11-24 19:00:00|2076947147|562903918  |McDonald's    ccd id: 135878  Ljubljana   11-24|31.37  |930259917 |\n",
      "|2017-11-24 19:00:00|2076947146|511877722  |unkn     ccd id: 598521     Ankara   11-26     |1915.35|1698762556|\n",
      "|2017-11-24 19:00:00|2076947113|1996661856 |AutoZone  arc id: 998454    11-25              |1523.6 |1759612211|\n",
      "|2017-11-24 19:00:00|2076947018|902350112  |DineEquity    arc id: 1075293                  |22.28  |2130657559|\n",
      "|2017-11-24 19:00:00|2076946994|1898522855 |Target    ppd id: 336785                       |2589.93|2074005445|\n",
      "|2017-11-24 19:00:00|2076946985|847200066  |Wal-Mart    ppd id: 252763  11-26              |42.2   |459344513 |\n",
      "|2017-11-24 19:00:00|2076946960|386167994  |Wendy's  ppd id: 881511     11-24              |14.62  |352952442 |\n",
      "|2017-11-24 19:00:00|2076946954|486576507  |iTunes     ppd id: 121397                      |37.42  |485114748 |\n",
      "+-------------------+----------+-----------+-----------------------------------------------+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1102576"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_pq = spark.read.parquet(\"data/sales_data.parquet\")\n",
    "\n",
    "sales_pq.printSchema()\n",
    "sales_pq.show(truncate=False)\n",
    "sales_pq.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- transacted_at: timestamp (nullable = true)\n",
      " |-- trx_id: integer (nullable = true)\n",
      " |-- retailer_id: integer (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- city_id: integer (nullable = true)\n",
      "\n",
      "+-------------------+----------+-----------+--------------------+-------+----------+\n",
      "|      transacted_at|    trx_id|retailer_id|         description| amount|   city_id|\n",
      "+-------------------+----------+-----------+--------------------+-------+----------+\n",
      "|2017-11-24 19:00:00|1995601912| 2077350195|Walgreen       11-25| 197.23| 216510442|\n",
      "|2017-11-24 19:00:00|1734117021|  644879053|unkn    ppd id: 7...|   8.58| 930259917|\n",
      "|2017-11-24 19:00:00|1734117022|  847200066|Wal-Mart  ppd id:...|1737.26|1646415505|\n",
      "|2017-11-24 19:00:00|1734117030| 1953761884|Home Depot     pp...|  384.5| 287177635|\n",
      "|2017-11-24 19:00:00|1734117089| 1898522855| Target        11-25|  66.33|1855530529|\n",
      "|2017-11-24 19:00:00|1734117117|  997626433|Sears  ppd id: 85...| 298.87| 957346984|\n",
      "|2017-11-24 19:00:00|1734117123| 1953761884|unkn   ppd id: 15...|  19.55|  45522086|\n",
      "|2017-11-24 19:00:00|1734117152| 1429095612|Ikea     arc id: ...|   9.39|1268541279|\n",
      "|2017-11-24 19:00:00|1734117153|  847200066|unkn        Kings...|2907.57|1483931123|\n",
      "|2017-11-24 19:00:00|1734117212| 1996661856|unkn    ppd id: 4...| 140.38| 336763936|\n",
      "|2017-11-24 19:00:00|1734117241|  486576507|              iTunes|2912.67|1663872965|\n",
      "|2017-11-24 19:00:00|2076947148|  847200066|Wal-Mart         ...|  62.83|1556600840|\n",
      "|2017-11-24 19:00:00|2076947147|  562903918|McDonald's    ccd...|  31.37| 930259917|\n",
      "|2017-11-24 19:00:00|2076947146|  511877722|unkn     ccd id: ...|1915.35|1698762556|\n",
      "|2017-11-24 19:00:00|2076947113| 1996661856|AutoZone  arc id:...| 1523.6|1759612211|\n",
      "|2017-11-24 19:00:00|2076947018|  902350112|DineEquity    arc...|  22.28|2130657559|\n",
      "|2017-11-24 19:00:00|2076946994| 1898522855|Target    ppd id:...|2589.93|2074005445|\n",
      "|2017-11-24 19:00:00|2076946985|  847200066|Wal-Mart    ppd i...|   42.2| 459344513|\n",
      "|2017-11-24 19:00:00|2076946960|  386167994|Wendy's  ppd id: ...|  14.62| 352952442|\n",
      "|2017-11-24 19:00:00|2076946954|  486576507|iTunes     ppd id...|  37.42| 485114748|\n",
      "+-------------------+----------+-----------+--------------------+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1102576"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_orc = spark.read.orc(\"data/sales_data.orc\")\n",
    "\n",
    "sales_orc.printSchema()\n",
    "sales_orc.show()\n",
    "sales_orc.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1102576"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading all files in a folder\n",
    "\n",
    "sales_m_pq = spark.read.parquet('data/sales_total_parquet/*.parquet')\n",
    "sales_m_pq.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise a decorator to track operation time\n",
    "\n",
    "import time\n",
    "\n",
    "def get_time(func):\n",
    "    def inner_get_time() -> str:\n",
    "        st = time.time()\n",
    "        func()\n",
    "        return (f\"Execution Time: {(time.time() - st) * 1000:.4f} ms\")\n",
    "    print(inner_get_time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time: 377.0742 ms\n"
     ]
    }
   ],
   "source": [
    "@get_time\n",
    "def x():\n",
    "    spark.read.parquet(\"data/sales_data.parquet\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time: 215.5883 ms\n"
     ]
    }
   ],
   "source": [
    "@get_time\n",
    "def x():\n",
    "    spark.read.parquet(\"data/sales_data.parquet\").select(\"trx_id\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+-----------+--------------------+------+---------+\n",
      "|      transacted_at|    trx_id|retailer_id|         description|amount|  city_id|\n",
      "+-------------------+----------+-----------+--------------------+------+---------+\n",
      "|2017-11-24 19:00:00|1734117123| 1953761884|unkn   ppd id: 15...| 19.55| 45522086|\n",
      "|2017-11-24 19:00:00|1734117021|  644879053|unkn    ppd id: 7...|  8.58|930259917|\n",
      "+-------------------+----------+-----------+--------------------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load nested files within subfolders\n",
    "\n",
    "spark.read.parquet(\"data/sales_recursive/\", recursiveFileLookup=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment\n",
    "\n",
    "Calculate the average amount per retailer and append a column displaying the corresponding value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|retailer_id|        avg_amount|\n",
      "+-----------+------------------+\n",
      "|  862777075| 400.6277299001569|\n",
      "| 2092104004| 395.6865486725659|\n",
      "|  495545430|394.33791799362956|\n",
      "|  771821475| 393.7823759630201|\n",
      "|  508452694| 390.6041735537194|\n",
      "| 1295306792|389.76992566161067|\n",
      "| 1006678445| 389.4951192145855|\n",
      "|  316135668|386.18770249924586|\n",
      "|  860355551|384.89784578313174|\n",
      "|  162598651| 384.4054847207588|\n",
      "|  386167994|  383.733645990921|\n",
      "| 2145070162| 382.9899318729707|\n",
      "|  304276488|381.42095686663316|\n",
      "|  270266090| 381.3091858037574|\n",
      "|  606497335| 380.2884948915997|\n",
      "| 1720938479| 379.8883515482675|\n",
      "|  997626433| 379.4370040204126|\n",
      "| 1445595477| 379.0481333504646|\n",
      "|  143327090| 379.0115743380849|\n",
      "|  771066397| 378.9928698752224|\n",
      "+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.groupBy(\"retailer_id\").agg(avg(\"amount\").alias(\"avg_amount\")).orderBy(desc(\"avg_amount\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+-----------+--------------------+-------+----------+-------------------+\n",
      "|      transacted_at|    trx_id|retailer_id|         description| amount|   city_id|retailer_avg_amount|\n",
      "+-------------------+----------+-----------+--------------------+-------+----------+-------------------+\n",
      "|2017-01-01 19:00:00|1853124918|  582210968|Family Dollar Sto...|  757.1| 637093548| 367.40095518419776|\n",
      "|2017-01-01 19:00:00|1853101949|  914585647|Whole Foods Marke...|    3.5| 903387909| 356.07879924407587|\n",
      "|2017-01-01 19:00:00|1853158553|  582210968|Family Dollar Sto...| 1975.2|1912579202| 367.40095518419776|\n",
      "|2017-01-01 19:00:00|1853160484|  304276488|Belk    arc id: 6...|   3.67| 333864585| 381.42095686663316|\n",
      "|2017-01-01 19:00:00|1852974205|  386167994|unkn     ppd id: ...|1309.26| 352952442|   383.733645990921|\n",
      "|2017-01-01 19:00:00|1853035514|  386167994|Wendy's  ppd id: ...| 207.15|1462628288|   383.733645990921|\n",
      "|2017-01-01 19:00:00|1853039330|  304276488|                Belk|  677.6|1107275933| 381.42095686663316|\n",
      "|2017-01-01 19:00:00|1852974072|  582210968|unkn  arc id: 276422|  17.38|1744912105| 367.40095518419776|\n",
      "|2017-01-01 19:00:00|1852978849|  582210968|                unkn|  85.32| 576697624| 367.40095518419776|\n",
      "|2017-01-01 19:00:00|1853068258|  582210968|Family Dollar Sto...|  78.19|1487919653| 367.40095518419776|\n",
      "|2017-01-01 19:00:00|1853130659|  386167994|             Wendy's| 493.15| 455476705|   383.733645990921|\n",
      "|2017-01-01 19:00:00|1852975969|  914585647|Whole Foods Marke...|2535.37| 559832710| 356.07879924407587|\n",
      "|2017-01-01 19:00:00|1852978792|  914585647|unkn    ccd id: 4...|2839.85|1790189812| 356.07879924407587|\n",
      "|2017-01-01 19:00:00|1853101863|  914585647|Whole Foods Marke...|   35.9| 218089872| 356.07879924407587|\n",
      "|2017-01-01 19:00:00|1853004052| 1817581369|Dollar Tree  ccd ...|  16.86| 129711554| 373.63467544910094|\n",
      "|2017-01-01 19:00:00|1853035708| 1817581369|Dollar Tree  ccd ...|1810.34|  28424447| 373.63467544910094|\n",
      "|2017-01-01 19:00:00|1853042275| 1817581369|Dollar Tree    As...| 447.83| 957346984| 373.63467544910094|\n",
      "|2017-01-01 19:00:00|1853072188| 1817581369|Dollar Tree    pp...|   47.7|1609326953| 373.63467544910094|\n",
      "|2017-01-01 19:00:00| 475421147| 1888067226|Neiman Marcus    ...|   4.58| 637093548| 362.73621087314586|\n",
      "|2017-01-01 19:00:00|1853004829| 1888067226|unkn  arc id: 602947|  13.16|1655945659| 362.73621087314586|\n",
      "+-------------------+----------+-----------+--------------------+-------+----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "window = Window.partitionBy(\"retailer_id\")\n",
    "avg_amount = avg(\"amount\").over(window)\n",
    "\n",
    "sales.withColumn(\"retailer_avg_amount\", avg_amount).orderBy(asc(\"transacted_at\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- contact: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_line_items: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- amount: double (nullable = true)\n",
      " |    |    |-- item_id: string (nullable = true)\n",
      " |    |    |-- qty: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading a single line JSON file\n",
    "\n",
    "spark.read.json(\"data/order_singleline.json\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- contact: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_line_items: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- amount: double (nullable = true)\n",
      " |    |    |-- item_id: string (nullable = true)\n",
      " |    |    |-- qty: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading a multi-line JSON file\n",
    "\n",
    "spark.read.json(\"data/order_multiline.json\", multiLine=True).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------+--------------------+\n",
      "|             contact|customer_id|order_id|    order_line_items|\n",
      "+--------------------+-----------+--------+--------------------+\n",
      "|[9000010000, 9000...|       C001|    O101|[{102.45, I001, 6...|\n",
      "+--------------------+-----------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading and storing a single-line JSON file\n",
    "\n",
    "order = spark.read.json(\"data/order_singleline.json\")\n",
    "order.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- contact: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_line_items: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- amount: double (nullable = true)\n",
      " |    |    |-- item_id: string (nullable = true)\n",
      " |    |    |-- qty: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Defining a complex schema for the JSON file\n",
    "\n",
    "complex_schema = \"contact array<string>, customer_id string, order_id string, order_line_items array<struct<amount double, item_id string, qty long>>\"\n",
    "\n",
    "spark.read.json(\"data/order_singleline.json\", schema=complex_schema).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"order_id\":\"O101...|\n",
      "+--------------------+\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|               value|              parsed|\n",
      "+--------------------+--------------------+\n",
      "|{\"order_id\":\"O101...|{[9000010000, 900...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# String to JSON using from_json function, with a schema\n",
    "\n",
    "json_str = spark.read.text(\"data/order_singleline.json\")\n",
    "json_str.show()\n",
    "\n",
    "json_expanded = json_str.withColumn(\"parsed\", from_json(json_str.value, complex_schema))\n",
    "json_expanded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|unparsed                                                                                                                                                                               |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|{\"contact\":[\"9000010000\",\"9000010001\"],\"customer_id\":\"C001\",\"order_id\":\"O101\",\"order_line_items\":[{\"amount\":102.45,\"item_id\":\"I001\",\"qty\":6},{\"amount\":2.01,\"item_id\":\"I003\",\"qty\":2}]}|\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# JSON to String using to_json\n",
    "\n",
    "json_expanded.withColumn(\"unparsed\", to_json(json_expanded.parsed)).select(\"unparsed\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------+\n",
      "|parsed                                                                      |\n",
      "+----------------------------------------------------------------------------+\n",
      "|{[9000010000, 9000010001], C001, O101, [{102.45, I001, 6}, {2.01, I003, 2}]}|\n",
      "+----------------------------------------------------------------------------+\n",
      "\n",
      "+------------------------+-----------+--------+------------------------------------+\n",
      "|contact                 |customer_id|order_id|order_line_items                    |\n",
      "+------------------------+-----------+--------+------------------------------------+\n",
      "|[9000010000, 9000010001]|C001       |O101    |[{102.45, I001, 6}, {2.01, I003, 2}]|\n",
      "+------------------------+-----------+--------+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract/Expand struct/dict data in JSON\n",
    "\n",
    "json_expanded.select(\"parsed\").show(truncate=False)\n",
    "\n",
    "json_expanded.select(\"parsed.*\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------+--------------------+-----------------+\n",
      "|             contact|customer_id|order_id|    order_line_items|         exploded|\n",
      "+--------------------+-----------+--------+--------------------+-----------------+\n",
      "|[9000010000, 9000...|       C001|    O101|[{102.45, I001, 6...|{102.45, I001, 6}|\n",
      "|[9000010000, 9000...|       C001|    O101|[{102.45, I001, 6...|  {2.01, I003, 2}|\n",
      "+--------------------+-----------+--------+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_expanded.select(\"parsed.*\").withColumn(\"exploded\", explode(\"order_line_items\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://a9106065823c:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Writing Data</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fd7c2b50220>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Writing Data\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the available number of cores\n",
    "\n",
    "spark.sparkContext.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: string (nullable = true)\n",
      " |-- department_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      " |-- hire_date: string (nullable = true)\n",
      "\n",
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "|employee_id|department_id|         name|age|gender|salary| hire_date|\n",
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|\n",
      "|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|\n",
      "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|\n",
      "|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|\n",
      "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|\n",
      "|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|\n",
      "|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|\n",
      "|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|\n",
      "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|\n",
      "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|\n",
      "|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|\n",
      "|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|\n",
      "|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|\n",
      "|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|\n",
      "|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|\n",
      "|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|\n",
      "|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|\n",
      "|        018|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|\n",
      "|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|\n",
      "|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|\n",
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp = spark.read.csv(\"data/emp.csv\", header=True)\n",
    "\n",
    "emp.printSchema()\n",
    "emp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save/Write the data in parquet format\n",
    "\n",
    "emp.write.parquet(\"data/output/emp.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+------------+\n",
      "|employee_id|department_id|         name|age|gender|salary| hire_date|partition_id|\n",
      "+-----------+-------------+-------------+---+------+------+----------+------------+\n",
      "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|           0|\n",
      "|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|           0|\n",
      "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|           0|\n",
      "|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|           0|\n",
      "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|           0|\n",
      "|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|           0|\n",
      "|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|           0|\n",
      "|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|           0|\n",
      "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|           0|\n",
      "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|           0|\n",
      "|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|           0|\n",
      "|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|           0|\n",
      "|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|           0|\n",
      "|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|           0|\n",
      "|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|           0|\n",
      "|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|           0|\n",
      "|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|           0|\n",
      "|        018|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|           0|\n",
      "|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|           0|\n",
      "|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|           0|\n",
      "+-----------+-------------+-------------+---+------+------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.withColumn(\"partition_id\", spark_partition_id()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the file wrt partitions\n",
    "# ! Write Modes: Append, Overwrite, Ignore, Error\n",
    "\n",
    "emp.write.format(\"csv\").partitionBy(\"department_id\").option(\"header\", True).save(\"data/output/emp.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Understanding Clusters](https://youtu.be/5RosqOeJrrs?t=8510)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Use the docker compose file to setup/deploy required containers\n",
    "- Execute command from docker container, from the `/spark` dir\n",
    "\n",
    "    - <pre>./bin/spark-submit --master spark://78962bfc976e:7077 <b>[Replace with master server address]</b> --num-executors 3 --executor-cores 2 </br>--executor-memory 512M /data/12_understand_cluster.py</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.extraJavaOptions',\n",
       "  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n",
       " ('spark.app.startTime', '1746507577605'),\n",
       " ('spark.executor.instances', '4'),\n",
       " ('spark.exectuor.memory', '512M'),\n",
       " ('spark.app.name', 'Cluster Execution'),\n",
       " ('spark.master', 'spark://78962bfc976e:7077'),\n",
       " ('spark.app.submitTime', '1746503659432'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.sql.warehouse.dir', 'file:/home/jupyter/spark-warehouse'),\n",
       " ('spark.executor.cores', '4'),\n",
       " ('spark.driver.host', 'ae2e94ccea8e'),\n",
       " ('spark.app.id', 'app-20250506045937-0007'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.driver.port', '44573'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Cluster Execution\")\n",
    "    .master(\"spark://78962bfc976e:7077\")\n",
    "    .config(\"spark.executor.instances\", 4)\n",
    "    .config(\"spark.executor.cores\", 4)\n",
    "    .config(\"spark.exectuor.memory\", \"512M\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.range(10).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-Defined Functions (UDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Slow since python requires Serializing/De-Serailizing data.\n",
    "- Can be mitigated by\n",
    "    - Using in-built higher-order functions\n",
    "    - Writing the functions in Java or Scala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://a4bb918641d2:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://47761edfd1ef:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>UDF</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe61f64c6a0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"UDF\")\n",
    "    .master(\"spark://47761edfd1ef:7077\")\n",
    "    .config(\"spark.executor.cores\", 2)\n",
    "    .config(\"spark.cores.max\", 6)\n",
    "    .config(\"spark.executor.memory\", \"512M\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "|employee_id|department_id|         name|age|gender|salary| hire_date|\n",
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|\n",
      "|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|\n",
      "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|\n",
      "|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|\n",
      "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|\n",
      "|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|\n",
      "|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|\n",
      "|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|\n",
      "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|\n",
      "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|\n",
      "|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|\n",
      "|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|\n",
      "|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|\n",
      "|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|\n",
      "|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|\n",
      "|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|\n",
      "|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|\n",
      "|        018|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|\n",
      "|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|\n",
      "|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|\n",
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp = spark.read.csv(\"/data/emp.csv\", header=True)\n",
    "\n",
    "emp.show()\n",
    "emp.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDF\n",
    "\n",
    "def bonus(salary):\n",
    "    return int(salary) * 0.1\n",
    "\n",
    "bonus_udf = udf(bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o92.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5) (172.18.0.6 executor 1): java.io.IOException: Cannot run program \"/usr/local/bin/python\": error=2, No such file or directory\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:216)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:134)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:81)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:130)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: error=2, No such file or directory\n\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:247)\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:134)\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\n\t... 26 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.io.IOException: Cannot run program \"/usr/local/bin/python\": error=2, No such file or directory\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:216)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:134)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:81)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:130)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: error=2, No such file or directory\n\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:247)\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:134)\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\n\t... 26 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43memp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbonus\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbonus_udf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msalary\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/spark/python/pyspark/sql/dataframe.py:606\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a bool\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 606\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/spark/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o92.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5) (172.18.0.6 executor 1): java.io.IOException: Cannot run program \"/usr/local/bin/python\": error=2, No such file or directory\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:216)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:134)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:81)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:130)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: error=2, No such file or directory\n\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:247)\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:134)\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\n\t... 26 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.io.IOException: Cannot run program \"/usr/local/bin/python\": error=2, No such file or directory\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:216)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:134)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:81)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:130)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: error=2, No such file or directory\n\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:247)\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:134)\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\n\t... 26 more\n"
     ]
    }
   ],
   "source": [
    "emp.withColumn('bonus', bonus_udf(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sparkContext.getConf().getAll()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
