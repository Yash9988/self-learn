{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Spark Tutorial](https://www.youtube.com/watch?v=5RosqOeJrrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# `local[*]` means use all available cores on the local machine.\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"spark-intro\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://3e6699c11a7b:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark-intro</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fc63f7ecc50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the number of driver cores.\n",
    "\n",
    "spark.sparkContext.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|employee_id|department_id|         name|age|gender|salary|          hire_date|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01 00:00:00|\n",
      "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15 00:00:00|\n",
      "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01 00:00:00|\n",
      "|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30 00:00:00|\n",
      "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01 00:00:00|\n",
      "|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01 00:00:00|\n",
      "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15 00:00:00|\n",
      "|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01 00:00:00|\n",
      "|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01 00:00:00|\n",
      "|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01 00:00:00|\n",
      "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01 00:00:00|\n",
      "|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15 00:00:00|\n",
      "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01 00:00:00|\n",
      "|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01 00:00:00|\n",
      "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30 00:00:00|\n",
      "|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01 00:00:00|\n",
      "|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15 00:00:00|\n",
      "|         18|          104|    Nancy Liu| 29|Female| 50000|2017-06-01 00:00:00|\n",
      "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01 00:00:00|\n",
      "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01 00:00:00|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "\n",
      "Number of rows in the Employees DataFrame: 20\n",
      "Number of partitions in the Employees DataFrame: 1\n"
     ]
    }
   ],
   "source": [
    "# Read csv into dataframe\n",
    "\n",
    "emp = spark.read.csv('data/emp.csv', header=True, inferSchema=True)\n",
    "emp.show()\n",
    "\n",
    "print(f\"Number of rows in the Employees DataFrame: {emp.count()}\")\n",
    "print(f\"Number of partitions in the Employees DataFrame: {emp.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- department_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- hire_date: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema of the dataframe\n",
    "\n",
    "emp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Increase the number of partitions to 10\n",
    "# (use `coalesce()` to decrease the number of partitions)\n",
    "\n",
    "emp_re = emp.repartition(10)\n",
    "emp_re.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|employee_id|department_id|         name|age|gender|salary|          hire_date|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15 00:00:00|\n",
      "|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15 00:00:00|\n",
      "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01 00:00:00|\n",
      "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01 00:00:00|\n",
      "|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01 00:00:00|\n",
      "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30 00:00:00|\n",
      "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01 00:00:00|\n",
      "|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01 00:00:00|\n",
      "|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15 00:00:00|\n",
      "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01 00:00:00|\n",
      "|         18|          104|    Nancy Liu| 29|Female| 50000|2017-06-01 00:00:00|\n",
      "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15 00:00:00|\n",
      "|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01 00:00:00|\n",
      "|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01 00:00:00|\n",
      "|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01 00:00:00|\n",
      "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01 00:00:00|\n",
      "|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01 00:00:00|\n",
      "|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01 00:00:00|\n",
      "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01 00:00:00|\n",
      "|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30 00:00:00|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "\n",
      "Number of rows in the Employees (Repartitioned) DataFrame: 20\n",
      "Number of partitions in the Employees (Repartitioned) DataFrame: 10\n"
     ]
    }
   ],
   "source": [
    "emp_re.show()\n",
    "\n",
    "print(f\"Number of rows in the Employees (Repartitioned) DataFrame: {emp_re.count()}\")\n",
    "print(f\"Number of partitions in the Employees (Repartitioned) DataFrame: {emp_re.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the Cities-DataFrame: 2349391\n",
      "Number of partitions in the Cities-DataFrame: 19\n"
     ]
    }
   ],
   "source": [
    "# Load another (much larger) csv into a new dataframe and check its properties.\n",
    "\n",
    "cities = spark.read.csv('data/cities.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Print the rows and partitions of the dataframe\n",
    "print(f\"Number of rows in the Cities-DataFrame: {cities.count()}\")\n",
    "print(f\"Number of partitions in the Cities-DataFrame: {cities.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Transformations 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('employee_id', IntegerType(), True), StructField('department_id', IntegerType(), True), StructField('name', StringType(), True), StructField('age', IntegerType(), True), StructField('gender', StringType(), True), StructField('salary', IntegerType(), True), StructField('hire_date', TimestampType(), True)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|employee_id|department_id|         name|age|gender|salary|          hire_date|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01 00:00:00|\n",
      "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15 00:00:00|\n",
      "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01 00:00:00|\n",
      "|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30 00:00:00|\n",
      "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01 00:00:00|\n",
      "|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01 00:00:00|\n",
      "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15 00:00:00|\n",
      "|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01 00:00:00|\n",
      "|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01 00:00:00|\n",
      "|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01 00:00:00|\n",
      "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01 00:00:00|\n",
      "|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15 00:00:00|\n",
      "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01 00:00:00|\n",
      "|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01 00:00:00|\n",
      "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30 00:00:00|\n",
      "|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01 00:00:00|\n",
      "|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15 00:00:00|\n",
      "|         18|          104|    Nancy Liu| 29|Female| 50000|2017-06-01 00:00:00|\n",
      "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01 00:00:00|\n",
      "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01 00:00:00|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('name', StringType(), True), StructField('age', IntegerType(), True)])\n",
      "StructType([StructField('name', StringType(), True), StructField('age', IntegerType(), True)])\n"
     ]
    }
   ],
   "source": [
    "# Creating a manual schema in Spark\n",
    "from pyspark.sql.types import _parse_datatype_string\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# ! IMPLICT INFERENCE\n",
    "# Spark can infer the schema from a string\n",
    "schema_string = \"name string, age int\"\n",
    "print(_parse_datatype_string(schema_string))\n",
    "\n",
    "# ! EXPLICIT INFERENCE\n",
    "# Template: StructType([StructField(name, dataType, nullable?)]) \n",
    "schema_spark = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "print(schema_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---+------+\n",
      "|employee_id|         name|age|salary|\n",
      "+-----------+-------------+---+------+\n",
      "|          1|     John Doe| 30| 50000|\n",
      "|          2|   Jane Smith| 25| 45000|\n",
      "|          3|    Bob Brown| 35| 55000|\n",
      "|          4|    Alice Lee| 28| 48000|\n",
      "|          5|    Jack Chan| 40| 60000|\n",
      "|          6|    Jill Wong| 32| 52000|\n",
      "|          7|James Johnson| 42| 70000|\n",
      "|          8|     Kate Kim| 29| 51000|\n",
      "|          9|      Tom Tan| 33| 58000|\n",
      "|         10|     Lisa Lee| 27| 47000|\n",
      "|         11|   David Park| 38| 65000|\n",
      "|         12|   Susan Chen| 31| 54000|\n",
      "|         13|    Brian Kim| 45| 75000|\n",
      "|         14|    Emily Lee| 26| 46000|\n",
      "|         15|  Michael Lee| 37| 63000|\n",
      "|         16|  Kelly Zhang| 30| 49000|\n",
      "|         17|  George Wang| 34| 57000|\n",
      "|         18|    Nancy Liu| 29| 50000|\n",
      "|         19|  Steven Chen| 36| 62000|\n",
      "|         20|    Grace Kim| 32| 53000|\n",
      "+-----------+-------------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Columns and Expressions\n",
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "# ? col(\"name\") == expr(\"name\"), since both are Column objects and hence treated as same.\n",
    "# select employee_id, name, age, salary from emp\n",
    "\n",
    "emp_filtered = emp.select(col(\"employee_id\"), expr(\"name\"), emp.age, emp.salary)    # ! TRANSFORMATION\n",
    "emp_filtered.show() # ! ACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+---+------+\n",
      "|emp_id|         name|age|salary|\n",
      "+------+-------------+---+------+\n",
      "|     1|     John Doe| 30| 50000|\n",
      "|     2|   Jane Smith| 25| 45000|\n",
      "|     3|    Bob Brown| 35| 55000|\n",
      "|     4|    Alice Lee| 28| 48000|\n",
      "|     5|    Jack Chan| 40| 60000|\n",
      "|     6|    Jill Wong| 32| 52000|\n",
      "|     7|James Johnson| 42| 70000|\n",
      "|     8|     Kate Kim| 29| 51000|\n",
      "|     9|      Tom Tan| 33| 58000|\n",
      "|    10|     Lisa Lee| 27| 47000|\n",
      "|    11|   David Park| 38| 65000|\n",
      "|    12|   Susan Chen| 31| 54000|\n",
      "|    13|    Brian Kim| 45| 75000|\n",
      "|    14|    Emily Lee| 26| 46000|\n",
      "|    15|  Michael Lee| 37| 63000|\n",
      "|    16|  Kelly Zhang| 30| 49000|\n",
      "|    17|  George Wang| 34| 57000|\n",
      "|    18|    Nancy Liu| 29| 50000|\n",
      "|    19|  Steven Chen| 36| 62000|\n",
      "|    20|    Grace Kim| 32| 53000|\n",
      "+------+-------------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_casted = emp_filtered.select(expr(\"employee_id as emp_id\"), emp_filtered.name, expr(\"cast(age as int) as age\"), emp_filtered.salary)\n",
    "emp_casted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_casted.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+---+------+\n",
      "|emp_id|         name|age|salary|\n",
      "+------+-------------+---+------+\n",
      "|     1|     John Doe| 30| 50000|\n",
      "|     2|   Jane Smith| 25| 45000|\n",
      "|     3|    Bob Brown| 35| 55000|\n",
      "|     4|    Alice Lee| 28| 48000|\n",
      "|     5|    Jack Chan| 40| 60000|\n",
      "|     6|    Jill Wong| 32| 52000|\n",
      "|     7|James Johnson| 42| 70000|\n",
      "|     8|     Kate Kim| 29| 51000|\n",
      "|     9|      Tom Tan| 33| 58000|\n",
      "|    10|     Lisa Lee| 27| 47000|\n",
      "|    11|   David Park| 38| 65000|\n",
      "|    12|   Susan Chen| 31| 54000|\n",
      "|    13|    Brian Kim| 45| 75000|\n",
      "|    14|    Emily Lee| 26| 46000|\n",
      "|    15|  Michael Lee| 37| 63000|\n",
      "|    16|  Kelly Zhang| 30| 49000|\n",
      "|    17|  George Wang| 34| 57000|\n",
      "|    18|    Nancy Liu| 29| 50000|\n",
      "|    19|  Steven Chen| 36| 62000|\n",
      "|    20|    Grace Kim| 32| 53000|\n",
      "+------+-------------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_casted_alt = emp_filtered.selectExpr(\"employee_id as emp_id\", \"name\", \"cast(age as int) as age\", \"salary\")\n",
    "emp_casted_alt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+---+------+\n",
      "|emp_id|         name|age|salary|\n",
      "+------+-------------+---+------+\n",
      "|     3|    Bob Brown| 35| 55000|\n",
      "|     5|    Jack Chan| 40| 60000|\n",
      "|     6|    Jill Wong| 32| 52000|\n",
      "|     7|James Johnson| 42| 70000|\n",
      "|     9|      Tom Tan| 33| 58000|\n",
      "|    11|   David Park| 38| 65000|\n",
      "|    12|   Susan Chen| 31| 54000|\n",
      "|    13|    Brian Kim| 45| 75000|\n",
      "|    15|  Michael Lee| 37| 63000|\n",
      "|    17|  George Wang| 34| 57000|\n",
      "|    19|  Steven Chen| 36| 62000|\n",
      "|    20|    Grace Kim| 32| 53000|\n",
      "+------+-------------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter emp_casted based on Age > 30\n",
    "\n",
    "emp_casted.select(\"emp_id\", \"name\", \"age\", \"salary\").where(\"age > 30\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Transformations 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|employee_id|department_id|         name|age|gender|salary|          hire_date|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01 00:00:00|\n",
      "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15 00:00:00|\n",
      "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01 00:00:00|\n",
      "|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30 00:00:00|\n",
      "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01 00:00:00|\n",
      "|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01 00:00:00|\n",
      "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15 00:00:00|\n",
      "|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01 00:00:00|\n",
      "|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01 00:00:00|\n",
      "|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01 00:00:00|\n",
      "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01 00:00:00|\n",
      "|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15 00:00:00|\n",
      "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01 00:00:00|\n",
      "|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01 00:00:00|\n",
      "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30 00:00:00|\n",
      "|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01 00:00:00|\n",
      "|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15 00:00:00|\n",
      "|         18|          104|    Nancy Liu| 29|Female| 50000|2017-06-01 00:00:00|\n",
      "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01 00:00:00|\n",
      "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01 00:00:00|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- department_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- hire_date: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, cast\n",
    "\n",
    "emp.select(\"employee_id\", \"name\", \"age\", col(\"salary\").cast(\"double\")).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---+-------+-------+\n",
      "|employee_id|         name|age| salary|    tax|\n",
      "+-----------+-------------+---+-------+-------+\n",
      "|          1|     John Doe| 30|50000.0|10000.0|\n",
      "|          2|   Jane Smith| 25|45000.0| 9000.0|\n",
      "|          3|    Bob Brown| 35|55000.0|11000.0|\n",
      "|          4|    Alice Lee| 28|48000.0| 9600.0|\n",
      "|          5|    Jack Chan| 40|60000.0|12000.0|\n",
      "|          6|    Jill Wong| 32|52000.0|10400.0|\n",
      "|          7|James Johnson| 42|70000.0|14000.0|\n",
      "|          8|     Kate Kim| 29|51000.0|10200.0|\n",
      "|          9|      Tom Tan| 33|58000.0|11600.0|\n",
      "|         10|     Lisa Lee| 27|47000.0| 9400.0|\n",
      "|         11|   David Park| 38|65000.0|13000.0|\n",
      "|         12|   Susan Chen| 31|54000.0|10800.0|\n",
      "|         13|    Brian Kim| 45|75000.0|15000.0|\n",
      "|         14|    Emily Lee| 26|46000.0| 9200.0|\n",
      "|         15|  Michael Lee| 37|63000.0|12600.0|\n",
      "|         16|  Kelly Zhang| 30|49000.0| 9800.0|\n",
      "|         17|  George Wang| 34|57000.0|11400.0|\n",
      "|         18|    Nancy Liu| 29|50000.0|10000.0|\n",
      "|         19|  Steven Chen| 36|62000.0|12400.0|\n",
      "|         20|    Grace Kim| 32|53000.0|10600.0|\n",
      "+-----------+-------------+---+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adding new columns to the DataFrame\n",
    "\n",
    "emp_casted = emp.select(\"employee_id\", \"name\", \"age\", col(\"salary\").cast(\"double\"))\n",
    "\n",
    "emp_taxed = emp_casted.withColumn(\"tax\", col(\"salary\") * 0.2)\n",
    "emp_taxed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---+-------+-------+---------+---------+\n",
      "|employee_id|         name|age| salary|    tax|columnOne|columnTwo|\n",
      "+-----------+-------------+---+-------+-------+---------+---------+\n",
      "|          1|     John Doe| 30|50000.0|10000.0|        1|      two|\n",
      "|          2|   Jane Smith| 25|45000.0| 9000.0|        1|      two|\n",
      "|          3|    Bob Brown| 35|55000.0|11000.0|        1|      two|\n",
      "|          4|    Alice Lee| 28|48000.0| 9600.0|        1|      two|\n",
      "|          5|    Jack Chan| 40|60000.0|12000.0|        1|      two|\n",
      "|          6|    Jill Wong| 32|52000.0|10400.0|        1|      two|\n",
      "|          7|James Johnson| 42|70000.0|14000.0|        1|      two|\n",
      "|          8|     Kate Kim| 29|51000.0|10200.0|        1|      two|\n",
      "|          9|      Tom Tan| 33|58000.0|11600.0|        1|      two|\n",
      "|         10|     Lisa Lee| 27|47000.0| 9400.0|        1|      two|\n",
      "|         11|   David Park| 38|65000.0|13000.0|        1|      two|\n",
      "|         12|   Susan Chen| 31|54000.0|10800.0|        1|      two|\n",
      "|         13|    Brian Kim| 45|75000.0|15000.0|        1|      two|\n",
      "|         14|    Emily Lee| 26|46000.0| 9200.0|        1|      two|\n",
      "|         15|  Michael Lee| 37|63000.0|12600.0|        1|      two|\n",
      "|         16|  Kelly Zhang| 30|49000.0| 9800.0|        1|      two|\n",
      "|         17|  George Wang| 34|57000.0|11400.0|        1|      two|\n",
      "|         18|    Nancy Liu| 29|50000.0|10000.0|        1|      two|\n",
      "|         19|  Steven Chen| 36|62000.0|12400.0|        1|      two|\n",
      "|         20|    Grace Kim| 32|53000.0|10600.0|        1|      two|\n",
      "+-----------+-------------+---+-------+-------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Literals (Adding a constant to the DataFrame)\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "emp_new_cols = emp_taxed.withColumn(\"columnOne\", lit(1)).withColumn(\"columnTwo\", lit(\"two\"))\n",
    "emp_new_cols.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+---+-------+-------+---------+---------+\n",
      "|emp_id|         name|age| salary|    tax|columnOne|columnTwo|\n",
      "+------+-------------+---+-------+-------+---------+---------+\n",
      "|     1|     John Doe| 30|50000.0|10000.0|        1|      two|\n",
      "|     2|   Jane Smith| 25|45000.0| 9000.0|        1|      two|\n",
      "|     3|    Bob Brown| 35|55000.0|11000.0|        1|      two|\n",
      "|     4|    Alice Lee| 28|48000.0| 9600.0|        1|      two|\n",
      "|     5|    Jack Chan| 40|60000.0|12000.0|        1|      two|\n",
      "|     6|    Jill Wong| 32|52000.0|10400.0|        1|      two|\n",
      "|     7|James Johnson| 42|70000.0|14000.0|        1|      two|\n",
      "|     8|     Kate Kim| 29|51000.0|10200.0|        1|      two|\n",
      "|     9|      Tom Tan| 33|58000.0|11600.0|        1|      two|\n",
      "|    10|     Lisa Lee| 27|47000.0| 9400.0|        1|      two|\n",
      "|    11|   David Park| 38|65000.0|13000.0|        1|      two|\n",
      "|    12|   Susan Chen| 31|54000.0|10800.0|        1|      two|\n",
      "|    13|    Brian Kim| 45|75000.0|15000.0|        1|      two|\n",
      "|    14|    Emily Lee| 26|46000.0| 9200.0|        1|      two|\n",
      "|    15|  Michael Lee| 37|63000.0|12600.0|        1|      two|\n",
      "|    16|  Kelly Zhang| 30|49000.0| 9800.0|        1|      two|\n",
      "|    17|  George Wang| 34|57000.0|11400.0|        1|      two|\n",
      "|    18|    Nancy Liu| 29|50000.0|10000.0|        1|      two|\n",
      "|    19|  Steven Chen| 36|62000.0|12400.0|        1|      two|\n",
      "|    20|    Grace Kim| 32|53000.0|10600.0|        1|      two|\n",
      "+------+-------------+---+-------+-------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_new_cols.withColumnRenamed(\"employee_id\", \"emp_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---+-------+-------+---------+\n",
      "|employee_id|         name|age| salary|    tax|columnOne|\n",
      "+-----------+-------------+---+-------+-------+---------+\n",
      "|          1|     John Doe| 30|50000.0|10000.0|        1|\n",
      "|          2|   Jane Smith| 25|45000.0| 9000.0|        1|\n",
      "|          3|    Bob Brown| 35|55000.0|11000.0|        1|\n",
      "|          4|    Alice Lee| 28|48000.0| 9600.0|        1|\n",
      "|          5|    Jack Chan| 40|60000.0|12000.0|        1|\n",
      "|          6|    Jill Wong| 32|52000.0|10400.0|        1|\n",
      "|          7|James Johnson| 42|70000.0|14000.0|        1|\n",
      "|          8|     Kate Kim| 29|51000.0|10200.0|        1|\n",
      "|          9|      Tom Tan| 33|58000.0|11600.0|        1|\n",
      "|         10|     Lisa Lee| 27|47000.0| 9400.0|        1|\n",
      "|         11|   David Park| 38|65000.0|13000.0|        1|\n",
      "|         12|   Susan Chen| 31|54000.0|10800.0|        1|\n",
      "|         13|    Brian Kim| 45|75000.0|15000.0|        1|\n",
      "|         14|    Emily Lee| 26|46000.0| 9200.0|        1|\n",
      "|         15|  Michael Lee| 37|63000.0|12600.0|        1|\n",
      "|         16|  Kelly Zhang| 30|49000.0| 9800.0|        1|\n",
      "|         17|  George Wang| 34|57000.0|11400.0|        1|\n",
      "|         18|    Nancy Liu| 29|50000.0|10000.0|        1|\n",
      "|         19|  Steven Chen| 36|62000.0|12400.0|        1|\n",
      "|         20|    Grace Kim| 32|53000.0|10600.0|        1|\n",
      "+-----------+-------------+---+-------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dropping columns from the DataFrame\n",
    "\n",
    "emp_new_cols.drop(\"columnTwo\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---+-------+-------+---------+---------+\n",
      "|employee_id|         name|age| salary|    tax|columnOne|columnTwo|\n",
      "+-----------+-------------+---+-------+-------+---------+---------+\n",
      "|          1|     John Doe| 30|50000.0|10000.0|        1|      two|\n",
      "|          2|   Jane Smith| 25|45000.0| 9000.0|        1|      two|\n",
      "|          3|    Bob Brown| 35|55000.0|11000.0|        1|      two|\n",
      "|          4|    Alice Lee| 28|48000.0| 9600.0|        1|      two|\n",
      "|          5|    Jack Chan| 40|60000.0|12000.0|        1|      two|\n",
      "|          6|    Jill Wong| 32|52000.0|10400.0|        1|      two|\n",
      "|          7|James Johnson| 42|70000.0|14000.0|        1|      two|\n",
      "|          8|     Kate Kim| 29|51000.0|10200.0|        1|      two|\n",
      "|          9|      Tom Tan| 33|58000.0|11600.0|        1|      two|\n",
      "|         10|     Lisa Lee| 27|47000.0| 9400.0|        1|      two|\n",
      "|         11|   David Park| 38|65000.0|13000.0|        1|      two|\n",
      "|         12|   Susan Chen| 31|54000.0|10800.0|        1|      two|\n",
      "|         13|    Brian Kim| 45|75000.0|15000.0|        1|      two|\n",
      "|         14|    Emily Lee| 26|46000.0| 9200.0|        1|      two|\n",
      "|         15|  Michael Lee| 37|63000.0|12600.0|        1|      two|\n",
      "|         16|  Kelly Zhang| 30|49000.0| 9800.0|        1|      two|\n",
      "|         17|  George Wang| 34|57000.0|11400.0|        1|      two|\n",
      "|         18|    Nancy Liu| 29|50000.0|10000.0|        1|      two|\n",
      "|         19|  Steven Chen| 36|62000.0|12400.0|        1|      two|\n",
      "|         20|    Grace Kim| 32|53000.0|10600.0|        1|      two|\n",
      "+-----------+-------------+---+-------+-------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_new_cols.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---+-------+-------+\n",
      "|employee_id|         name|age| salary|    tax|\n",
      "+-----------+-------------+---+-------+-------+\n",
      "|          3|    Bob Brown| 35|55000.0|11000.0|\n",
      "|          5|    Jack Chan| 40|60000.0|12000.0|\n",
      "|          6|    Jill Wong| 32|52000.0|10400.0|\n",
      "|          7|James Johnson| 42|70000.0|14000.0|\n",
      "|          8|     Kate Kim| 29|51000.0|10200.0|\n",
      "+-----------+-------------+---+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter DataFrame where tax > 10000, along with LIMIT to 5 rows\n",
    "\n",
    "emp_taxed.where(\"tax > 10000\").limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---+-------+-------+------+\n",
      "|employee_id|         name|age| salary|    tax| bonus|\n",
      "+-----------+-------------+---+-------+-------+------+\n",
      "|          1|     John Doe| 30|50000.0|10000.0|5000.0|\n",
      "|          2|   Jane Smith| 25|45000.0| 9000.0|4500.0|\n",
      "|          3|    Bob Brown| 35|55000.0|11000.0|5500.0|\n",
      "|          4|    Alice Lee| 28|48000.0| 9600.0|4800.0|\n",
      "|          5|    Jack Chan| 40|60000.0|12000.0|6000.0|\n",
      "|          6|    Jill Wong| 32|52000.0|10400.0|5200.0|\n",
      "|          7|James Johnson| 42|70000.0|14000.0|7000.0|\n",
      "|          8|     Kate Kim| 29|51000.0|10200.0|5100.0|\n",
      "|          9|      Tom Tan| 33|58000.0|11600.0|5800.0|\n",
      "|         10|     Lisa Lee| 27|47000.0| 9400.0|4700.0|\n",
      "|         11|   David Park| 38|65000.0|13000.0|6500.0|\n",
      "|         12|   Susan Chen| 31|54000.0|10800.0|5400.0|\n",
      "|         13|    Brian Kim| 45|75000.0|15000.0|7500.0|\n",
      "|         14|    Emily Lee| 26|46000.0| 9200.0|4600.0|\n",
      "|         15|  Michael Lee| 37|63000.0|12600.0|6300.0|\n",
      "|         16|  Kelly Zhang| 30|49000.0| 9800.0|4900.0|\n",
      "|         17|  George Wang| 34|57000.0|11400.0|5700.0|\n",
      "|         18|    Nancy Liu| 29|50000.0|10000.0|5000.0|\n",
      "|         19|  Steven Chen| 36|62000.0|12400.0|6200.0|\n",
      "|         20|    Grace Kim| 32|53000.0|10600.0|5300.0|\n",
      "+-----------+-------------+---+-------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Bonus: Adding multiple columns to the dataframe at once\n",
    "\n",
    "columns = {\n",
    "    'tax': col('salary') * 0.2,\n",
    "    'bonus': col('salary') * 0.1\n",
    "}\n",
    "\n",
    "emp_casted.withColumns(columns).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String and Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://3e6699c11a7b:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark-intro</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fc63f7ecc50>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"String & Dates\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+-------------------+----------+\n",
      "|employee_id|department_id|         name|age|gender|salary|          hire_date|new_gender|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+----------+\n",
      "|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01 00:00:00|         M|\n",
      "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15 00:00:00|         F|\n",
      "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01 00:00:00|         M|\n",
      "|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30 00:00:00|         F|\n",
      "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01 00:00:00|         M|\n",
      "|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01 00:00:00|         F|\n",
      "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15 00:00:00|         M|\n",
      "|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01 00:00:00|         F|\n",
      "|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01 00:00:00|         M|\n",
      "|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01 00:00:00|         F|\n",
      "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01 00:00:00|         M|\n",
      "|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15 00:00:00|         F|\n",
      "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01 00:00:00|         M|\n",
      "|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01 00:00:00|         F|\n",
      "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30 00:00:00|         M|\n",
      "|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01 00:00:00|         F|\n",
      "|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15 00:00:00|         M|\n",
      "|         18|          104|    Nancy Liu| 29|Female| 50000|2017-06-01 00:00:00|         F|\n",
      "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01 00:00:00|         M|\n",
      "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01 00:00:00|         F|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a \"Case\" column to the DataFrame based on conditions\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "emp.withColumn('new_gender', when(col('gender') == 'Male', 'M').when(col('gender') == 'Female', 'F').otherwise(None)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+-------------------+-------------+\n",
      "|employee_id|department_id|         name|age|gender|salary|          hire_date|     new_name|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+-------------+\n",
      "|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01 00:00:00|     Zohn Doe|\n",
      "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15 00:00:00|   Zane Smith|\n",
      "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01 00:00:00|    Bob Brown|\n",
      "|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30 00:00:00|    Alice Lee|\n",
      "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01 00:00:00|    Zack Chan|\n",
      "|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01 00:00:00|    Zill Wong|\n",
      "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15 00:00:00|Zames Zohnson|\n",
      "|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01 00:00:00|     Kate Kim|\n",
      "|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01 00:00:00|      Tom Tan|\n",
      "|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01 00:00:00|     Lisa Lee|\n",
      "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01 00:00:00|   David Park|\n",
      "|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15 00:00:00|   Susan Chen|\n",
      "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01 00:00:00|    Brian Kim|\n",
      "|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01 00:00:00|    Emily Lee|\n",
      "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30 00:00:00|  Michael Lee|\n",
      "|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01 00:00:00|  Kelly Zhang|\n",
      "|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15 00:00:00|  George Wang|\n",
      "|         18|          104|    Nancy Liu| 29|Female| 50000|2017-06-01 00:00:00|    Nancy Liu|\n",
      "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01 00:00:00|  Steven Chen|\n",
      "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01 00:00:00|    Grace Kim|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace in Strings\n",
    "\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "emp.withColumn(\"new_name\", regexp_replace(\"name\", \"J\", \"Z\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- department_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert Timestamp (String) type column to Date type\n",
    "\n",
    "from pyspark.sql.functions import to_date, col\n",
    "\n",
    "emp.withColumn(\"hire_date\", to_date(col(\"hire_date\"), 'yyyy-MM-dd')).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+-------------------+------------+--------------------------+\n",
      "|employee_id|department_id|name         |age|gender|salary|hire_date          |current_date|current_timestamp         |\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+------------+--------------------------+\n",
      "|1          |101          |John Doe     |30 |Male  |50000 |2015-01-01 00:00:00|2025-04-30  |2025-04-30 02:52:20.972595|\n",
      "|2          |101          |Jane Smith   |25 |Female|45000 |2016-02-15 00:00:00|2025-04-30  |2025-04-30 02:52:20.972595|\n",
      "|3          |102          |Bob Brown    |35 |Male  |55000 |2014-05-01 00:00:00|2025-04-30  |2025-04-30 02:52:20.972595|\n",
      "|4          |102          |Alice Lee    |28 |Female|48000 |2017-09-30 00:00:00|2025-04-30  |2025-04-30 02:52:20.972595|\n",
      "|5          |103          |Jack Chan    |40 |Male  |60000 |2013-04-01 00:00:00|2025-04-30  |2025-04-30 02:52:20.972595|\n",
      "|6          |103          |Jill Wong    |32 |Female|52000 |2018-07-01 00:00:00|2025-04-30  |2025-04-30 02:52:20.972595|\n",
      "|7          |101          |James Johnson|42 |Male  |70000 |2012-03-15 00:00:00|2025-04-30  |2025-04-30 02:52:20.972595|\n",
      "|8          |102          |Kate Kim     |29 |Female|51000 |2019-10-01 00:00:00|2025-04-30  |2025-04-30 02:52:20.972595|\n",
      "|9          |103          |Tom Tan      |33 |Male  |58000 |2016-06-01 00:00:00|2025-04-30  |2025-04-30 02:52:20.972595|\n",
      "|10         |104          |Lisa Lee     |27 |Female|47000 |2018-08-01 00:00:00|2025-04-30  |2025-04-30 02:52:20.972595|\n",
      "|11         |104          |David Park   |38 |Male  |65000 |2015-11-01 00:00:00|2025-04-30  |2025-04-30 02:52:20.972595|\n",
      "|12         |105          |Susan Chen   |31 |Female|54000 |2017-02-15 00:00:00|2025-04-30  |2025-04-30 02:52:20.972595|\n",
      "|13         |106          |Brian Kim    |45 |Male  |75000 |2011-07-01 00:00:00|2025-04-30  |2025-04-30 02:52:20.972595|\n",
      "|14         |107          |Emily Lee    |26 |Female|46000 |2019-01-01 00:00:00|2025-04-30  |2025-04-30 02:52:20.972595|\n",
      "|15         |106          |Michael Lee  |37 |Male  |63000 |2014-09-30 00:00:00|2025-04-30  |2025-04-30 02:52:20.972595|\n",
      "|16         |107          |Kelly Zhang  |30 |Female|49000 |2018-04-01 00:00:00|2025-04-30  |2025-04-30 02:52:20.972595|\n",
      "|17         |105          |George Wang  |34 |Male  |57000 |2016-03-15 00:00:00|2025-04-30  |2025-04-30 02:52:20.972595|\n",
      "|18         |104          |Nancy Liu    |29 |Female|50000 |2017-06-01 00:00:00|2025-04-30  |2025-04-30 02:52:20.972595|\n",
      "|19         |103          |Steven Chen  |36 |Male  |62000 |2015-08-01 00:00:00|2025-04-30  |2025-04-30 02:52:20.972595|\n",
      "|20         |102          |Grace Kim    |32 |Female|53000 |2018-11-01 00:00:00|2025-04-30  |2025-04-30 02:52:20.972595|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add Current Date and Timestamp columns\n",
    "\n",
    "from pyspark.sql.functions import current_date, current_timestamp\n",
    "\n",
    "columns = {\n",
    "    \"current_date\": current_date(),\n",
    "    \"current_timestamp\": current_timestamp()\n",
    "}\n",
    "\n",
    "emp.withColumns(columns).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|employee_id|department_id|         name|age|gender|salary|          hire_date|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01 00:00:00|\n",
      "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15 00:00:00|\n",
      "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01 00:00:00|\n",
      "|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30 00:00:00|\n",
      "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01 00:00:00|\n",
      "|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01 00:00:00|\n",
      "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15 00:00:00|\n",
      "|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01 00:00:00|\n",
      "|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01 00:00:00|\n",
      "|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01 00:00:00|\n",
      "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01 00:00:00|\n",
      "|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15 00:00:00|\n",
      "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01 00:00:00|\n",
      "|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01 00:00:00|\n",
      "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30 00:00:00|\n",
      "|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01 00:00:00|\n",
      "|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15 00:00:00|\n",
      "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01 00:00:00|\n",
      "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01 00:00:00|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with null values in a column\n",
    "\n",
    "temp = emp.withColumn(\"gender\", when(col(\"name\") == \"Nancy Liu\", None).otherwise(col(\"gender\")))\n",
    "temp.na.drop().show()\n",
    "\n",
    "del temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+-------+------+-------------------+\n",
      "|employee_id|department_id|         name|age| gender|salary|          hire_date|\n",
      "+-----------+-------------+-------------+---+-------+------+-------------------+\n",
      "|          1|          101|     John Doe| 30|   Male| 50000|2015-01-01 00:00:00|\n",
      "|          2|          101|   Jane Smith| 25| Female| 45000|2016-02-15 00:00:00|\n",
      "|          3|          102|    Bob Brown| 35|   Male| 55000|2014-05-01 00:00:00|\n",
      "|          4|          102|    Alice Lee| 28| Female| 48000|2017-09-30 00:00:00|\n",
      "|          5|          103|    Jack Chan| 40|   Male| 60000|2013-04-01 00:00:00|\n",
      "|          6|          103|    Jill Wong| 32| Female| 52000|2018-07-01 00:00:00|\n",
      "|          7|          101|James Johnson| 42|   Male| 70000|2012-03-15 00:00:00|\n",
      "|          8|          102|     Kate Kim| 29| Female| 51000|2019-10-01 00:00:00|\n",
      "|          9|          103|      Tom Tan| 33|   Male| 58000|2016-06-01 00:00:00|\n",
      "|         10|          104|     Lisa Lee| 27| Female| 47000|2018-08-01 00:00:00|\n",
      "|         11|          104|   David Park| 38|   Male| 65000|2015-11-01 00:00:00|\n",
      "|         12|          105|   Susan Chen| 31| Female| 54000|2017-02-15 00:00:00|\n",
      "|         13|          106|    Brian Kim| 45|   Male| 75000|2011-07-01 00:00:00|\n",
      "|         14|          107|    Emily Lee| 26| Female| 46000|2019-01-01 00:00:00|\n",
      "|         15|          106|  Michael Lee| 37|   Male| 63000|2014-09-30 00:00:00|\n",
      "|         16|          107|  Kelly Zhang| 30| Female| 49000|2018-04-01 00:00:00|\n",
      "|         17|          105|  George Wang| 34|   Male| 57000|2016-03-15 00:00:00|\n",
      "|         18|          104|    Nancy Liu| 29|Unknown| 50000|2017-06-01 00:00:00|\n",
      "|         19|          103|  Steven Chen| 36|   Male| 62000|2015-08-01 00:00:00|\n",
      "|         20|          102|    Grace Kim| 32| Female| 53000|2018-11-01 00:00:00|\n",
      "+-----------+-------------+-------------+---+-------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fix null values with coalesce\n",
    "from pyspark.sql.functions import coalesce, lit\n",
    "\n",
    "temp = emp.withColumn(\"gender\", when(col(\"name\") == \"Nancy Liu\", None).otherwise(col(\"gender\")))\n",
    "\n",
    "temp.withColumn(\"gender\", coalesce(col(\"gender\"), lit(\"Unknown\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+-------------------+---------+\n",
      "|employee_id|department_id|         name|age|gender|salary|          hire_date|hire_year|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+---------+\n",
      "|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01 00:00:00|     2015|\n",
      "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15 00:00:00|     2016|\n",
      "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01 00:00:00|     2014|\n",
      "|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30 00:00:00|     2017|\n",
      "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01 00:00:00|     2013|\n",
      "|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01 00:00:00|     2018|\n",
      "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15 00:00:00|     2012|\n",
      "|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01 00:00:00|     2019|\n",
      "|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01 00:00:00|     2016|\n",
      "|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01 00:00:00|     2018|\n",
      "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01 00:00:00|     2015|\n",
      "|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15 00:00:00|     2017|\n",
      "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01 00:00:00|     2011|\n",
      "|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01 00:00:00|     2019|\n",
      "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30 00:00:00|     2014|\n",
      "|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01 00:00:00|     2018|\n",
      "|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15 00:00:00|     2016|\n",
      "|         18|          104|    Nancy Liu| 29|Female| 50000|2017-06-01 00:00:00|     2017|\n",
      "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01 00:00:00|     2015|\n",
      "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01 00:00:00|     2018|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert date/timestamp into string and extract information from it\n",
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "emp.withColumn(\"hire_year\", date_format(col(\"hire_date\"), \"yyyy\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort, Union & Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: string (nullable = true)\n",
      " |-- department_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      " |-- hire_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert all columns to string type\n",
    "\n",
    "emp_str = emp.select([col(c).cast(\"string\") for c in emp.columns])\n",
    "emp_str.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataframe into 2 parts\n",
    "\n",
    "emp_str1 = emp_str.filter(emp.employee_id < 11)\n",
    "emp_str2 = emp_str.filter(emp.employee_id > 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|employee_id|department_id|         name|age|gender|salary|          hire_date|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01 00:00:00|\n",
      "|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15 00:00:00|\n",
      "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01 00:00:00|\n",
      "|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01 00:00:00|\n",
      "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30 00:00:00|\n",
      "|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01 00:00:00|\n",
      "|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15 00:00:00|\n",
      "|         18|          104|    Nancy Liu| 29|Female| 50000|2017-06-01 00:00:00|\n",
      "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01 00:00:00|\n",
      "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01 00:00:00|\n",
      "|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01 00:00:00|\n",
      "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15 00:00:00|\n",
      "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01 00:00:00|\n",
      "|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30 00:00:00|\n",
      "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01 00:00:00|\n",
      "|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01 00:00:00|\n",
      "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15 00:00:00|\n",
      "|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01 00:00:00|\n",
      "|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01 00:00:00|\n",
      "|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01 00:00:00|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Union and Union All (remove duplicates)\n",
    "# ! The columns must be in the same order and have the same data types\n",
    "# ? UnionByName can used when the column names are different but data types are same\n",
    "\n",
    "emp_str2.union(emp_str1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|employee_id|department_id|         name|age|gender|salary|          hire_date|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01 00:00:00|\n",
      "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15 00:00:00|\n",
      "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01 00:00:00|\n",
      "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30 00:00:00|\n",
      "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01 00:00:00|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|employee_id|department_id|         name|age|gender|salary|          hire_date|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01 00:00:00|\n",
      "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15 00:00:00|\n",
      "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01 00:00:00|\n",
      "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01 00:00:00|\n",
      "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30 00:00:00|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sorting the dataframe\n",
    "\n",
    "from pyspark.sql.functions import asc, desc\n",
    "\n",
    "emp.orderBy(desc(\"salary\")).show(5)\n",
    "\n",
    "emp.orderBy(asc(\"hire_date\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+--------+\n",
      "|department_id|dept_count|dept_pay|\n",
      "+-------------+----------+--------+\n",
      "|          103|         4|  232000|\n",
      "|          102|         4|  207000|\n",
      "|          101|         3|  165000|\n",
      "|          104|         3|  162000|\n",
      "|          106|         2|  138000|\n",
      "|          105|         2|  111000|\n",
      "|          107|         2|   95000|\n",
      "+-------------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregation functions\n",
    "from pyspark.sql.functions import count, sum, avg, max, min\n",
    "\n",
    "emp.groupBy(\"department_id\").agg(count(\"employee_id\").alias(\"dept_count\"), sum(\"salary\").alias(\"dept_pay\")).orderBy(desc(\"dept_pay\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+\n",
      "|department_id|avg_dept_salary|\n",
      "+-------------+---------------+\n",
      "|          101|        55000.0|\n",
      "|          103|        58000.0|\n",
      "|          102|        51750.0|\n",
      "|          105|        55500.0|\n",
      "|          106|        69000.0|\n",
      "|          104|        54000.0|\n",
      "+-------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.groupby(\"department_id\").agg(avg(\"salary\").alias(\"avg_dept_salary\")).where(col(\"avg_dept_salary\") > 50000).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique Data and Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|employee_id|department_id|         name|age|gender|salary|          hire_date|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01 00:00:00|\n",
      "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01 00:00:00|\n",
      "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01 00:00:00|\n",
      "|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01 00:00:00|\n",
      "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01 00:00:00|\n",
      "|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01 00:00:00|\n",
      "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01 00:00:00|\n",
      "|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01 00:00:00|\n",
      "|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15 00:00:00|\n",
      "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15 00:00:00|\n",
      "|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01 00:00:00|\n",
      "|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30 00:00:00|\n",
      "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01 00:00:00|\n",
      "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15 00:00:00|\n",
      "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01 00:00:00|\n",
      "|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15 00:00:00|\n",
      "|         18|          104|    Nancy Liu| 29|Female| 50000|2017-06-01 00:00:00|\n",
      "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30 00:00:00|\n",
      "|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01 00:00:00|\n",
      "|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01 00:00:00|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get unique data from the dataframe\n",
    "\n",
    "emp.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|department_id|\n",
      "+-------------+\n",
      "|          101|\n",
      "|          103|\n",
      "|          107|\n",
      "|          102|\n",
      "|          105|\n",
      "|          106|\n",
      "|          104|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.select('department_id').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+-------------------+---------------+\n",
      "|employee_id|department_id|         name|age|gender|salary|          hire_date|max_dept_salary|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+---------------+\n",
      "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15 00:00:00|          70000|\n",
      "|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01 00:00:00|          70000|\n",
      "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15 00:00:00|          70000|\n",
      "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01 00:00:00|          55000|\n",
      "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01 00:00:00|          55000|\n",
      "|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01 00:00:00|          55000|\n",
      "|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30 00:00:00|          55000|\n",
      "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01 00:00:00|          62000|\n",
      "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01 00:00:00|          62000|\n",
      "|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01 00:00:00|          62000|\n",
      "|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01 00:00:00|          62000|\n",
      "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01 00:00:00|          65000|\n",
      "|         18|          104|    Nancy Liu| 29|Female| 50000|2017-06-01 00:00:00|          65000|\n",
      "|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01 00:00:00|          65000|\n",
      "|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15 00:00:00|          57000|\n",
      "|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15 00:00:00|          57000|\n",
      "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01 00:00:00|          75000|\n",
      "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30 00:00:00|          75000|\n",
      "|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01 00:00:00|          49000|\n",
      "|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01 00:00:00|          49000|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Window functions\n",
    "# ? Allows to compute values based on a \"window\" of rows without collapsing them into a single row, unlike groupBy()\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import max, col, desc\n",
    "\n",
    "window_spec = Window.partitionBy(\"department_id\").orderBy(desc(\"salary\"))\n",
    "max_func = max(col(\"salary\")).over(window_spec)\n",
    "\n",
    "emp.withColumn(\"max_dept_salary\", max_func).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-----------+---+------+------+-------------------+----+\n",
      "|employee_id|department_id|       name|age|gender|salary|          hire_date|rank|\n",
      "+-----------+-------------+-----------+---+------+------+-------------------+----+\n",
      "|          1|          101|   John Doe| 30|  Male| 50000|2015-01-01 00:00:00|   2|\n",
      "|         20|          102|  Grace Kim| 32|Female| 53000|2018-11-01 00:00:00|   2|\n",
      "|          5|          103|  Jack Chan| 40|  Male| 60000|2013-04-01 00:00:00|   2|\n",
      "|         18|          104|  Nancy Liu| 29|Female| 50000|2017-06-01 00:00:00|   2|\n",
      "|         12|          105| Susan Chen| 31|Female| 54000|2017-02-15 00:00:00|   2|\n",
      "|         15|          106|Michael Lee| 37|  Male| 63000|2014-09-30 00:00:00|   2|\n",
      "|         14|          107|  Emily Lee| 26|Female| 46000|2019-01-01 00:00:00|   2|\n",
      "+-----------+-------------+-----------+---+------+------+-------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the 2nd highest salary in each department\n",
    "\n",
    "from pyspark.sql.functions import rank, col\n",
    "\n",
    "rank_spec = Window.partitionBy(\"department_id\").orderBy(desc(\"salary\"))\n",
    "rank_func = rank().over(rank_spec)\n",
    "\n",
    "emp.withColumn(\"rank\", rank_func).filter(col(\"rank\") == 2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joins and Data Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://a9106065823c:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Join & Partition</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f461c1b5300>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 36290)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/local/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/local/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/local/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/spark/python/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/spark/python/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/spark/python/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/spark/python/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Join & Partition\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: string (nullable = true)\n",
      " |-- department_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      " |-- hire_date: string (nullable = true)\n",
      "\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|employee_id|department_id|         name|age|gender|salary|          hire_date|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01 00:00:00|\n",
      "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15 00:00:00|\n",
      "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01 00:00:00|\n",
      "|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30 00:00:00|\n",
      "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01 00:00:00|\n",
      "|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01 00:00:00|\n",
      "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15 00:00:00|\n",
      "|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01 00:00:00|\n",
      "|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01 00:00:00|\n",
      "|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01 00:00:00|\n",
      "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01 00:00:00|\n",
      "|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15 00:00:00|\n",
      "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01 00:00:00|\n",
      "|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01 00:00:00|\n",
      "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30 00:00:00|\n",
      "|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01 00:00:00|\n",
      "|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15 00:00:00|\n",
      "|         18|          104|    Nancy Liu| 29|Female| 50000|2017-06-01 00:00:00|\n",
      "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01 00:00:00|\n",
      "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01 00:00:00|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp = spark.read.csv(\"data/emp.csv\", header=True, inferSchema=True)\n",
    "emp = emp.select([col(c).cast(\"string\") for c in emp.columns])\n",
    "\n",
    "emp.printSchema()\n",
    "emp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- department_id: string (nullable = true)\n",
      " |-- department_name: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- budget: string (nullable = true)\n",
      "\n",
      "+-------------+--------------------+-------+---------+-------+\n",
      "|department_id|     department_name|   city|  country| budget|\n",
      "+-------------+--------------------+-------+---------+-------+\n",
      "|          101|               Sales|    NYC|       US|1000000|\n",
      "|          102|           Marketing|     LA|       US| 900000|\n",
      "|          103|             Finance| London|       UK|1200000|\n",
      "|          104|         Engineering|Beijing|    China|1500000|\n",
      "|          105|     Human Resources|  Tokyo|    Japan| 800000|\n",
      "|          106|Research and Deve...|  Perth|Australia|1100000|\n",
      "|          107|    Customer Service| Sydney|Australia| 950000|\n",
      "+-------------+--------------------+-------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept_data = [\n",
    "    [\"101\", \"Sales\", \"NYC\", \"US\", \"1000000\"],\n",
    "    [\"102\", \"Marketing\", \"LA\", \"US\", \"900000\"],\n",
    "    [\"103\", \"Finance\", \"London\", \"UK\", \"1200000\"],\n",
    "    [\"104\", \"Engineering\", \"Beijing\", \"China\", \"1500000\"],\n",
    "    [\"105\", \"Human Resources\", \"Tokyo\", \"Japan\", \"800000\"],\n",
    "    [\"106\", \"Research and Development\", \"Perth\", \"Australia\", \"1100000\"],\n",
    "    [\"107\", \"Customer Service\", \"Sydney\", \"Australia\", \"950000\"]\n",
    "]\n",
    "\n",
    "dept_schema = \"department_id string, department_name string, city string, country string, budget string\"\n",
    "\n",
    "dept = spark.createDataFrame(data=dept_data, schema=dept_schema)\n",
    "\n",
    "dept.printSchema()\n",
    "dept.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions in Employee dataset: 1\n",
      "Partitions in Department dataset: 24\n"
     ]
    }
   ],
   "source": [
    "print(f\"Partitions in Employee dataset: {emp.rdd.getNumPartitions()}\")\n",
    "print(f\"Partitions in Department dataset: {dept.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp.repartition(4).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dept.repartition(8).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `coalesce` can help *reduce* the number of partitions without reshuffle (shuffling b/w the executors).\n",
    "# It also doesn't guarantee uniform data distribution, while repartition does.\n",
    "\n",
    "emp.coalesce(4).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+-------------------+-------------+\n",
      "|employee_id|department_id|         name|age|gender|salary|          hire_date|partition_num|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+-------------+\n",
      "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01 00:00:00|            0|\n",
      "|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30 00:00:00|            0|\n",
      "|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01 00:00:00|            0|\n",
      "|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01 00:00:00|            0|\n",
      "|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01 00:00:00|            0|\n",
      "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01 00:00:00|            0|\n",
      "|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15 00:00:00|            1|\n",
      "|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15 00:00:00|            1|\n",
      "|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01 00:00:00|            2|\n",
      "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01 00:00:00|            2|\n",
      "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01 00:00:00|            2|\n",
      "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30 00:00:00|            2|\n",
      "|         18|          104|    Nancy Liu| 29|Female| 50000|2017-06-01 00:00:00|            2|\n",
      "|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01 00:00:00|            3|\n",
      "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15 00:00:00|            3|\n",
      "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01 00:00:00|            3|\n",
      "|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01 00:00:00|            3|\n",
      "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15 00:00:00|            3|\n",
      "|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01 00:00:00|            3|\n",
      "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01 00:00:00|            3|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Repartition the Employee dataframe on/using the department_id column\n",
    "\n",
    "emp_part = emp.repartition(4, \"department_id\")\n",
    "emp_part.withColumn(\"partition_num\", spark_partition_id()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+--------------------+------+\n",
      "|         name|department_id|     department_name|salary|\n",
      "+-------------+-------------+--------------------+------+\n",
      "|James Johnson|          101|               Sales| 70000|\n",
      "|   Jane Smith|          101|               Sales| 45000|\n",
      "|     John Doe|          101|               Sales| 50000|\n",
      "|    Grace Kim|          102|           Marketing| 53000|\n",
      "|     Kate Kim|          102|           Marketing| 51000|\n",
      "|    Alice Lee|          102|           Marketing| 48000|\n",
      "|    Bob Brown|          102|           Marketing| 55000|\n",
      "|  Steven Chen|          103|             Finance| 62000|\n",
      "|      Tom Tan|          103|             Finance| 58000|\n",
      "|    Jill Wong|          103|             Finance| 52000|\n",
      "|    Jack Chan|          103|             Finance| 60000|\n",
      "|    Nancy Liu|          104|         Engineering| 50000|\n",
      "|   David Park|          104|         Engineering| 65000|\n",
      "|     Lisa Lee|          104|         Engineering| 47000|\n",
      "|  George Wang|          105|     Human Resources| 57000|\n",
      "|   Susan Chen|          105|     Human Resources| 54000|\n",
      "|  Michael Lee|          106|Research and Deve...| 63000|\n",
      "|    Brian Kim|          106|Research and Deve...| 75000|\n",
      "|  Kelly Zhang|          107|    Customer Service| 49000|\n",
      "|    Emily Lee|          107|    Customer Service| 46000|\n",
      "+-------------+-------------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# (Inner) join the Employee and Department dataframes and show a select set of columns\n",
    "\n",
    "emp.join(dept, how=\"inner\", on=emp.department_id==dept.department_id).select(emp.name, dept.department_id, dept.department_name, emp.salary).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading from CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- department_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- hire_date: timestamp (nullable = true)\n",
      "\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|employee_id|department_id|         name|age|gender|salary|          hire_date|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01 00:00:00|\n",
      "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15 00:00:00|\n",
      "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01 00:00:00|\n",
      "|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30 00:00:00|\n",
      "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01 00:00:00|\n",
      "|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01 00:00:00|\n",
      "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15 00:00:00|\n",
      "|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01 00:00:00|\n",
      "|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01 00:00:00|\n",
      "|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01 00:00:00|\n",
      "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01 00:00:00|\n",
      "|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15 00:00:00|\n",
      "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01 00:00:00|\n",
      "|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01 00:00:00|\n",
      "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30 00:00:00|\n",
      "|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01 00:00:00|\n",
      "|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15 00:00:00|\n",
      "|         18|          104|    Nancy Liu| 29|Female| 50000|2017-06-01 00:00:00|\n",
      "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01 00:00:00|\n",
      "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01 00:00:00|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data/emp.csv\", header=True, inferSchema=True)\n",
    "\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proactively passing Spark the dataframe-schema helps optimise the process by preventing \n",
    "# Spark from having to read any data and infer the header/schema by itself.\n",
    "# (No new \"Job\" was initiated for this transformation)\n",
    "\n",
    "emp_schema = \"employee_id int, department_id int, name string, age int, gender string, salary int, hire_date timestamp\"\n",
    "\n",
    "emp_give_schema = spark.read.csv(\"data/emp.csv\", header=True, schema=emp_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|employee_id|department_id|         name|age|gender|salary|          hire_date|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01 00:00:00|\n",
      "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15 00:00:00|\n",
      "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01 00:00:00|\n",
      "|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30 00:00:00|\n",
      "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01 00:00:00|\n",
      "|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01 00:00:00|\n",
      "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15 00:00:00|\n",
      "|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01 00:00:00|\n",
      "|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01 00:00:00|\n",
      "|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01 00:00:00|\n",
      "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01 00:00:00|\n",
      "|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15 00:00:00|\n",
      "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01 00:00:00|\n",
      "|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01 00:00:00|\n",
      "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30 00:00:00|\n",
      "|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01 00:00:00|\n",
      "|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15 00:00:00|\n",
      "|         18|          104|    Nancy Liu| 29|Female| 50000|2017-06-01 00:00:00|\n",
      "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01 00:00:00|\n",
      "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01 00:00:00|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_give_schema.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+-------------------+--------------------------------------------+\n",
      "|employee_id|department_id|name         |age|gender|salary|hire_date          |_corrupt_record                             |\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+--------------------------------------------+\n",
      "|1          |101          |John Doe     |30 |Male  |50000 |2015-01-01 00:00:00|null                                        |\n",
      "|2          |101          |Jane Smith   |25 |Female|45000 |2016-02-15 00:00:00|null                                        |\n",
      "|3          |102          |Bob Brown    |35 |Male  |55000 |2014-05-01 00:00:00|null                                        |\n",
      "|4          |102          |Alice Lee    |28 |Female|48000 |2017-09-30 00:00:00|null                                        |\n",
      "|5          |103          |Jack Chan    |40 |Male  |60000 |2013-04-01 00:00:00|null                                        |\n",
      "|6          |103          |Jill Wong    |32 |Female|52000 |2018-07-01 00:00:00|null                                        |\n",
      "|7          |101          |James Johnson|42 |Male  |null  |2012-03-15 00:00:00|007,101,James Johnson,42,Male,Low,2012-03-15|\n",
      "|8          |102          |Kate Kim     |29 |Female|51000 |2019-10-01 00:00:00|null                                        |\n",
      "|9          |103          |Tom Tan      |33 |Male  |58000 |2016-06-01 00:00:00|null                                        |\n",
      "|10         |104          |Lisa Lee     |27 |Female|47000 |2018-08-01 00:00:00|null                                        |\n",
      "|11         |104          |David Park   |38 |Male  |65000 |null               |011,104,David Park,38,Male,65000,no date    |\n",
      "|12         |105          |Susan Chen   |31 |Female|54000 |2017-02-15 00:00:00|null                                        |\n",
      "|13         |106          |Brian Kim    |45 |Male  |75000 |2011-07-01 00:00:00|null                                        |\n",
      "|14         |107          |Emily Lee    |26 |Female|46000 |2019-01-01 00:00:00|null                                        |\n",
      "|15         |106          |Michael Lee  |37 |Male  |63000 |2014-09-30 00:00:00|null                                        |\n",
      "|16         |107          |Kelly Zhang  |30 |Female|49000 |2018-04-01 00:00:00|null                                        |\n",
      "|17         |105          |George Wang  |34 |Male  |57000 |2016-03-15 00:00:00|null                                        |\n",
      "|18         |104          |Nancy Liu    |29 |Female|50000 |2017-06-01 00:00:00|null                                        |\n",
      "|19         |103          |Steven Chen  |36 |Male  |62000 |2015-08-01 00:00:00|null                                        |\n",
      "|20         |102          |Grace Kim    |32 |Female|53000 |2018-11-01 00:00:00|null                                        |\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+--------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The `mode` attribute (PERMISSIVE, by default) helps with handling \"bad\" records.\n",
    "\n",
    "emp_bad_schema = \"employee_id int, department_id int, name string, age int, gender string, salary int, hire_date timestamp, _corrupt_record string\"\n",
    "\n",
    "bad_df = spark.read.csv(\"data/emp_new.csv\", header=True, schema=emp_bad_schema)\n",
    "bad_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-----------+---+------+------+-------------------+---------------+\n",
      "|employee_id|department_id|       name|age|gender|salary|          hire_date|_corrupt_record|\n",
      "+-----------+-------------+-----------+---+------+------+-------------------+---------------+\n",
      "|          1|          101|   John Doe| 30|  Male| 50000|2015-01-01 00:00:00|           null|\n",
      "|          2|          101| Jane Smith| 25|Female| 45000|2016-02-15 00:00:00|           null|\n",
      "|          3|          102|  Bob Brown| 35|  Male| 55000|2014-05-01 00:00:00|           null|\n",
      "|          4|          102|  Alice Lee| 28|Female| 48000|2017-09-30 00:00:00|           null|\n",
      "|          5|          103|  Jack Chan| 40|  Male| 60000|2013-04-01 00:00:00|           null|\n",
      "|          6|          103|  Jill Wong| 32|Female| 52000|2018-07-01 00:00:00|           null|\n",
      "|          8|          102|   Kate Kim| 29|Female| 51000|2019-10-01 00:00:00|           null|\n",
      "|          9|          103|    Tom Tan| 33|  Male| 58000|2016-06-01 00:00:00|           null|\n",
      "|         10|          104|   Lisa Lee| 27|Female| 47000|2018-08-01 00:00:00|           null|\n",
      "|         12|          105| Susan Chen| 31|Female| 54000|2017-02-15 00:00:00|           null|\n",
      "|         13|          106|  Brian Kim| 45|  Male| 75000|2011-07-01 00:00:00|           null|\n",
      "|         14|          107|  Emily Lee| 26|Female| 46000|2019-01-01 00:00:00|           null|\n",
      "|         15|          106|Michael Lee| 37|  Male| 63000|2014-09-30 00:00:00|           null|\n",
      "|         16|          107|Kelly Zhang| 30|Female| 49000|2018-04-01 00:00:00|           null|\n",
      "|         17|          105|George Wang| 34|  Male| 57000|2016-03-15 00:00:00|           null|\n",
      "|         18|          104|  Nancy Liu| 29|Female| 50000|2017-06-01 00:00:00|           null|\n",
      "|         19|          103|Steven Chen| 36|  Male| 62000|2015-08-01 00:00:00|           null|\n",
      "|         20|          102|  Grace Kim| 32|Female| 53000|2018-11-01 00:00:00|           null|\n",
      "+-----------+-------------+-----------+---+------+------+-------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ? `DROPMALFORMED` mode drops the corrupt rows in the dataframe\n",
    "# ? `FAILFAST` mode fails as soon as it encounters any corrupt data (Usually used in scenarios involving payments processing)\n",
    "\n",
    "spark.read.format(\"csv\").option(\"header\", True).option(\"mode\", \"DROPMALFORMED\").schema(emp_bad_schema).load(\"data/emp_new.csv\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|employee_id|department_id|         name|age|gender|salary|          hire_date|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01 00:00:00|\n",
      "|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15 00:00:00|\n",
      "|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01 00:00:00|\n",
      "|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30 00:00:00|\n",
      "|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01 00:00:00|\n",
      "|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01 00:00:00|\n",
      "|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15 00:00:00|\n",
      "|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01 00:00:00|\n",
      "|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01 00:00:00|\n",
      "|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01 00:00:00|\n",
      "|         11|          104|   David Park| 38|  Male| 65000|2015-11-01 00:00:00|\n",
      "|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15 00:00:00|\n",
      "|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01 00:00:00|\n",
      "|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01 00:00:00|\n",
      "|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30 00:00:00|\n",
      "|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01 00:00:00|\n",
      "|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15 00:00:00|\n",
      "|         18|          104|    Nancy Liu| 29|Female| 50000|2017-06-01 00:00:00|\n",
      "|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01 00:00:00|\n",
      "|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01 00:00:00|\n",
      "+-----------+-------------+-------------+---+------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pass multiple arguments for Option using dict()\n",
    "\n",
    "_options = {\n",
    "    \"header\": True,\n",
    "    \"inferSchema\": True,\n",
    "    \"mode\": \"PERMISSIVE\",\n",
    "}\n",
    "\n",
    "spark.read.format(\"csv\").options(**_options).load(\"data/emp.csv\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Complex Data Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`Columnar Format` benifits when extracting specific column information since it only needs to read, decompress and process a specific part for the current query. In comparison. `Row Format` needs to read, decompress and process the entire file to determine the result for a particular query.\n",
    "\n",
    "- Row Format: <pre>A001, Dexter, <b>500</b>, A002, Tom, <b>600</b>, A003, Jerry, <b>1000</b></pre>\n",
    "- Column Format: <pre>A001, A002, A003, Dexter, Tom, Jerry, <b>`500, 600, 1000`</b></pre>\n",
    "- Query: Print the salary for all employees\n",
    "<br>\n",
    "<br>\n",
    "<center><img src=\"images/format-comparison.png\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- transacted_at: timestamp (nullable = true)\n",
      " |-- trx_id: integer (nullable = true)\n",
      " |-- retailer_id: integer (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- city_id: integer (nullable = true)\n",
      "\n",
      "+-------------------+----------+-----------+-----------------------------------------------+-------+----------+\n",
      "|transacted_at      |trx_id    |retailer_id|description                                    |amount |city_id   |\n",
      "+-------------------+----------+-----------+-----------------------------------------------+-------+----------+\n",
      "|2017-11-24 19:00:00|1995601912|2077350195 |Walgreen       11-25                           |197.23 |216510442 |\n",
      "|2017-11-24 19:00:00|1734117021|644879053  |unkn    ppd id: 768641     11-26               |8.58   |930259917 |\n",
      "|2017-11-24 19:00:00|1734117022|847200066  |Wal-Mart  ppd id: 555914     Algiers    11-26  |1737.26|1646415505|\n",
      "|2017-11-24 19:00:00|1734117030|1953761884 |Home Depot     ppd id: 265293   11-25          |384.5  |287177635 |\n",
      "|2017-11-24 19:00:00|1734117089|1898522855 |Target        11-25                            |66.33  |1855530529|\n",
      "|2017-11-24 19:00:00|1734117117|997626433  |Sears  ppd id: 856095  Ashgabat                |298.87 |957346984 |\n",
      "|2017-11-24 19:00:00|1734117123|1953761884 |unkn   ppd id: 153174    Little Rock    11-25  |19.55  |45522086  |\n",
      "|2017-11-24 19:00:00|1734117152|1429095612 |Ikea     arc id: 527956  Saint John's   11-26  |9.39   |1268541279|\n",
      "|2017-11-24 19:00:00|1734117153|847200066  |unkn        Kingstown                          |2907.57|1483931123|\n",
      "|2017-11-24 19:00:00|1734117212|1996661856 |unkn    ppd id: 454437   11-24                 |140.38 |336763936 |\n",
      "|2017-11-24 19:00:00|1734117241|486576507  |iTunes                                         |2912.67|1663872965|\n",
      "|2017-11-24 19:00:00|2076947148|847200066  |Wal-Mart         11-24                         |62.83  |1556600840|\n",
      "|2017-11-24 19:00:00|2076947147|562903918  |McDonald's    ccd id: 135878  Ljubljana   11-24|31.37  |930259917 |\n",
      "|2017-11-24 19:00:00|2076947146|511877722  |unkn     ccd id: 598521     Ankara   11-26     |1915.35|1698762556|\n",
      "|2017-11-24 19:00:00|2076947113|1996661856 |AutoZone  arc id: 998454    11-25              |1523.6 |1759612211|\n",
      "|2017-11-24 19:00:00|2076947018|902350112  |DineEquity    arc id: 1075293                  |22.28  |2130657559|\n",
      "|2017-11-24 19:00:00|2076946994|1898522855 |Target    ppd id: 336785                       |2589.93|2074005445|\n",
      "|2017-11-24 19:00:00|2076946985|847200066  |Wal-Mart    ppd id: 252763  11-26              |42.2   |459344513 |\n",
      "|2017-11-24 19:00:00|2076946960|386167994  |Wendy's  ppd id: 881511     11-24              |14.62  |352952442 |\n",
      "|2017-11-24 19:00:00|2076946954|486576507  |iTunes     ppd id: 121397                      |37.42  |485114748 |\n",
      "+-------------------+----------+-----------+-----------------------------------------------+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1102576"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_pq = spark.read.parquet(\"data/sales_data.parquet\")\n",
    "\n",
    "sales_pq.printSchema()\n",
    "sales_pq.show(truncate=False)\n",
    "sales_pq.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- transacted_at: timestamp (nullable = true)\n",
      " |-- trx_id: integer (nullable = true)\n",
      " |-- retailer_id: integer (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- city_id: integer (nullable = true)\n",
      "\n",
      "+-------------------+----------+-----------+--------------------+-------+----------+\n",
      "|      transacted_at|    trx_id|retailer_id|         description| amount|   city_id|\n",
      "+-------------------+----------+-----------+--------------------+-------+----------+\n",
      "|2017-11-24 19:00:00|1995601912| 2077350195|Walgreen       11-25| 197.23| 216510442|\n",
      "|2017-11-24 19:00:00|1734117021|  644879053|unkn    ppd id: 7...|   8.58| 930259917|\n",
      "|2017-11-24 19:00:00|1734117022|  847200066|Wal-Mart  ppd id:...|1737.26|1646415505|\n",
      "|2017-11-24 19:00:00|1734117030| 1953761884|Home Depot     pp...|  384.5| 287177635|\n",
      "|2017-11-24 19:00:00|1734117089| 1898522855| Target        11-25|  66.33|1855530529|\n",
      "|2017-11-24 19:00:00|1734117117|  997626433|Sears  ppd id: 85...| 298.87| 957346984|\n",
      "|2017-11-24 19:00:00|1734117123| 1953761884|unkn   ppd id: 15...|  19.55|  45522086|\n",
      "|2017-11-24 19:00:00|1734117152| 1429095612|Ikea     arc id: ...|   9.39|1268541279|\n",
      "|2017-11-24 19:00:00|1734117153|  847200066|unkn        Kings...|2907.57|1483931123|\n",
      "|2017-11-24 19:00:00|1734117212| 1996661856|unkn    ppd id: 4...| 140.38| 336763936|\n",
      "|2017-11-24 19:00:00|1734117241|  486576507|              iTunes|2912.67|1663872965|\n",
      "|2017-11-24 19:00:00|2076947148|  847200066|Wal-Mart         ...|  62.83|1556600840|\n",
      "|2017-11-24 19:00:00|2076947147|  562903918|McDonald's    ccd...|  31.37| 930259917|\n",
      "|2017-11-24 19:00:00|2076947146|  511877722|unkn     ccd id: ...|1915.35|1698762556|\n",
      "|2017-11-24 19:00:00|2076947113| 1996661856|AutoZone  arc id:...| 1523.6|1759612211|\n",
      "|2017-11-24 19:00:00|2076947018|  902350112|DineEquity    arc...|  22.28|2130657559|\n",
      "|2017-11-24 19:00:00|2076946994| 1898522855|Target    ppd id:...|2589.93|2074005445|\n",
      "|2017-11-24 19:00:00|2076946985|  847200066|Wal-Mart    ppd i...|   42.2| 459344513|\n",
      "|2017-11-24 19:00:00|2076946960|  386167994|Wendy's  ppd id: ...|  14.62| 352952442|\n",
      "|2017-11-24 19:00:00|2076946954|  486576507|iTunes     ppd id...|  37.42| 485114748|\n",
      "+-------------------+----------+-----------+--------------------+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1102576"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_orc = spark.read.orc(\"data/sales_data.orc\")\n",
    "\n",
    "sales_orc.printSchema()\n",
    "sales_orc.show()\n",
    "sales_orc.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1102576"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading all files in a folder\n",
    "\n",
    "sales_m_pq = spark.read.parquet('data/sales_total_parquet/*.parquet')\n",
    "sales_m_pq.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise a decorator to track operation time\n",
    "\n",
    "import time\n",
    "\n",
    "def get_time(func):\n",
    "    def inner_get_time() -> str:\n",
    "        st = time.time()\n",
    "        func()\n",
    "        return (f\"Execution Time: {(time.time() - st) * 1000:.4f} ms\")\n",
    "    print(inner_get_time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time: 377.0742 ms\n"
     ]
    }
   ],
   "source": [
    "@get_time\n",
    "def x():\n",
    "    spark.read.parquet(\"data/sales_data.parquet\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time: 215.5883 ms\n"
     ]
    }
   ],
   "source": [
    "@get_time\n",
    "def x():\n",
    "    spark.read.parquet(\"data/sales_data.parquet\").select(\"trx_id\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+-----------+--------------------+------+---------+\n",
      "|      transacted_at|    trx_id|retailer_id|         description|amount|  city_id|\n",
      "+-------------------+----------+-----------+--------------------+------+---------+\n",
      "|2017-11-24 19:00:00|1734117123| 1953761884|unkn   ppd id: 15...| 19.55| 45522086|\n",
      "|2017-11-24 19:00:00|1734117021|  644879053|unkn    ppd id: 7...|  8.58|930259917|\n",
      "+-------------------+----------+-----------+--------------------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load nested files within subfolders\n",
    "\n",
    "spark.read.parquet(\"data/sales_recursive/\", recursiveFileLookup=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment\n",
    "\n",
    "Calculate the average amount per retailer and append a column displaying the corresponding value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|retailer_id|        avg_amount|\n",
      "+-----------+------------------+\n",
      "|  862777075| 400.6277299001569|\n",
      "| 2092104004| 395.6865486725659|\n",
      "|  495545430|394.33791799362956|\n",
      "|  771821475| 393.7823759630201|\n",
      "|  508452694| 390.6041735537194|\n",
      "| 1295306792|389.76992566161067|\n",
      "| 1006678445| 389.4951192145855|\n",
      "|  316135668|386.18770249924586|\n",
      "|  860355551|384.89784578313174|\n",
      "|  162598651| 384.4054847207588|\n",
      "|  386167994|  383.733645990921|\n",
      "| 2145070162| 382.9899318729707|\n",
      "|  304276488|381.42095686663316|\n",
      "|  270266090| 381.3091858037574|\n",
      "|  606497335| 380.2884948915997|\n",
      "| 1720938479| 379.8883515482675|\n",
      "|  997626433| 379.4370040204126|\n",
      "| 1445595477| 379.0481333504646|\n",
      "|  143327090| 379.0115743380849|\n",
      "|  771066397| 378.9928698752224|\n",
      "+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.groupBy(\"retailer_id\").agg(avg(\"amount\").alias(\"avg_amount\")).orderBy(desc(\"avg_amount\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+-----------+--------------------+-------+----------+-------------------+\n",
      "|      transacted_at|    trx_id|retailer_id|         description| amount|   city_id|retailer_avg_amount|\n",
      "+-------------------+----------+-----------+--------------------+-------+----------+-------------------+\n",
      "|2017-01-01 19:00:00|1853124918|  582210968|Family Dollar Sto...|  757.1| 637093548| 367.40095518419776|\n",
      "|2017-01-01 19:00:00|1853101949|  914585647|Whole Foods Marke...|    3.5| 903387909| 356.07879924407587|\n",
      "|2017-01-01 19:00:00|1853158553|  582210968|Family Dollar Sto...| 1975.2|1912579202| 367.40095518419776|\n",
      "|2017-01-01 19:00:00|1853160484|  304276488|Belk    arc id: 6...|   3.67| 333864585| 381.42095686663316|\n",
      "|2017-01-01 19:00:00|1852974205|  386167994|unkn     ppd id: ...|1309.26| 352952442|   383.733645990921|\n",
      "|2017-01-01 19:00:00|1853035514|  386167994|Wendy's  ppd id: ...| 207.15|1462628288|   383.733645990921|\n",
      "|2017-01-01 19:00:00|1853039330|  304276488|                Belk|  677.6|1107275933| 381.42095686663316|\n",
      "|2017-01-01 19:00:00|1852974072|  582210968|unkn  arc id: 276422|  17.38|1744912105| 367.40095518419776|\n",
      "|2017-01-01 19:00:00|1852978849|  582210968|                unkn|  85.32| 576697624| 367.40095518419776|\n",
      "|2017-01-01 19:00:00|1853068258|  582210968|Family Dollar Sto...|  78.19|1487919653| 367.40095518419776|\n",
      "|2017-01-01 19:00:00|1853130659|  386167994|             Wendy's| 493.15| 455476705|   383.733645990921|\n",
      "|2017-01-01 19:00:00|1852975969|  914585647|Whole Foods Marke...|2535.37| 559832710| 356.07879924407587|\n",
      "|2017-01-01 19:00:00|1852978792|  914585647|unkn    ccd id: 4...|2839.85|1790189812| 356.07879924407587|\n",
      "|2017-01-01 19:00:00|1853101863|  914585647|Whole Foods Marke...|   35.9| 218089872| 356.07879924407587|\n",
      "|2017-01-01 19:00:00|1853004052| 1817581369|Dollar Tree  ccd ...|  16.86| 129711554| 373.63467544910094|\n",
      "|2017-01-01 19:00:00|1853035708| 1817581369|Dollar Tree  ccd ...|1810.34|  28424447| 373.63467544910094|\n",
      "|2017-01-01 19:00:00|1853042275| 1817581369|Dollar Tree    As...| 447.83| 957346984| 373.63467544910094|\n",
      "|2017-01-01 19:00:00|1853072188| 1817581369|Dollar Tree    pp...|   47.7|1609326953| 373.63467544910094|\n",
      "|2017-01-01 19:00:00| 475421147| 1888067226|Neiman Marcus    ...|   4.58| 637093548| 362.73621087314586|\n",
      "|2017-01-01 19:00:00|1853004829| 1888067226|unkn  arc id: 602947|  13.16|1655945659| 362.73621087314586|\n",
      "+-------------------+----------+-----------+--------------------+-------+----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "window = Window.partitionBy(\"retailer_id\")\n",
    "avg_amount = avg(\"amount\").over(window)\n",
    "\n",
    "sales.withColumn(\"retailer_avg_amount\", avg_amount).orderBy(asc(\"transacted_at\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- contact: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_line_items: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- amount: double (nullable = true)\n",
      " |    |    |-- item_id: string (nullable = true)\n",
      " |    |    |-- qty: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading a single line JSON file\n",
    "\n",
    "spark.read.json(\"data/order_singleline.json\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- contact: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_line_items: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- amount: double (nullable = true)\n",
      " |    |    |-- item_id: string (nullable = true)\n",
      " |    |    |-- qty: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading a multi-line JSON file\n",
    "\n",
    "spark.read.json(\"data/order_multiline.json\", multiLine=True).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------+--------------------+\n",
      "|             contact|customer_id|order_id|    order_line_items|\n",
      "+--------------------+-----------+--------+--------------------+\n",
      "|[9000010000, 9000...|       C001|    O101|[{102.45, I001, 6...|\n",
      "+--------------------+-----------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading and storing a single-line JSON file\n",
    "\n",
    "order = spark.read.json(\"data/order_singleline.json\")\n",
    "order.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- contact: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_line_items: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- amount: double (nullable = true)\n",
      " |    |    |-- item_id: string (nullable = true)\n",
      " |    |    |-- qty: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Defining a complex schema for the JSON file\n",
    "\n",
    "complex_schema = \"contact array<string>, customer_id string, order_id string, order_line_items array<struct<amount double, item_id string, qty long>>\"\n",
    "\n",
    "spark.read.json(\"data/order_singleline.json\", schema=complex_schema).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"order_id\":\"O101...|\n",
      "+--------------------+\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|               value|              parsed|\n",
      "+--------------------+--------------------+\n",
      "|{\"order_id\":\"O101...|{[9000010000, 900...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# String to JSON using from_json function, with a schema\n",
    "\n",
    "json_str = spark.read.text(\"data/order_singleline.json\")\n",
    "json_str.show()\n",
    "\n",
    "json_expanded = json_str.withColumn(\"parsed\", from_json(json_str.value, complex_schema))\n",
    "json_expanded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|unparsed                                                                                                                                                                               |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|{\"contact\":[\"9000010000\",\"9000010001\"],\"customer_id\":\"C001\",\"order_id\":\"O101\",\"order_line_items\":[{\"amount\":102.45,\"item_id\":\"I001\",\"qty\":6},{\"amount\":2.01,\"item_id\":\"I003\",\"qty\":2}]}|\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# JSON to String using to_json\n",
    "\n",
    "json_expanded.withColumn(\"unparsed\", to_json(json_expanded.parsed)).select(\"unparsed\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------+\n",
      "|parsed                                                                      |\n",
      "+----------------------------------------------------------------------------+\n",
      "|{[9000010000, 9000010001], C001, O101, [{102.45, I001, 6}, {2.01, I003, 2}]}|\n",
      "+----------------------------------------------------------------------------+\n",
      "\n",
      "+------------------------+-----------+--------+------------------------------------+\n",
      "|contact                 |customer_id|order_id|order_line_items                    |\n",
      "+------------------------+-----------+--------+------------------------------------+\n",
      "|[9000010000, 9000010001]|C001       |O101    |[{102.45, I001, 6}, {2.01, I003, 2}]|\n",
      "+------------------------+-----------+--------+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract/Expand struct/dict data in JSON\n",
    "\n",
    "json_expanded.select(\"parsed\").show(truncate=False)\n",
    "\n",
    "json_expanded.select(\"parsed.*\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------+--------------------+-----------------+\n",
      "|             contact|customer_id|order_id|    order_line_items|         exploded|\n",
      "+--------------------+-----------+--------+--------------------+-----------------+\n",
      "|[9000010000, 9000...|       C001|    O101|[{102.45, I001, 6...|{102.45, I001, 6}|\n",
      "|[9000010000, 9000...|       C001|    O101|[{102.45, I001, 6...|  {2.01, I003, 2}|\n",
      "+--------------------+-----------+--------+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_expanded.select(\"parsed.*\").withColumn(\"exploded\", explode(\"order_line_items\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://a9106065823c:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Writing Data</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fd7c2b50220>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Writing Data\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the available number of cores\n",
    "\n",
    "spark.sparkContext.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: string (nullable = true)\n",
      " |-- department_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      " |-- hire_date: string (nullable = true)\n",
      "\n",
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "|employee_id|department_id|         name|age|gender|salary| hire_date|\n",
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|\n",
      "|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|\n",
      "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|\n",
      "|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|\n",
      "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|\n",
      "|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|\n",
      "|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|\n",
      "|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|\n",
      "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|\n",
      "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|\n",
      "|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|\n",
      "|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|\n",
      "|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|\n",
      "|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|\n",
      "|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|\n",
      "|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|\n",
      "|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|\n",
      "|        018|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|\n",
      "|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|\n",
      "|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|\n",
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp = spark.read.csv(\"data/emp.csv\", header=True)\n",
    "\n",
    "emp.printSchema()\n",
    "emp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save/Write the data in parquet format\n",
    "\n",
    "emp.write.parquet(\"data/output/emp.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+------------+\n",
      "|employee_id|department_id|         name|age|gender|salary| hire_date|partition_id|\n",
      "+-----------+-------------+-------------+---+------+------+----------+------------+\n",
      "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|           0|\n",
      "|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|           0|\n",
      "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|           0|\n",
      "|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|           0|\n",
      "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|           0|\n",
      "|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|           0|\n",
      "|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|           0|\n",
      "|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|           0|\n",
      "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|           0|\n",
      "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|           0|\n",
      "|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|           0|\n",
      "|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|           0|\n",
      "|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|           0|\n",
      "|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|           0|\n",
      "|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|           0|\n",
      "|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|           0|\n",
      "|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|           0|\n",
      "|        018|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|           0|\n",
      "|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|           0|\n",
      "|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|           0|\n",
      "+-----------+-------------+-------------+---+------+------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.withColumn(\"partition_id\", spark_partition_id()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the file wrt partitions\n",
    "# ! Write Modes: Append, Overwrite, Ignore, Error\n",
    "\n",
    "emp.write.format(\"csv\").partitionBy(\"department_id\").option(\"header\", True).save(\"data/output/emp.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Use the docker compose file to setup/deploy required containers\n",
    "- Execute command from docker container, from the `/spark` dir\n",
    "\n",
    "    - <pre>./bin/spark-submit --master spark://78962bfc976e:7077 <b>[Replace with master server address]</b> --num-executors 3 --executor-cores 2 </br>--executor-memory 512M /data/12_understand_cluster.py</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.extraJavaOptions',\n",
       "  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n",
       " ('spark.app.startTime', '1746507577605'),\n",
       " ('spark.executor.instances', '4'),\n",
       " ('spark.exectuor.memory', '512M'),\n",
       " ('spark.app.name', 'Cluster Execution'),\n",
       " ('spark.master', 'spark://78962bfc976e:7077'),\n",
       " ('spark.app.submitTime', '1746503659432'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.sql.warehouse.dir', 'file:/home/jupyter/spark-warehouse'),\n",
       " ('spark.executor.cores', '4'),\n",
       " ('spark.driver.host', 'ae2e94ccea8e'),\n",
       " ('spark.app.id', 'app-20250506045937-0007'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.driver.port', '44573'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Cluster Execution\")\n",
    "    .master(\"spark://78962bfc976e:7077\")\n",
    "    .config(\"spark.executor.instances\", 4)\n",
    "    .config(\"spark.executor.cores\", 4)\n",
    "    .config(\"spark.exectuor.memory\", \"512M\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.range(10).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-Defined Functions (UDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extremely slow and expensive since python requires Serializing/De-Serailizing data.\n",
    "- Can be mitigated by:\n",
    "    - Using in-built higher-order functions\n",
    "    - Writing the functions in Java or Scala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://b29f78abb2bc:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://eb22766c6b88:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>UDF</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f2f5eeb1e70>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"UDF\")\n",
    "    .master(\"spark://eb22766c6b88:7077\")\n",
    "    .config(\"spark.executor.cores\", 2)\n",
    "    .config(\"spark.cores.max\", 6)\n",
    "    .config(\"spark.executor.memory\", \"512M\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "|employee_id|department_id|         name|age|gender|salary| hire_date|\n",
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|\n",
      "|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|\n",
      "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|\n",
      "|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|\n",
      "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|\n",
      "|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|\n",
      "|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|\n",
      "|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|\n",
      "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|\n",
      "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|\n",
      "|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|\n",
      "|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|\n",
      "|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|\n",
      "|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|\n",
      "|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|\n",
      "|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|\n",
      "|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|\n",
      "|        018|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|\n",
      "|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|\n",
      "|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|\n",
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp = spark.read.csv(\"/data/emp.csv\", header=True)\n",
    "\n",
    "emp.show()\n",
    "emp.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDF\n",
    "\n",
    "def bonus(salary):\n",
    "    return int(salary) * 0.1\n",
    "\n",
    "bonus_udf = udf(bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ! Doesn't run, config error\n",
    "# emp.withColumn('bonus', bonus_udf(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sparkContext.getConf().getAll()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DAG Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://b29f78abb2bc:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DAG Plan</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f2f5ed854b0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"DAG Plan\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disabling Spark features that help with Optimisation\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1, df2 = spark.range(4, 200, 2), spark.range(2, 200, 4)\n",
    "df3, df4 = df1.repartition(5), df2.repartition(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1.rdd.getNumPartitions(), df2.rdd.getNumPartitions(), df3.rdd.getNumPartitions(), df4.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df34 = df3.join(df4, on='id')\n",
    "df34_sum = df34.selectExpr(\"sum(id) as total_sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|total_sum|\n",
      "+---------+\n",
      "|     4998|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df34_sum.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='images/DAG.png/' width=1200></center>\n",
    "\n",
    "<b>JOB 0 & 1</b>\n",
    "*spark.range*\n",
    "- 1 stage each\n",
    "- 24 cores = 24 tasks\n",
    "\n",
    "<b>JOB 2 & 3</b>\n",
    "*Repartition (involves reshuffle)*\n",
    "- 1 stage each\n",
    "- 5/7 partitions = 5/7 tasks\n",
    "\n",
    "<b>JOB 4</b>\n",
    "*Join (involves shuffle)*\n",
    "- 1 stage\n",
    "- defualt spark shuffle is 200 partitions = 200 tasks\n",
    "\n",
    "<b>JOB 5</b>\n",
    "*Sum*\n",
    "- 1 stage\n",
    "- sum to a single value = 1 task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[], functions=[sum(id#384L)])\n",
      "   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#1296]\n",
      "      +- HashAggregate(keys=[], functions=[partial_sum(id#384L)])\n",
      "         +- Project [id#384L]\n",
      "            +- SortMergeJoin [id#384L], [id#386L], Inner\n",
      "               :- Sort [id#384L ASC NULLS FIRST], false, 0\n",
      "               :  +- Exchange hashpartitioning(id#384L, 200), ENSURE_REQUIREMENTS, [id=#1288]\n",
      "               :     +- Exchange RoundRobinPartitioning(5), REPARTITION_BY_NUM, [id=#1279]\n",
      "               :        +- Range (4, 200, step=2, splits=24)\n",
      "               +- Sort [id#386L ASC NULLS FIRST], false, 0\n",
      "                  +- Exchange hashpartitioning(id#386L, 200), ENSURE_REQUIREMENTS, [id=#1289]\n",
      "                     +- Exchange RoundRobinPartitioning(7), REPARTITION_BY_NUM, [id=#1281]\n",
      "                        +- Range (2, 200, step=4, splits=24)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df34_sum.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimising Shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PySpark can perform multiple *narrow-transformations* together by creating a pipeline.\n",
    "- If a reshuffle is required, which is a *wide-transformation*, it will create 2 different pipleline (and hence 2 seperate stages).\n",
    "- In the midst of shuffling and switching pipelines, the files are written in the **Tungsten Binary Format (Unsafe row)** which can be directly read in disk-memory, thus imporving the read performance.\n",
    "- Since shuffling requires disk I/O access along with Network, it is a costly operation and should be avoided as much as possible. But it is also necssary for *narrow-transformations*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://b29f78abb2bc:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://eb22766c6b88:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Optimising Shuffle</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f08aeaa86a0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Optimising Shuffle\")\n",
    "    .master(\"spark://eb22766c6b88:7077\")\n",
    "    .config(\"spark.cores.max\", 16)\n",
    "    .config(\"spark.executor.cores\", 4)\n",
    "    .config(\"spark.executor.memory\", \"512M\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disabling Spark features that help with Optimisation (Adaptive Query Engine)\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_schema = \"first_name string, last_name string, job_title string, dob string, email string, phone string, salary double, department_id int\"\n",
    "\n",
    "empRec = spark.read.csv(\"/data/employee_records.csv\", header=True, schema=_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average salary per department\n",
    "\n",
    "emp_avg = empRec.groupBy(\"department_id\").agg(avg(\"salary\").alias(\"avg_sal\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing data for benchmarking (noop)\n",
    "\n",
    "emp_avg.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'200'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the Spark Shuffle Default Partitions\n",
    "\n",
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are many partitions that are doing nothing, and increasing the computing overhead. We can optimise this by reducing the default shuffle partitions\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'16'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_avg.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read partitioned data (benefits in computation around shuffle)\n",
    "\n",
    "empPart = spark.read.csv(\"/data/emp.csv/\", header=True, schema=_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_avg = empPart.groupby(\"department_id\").agg(avg(\"salary\")).alias(\"avg_sal\")\n",
    "emp_avg.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Caching Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://b29f78abb2bc:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Caching</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe93c14bc10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Spark Caching\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.executor.memory\", \"512M\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_schema = \"transacted_at string, trx_id string, retailer_id string, description string, amount double, city_id string\"\n",
    "\n",
    "sales = spark.read.csv(\"/data/new_sales.csv\", header=True, schema=_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----------+--------------------+-------+----------+\n",
      "|       transacted_at|    trx_id|retailer_id|         description| amount|   city_id|\n",
      "+--------------------+----------+-----------+--------------------+-------+----------+\n",
      "|2017-11-24T19:00:...|1734117022|  847200066|Wal-Mart  ppd id:...|1737.26|1646415505|\n",
      "|2017-11-24T19:00:...|1734117030| 1953761884|Home Depot     pp...|  384.5| 287177635|\n",
      "|2017-11-24T19:00:...|1734117153|  847200066|unkn        Kings...|2907.57|1483931123|\n",
      "|2017-11-24T19:00:...|1734117241|  486576507|              iTunes|2912.67|1663872965|\n",
      "|2017-11-24T19:00:...|2076947146|  511877722|unkn     ccd id: ...|1915.35|1698762556|\n",
      "|2017-11-24T19:00:...|2076947113| 1996661856|AutoZone  arc id:...| 1523.6|1759612211|\n",
      "|2017-11-24T19:00:...|2076946994| 1898522855|Target    ppd id:...|2589.93|2074005445|\n",
      "|2017-11-24T19:00:...|2076946121|  562903918|unkn    ccd id: 5...| 315.86|1773943669|\n",
      "|2017-11-24T19:00:...|2076946063| 1070485878|Amazon.com   arc ...| 785.27|1126623009|\n",
      "|2017-11-24T19:00:...|2076944979| 1654681099|Delhaize America ...|  303.1|1243655802|\n",
      "|2017-11-24T19:00:...|2076944941| 1157343460|unkn    ppd id: 1...|2853.33|1141716004|\n",
      "|2017-11-24T19:00:...|2076944228| 1522061472|         YUM! Brands|1737.45| 592064091|\n",
      "|2017-11-24T19:00:...|2076944195| 1070485878|unkn   ppd id: 11...|2440.55|1525790470|\n",
      "|2017-11-24T19:00:...|2076944142|  847200066|Wal-Mart  ppd id:...| 331.63|1345953582|\n",
      "|2017-11-24T19:00:...|2076944073| 2077350195|Walgreen     arc ...|  396.9|2001708947|\n",
      "|2017-11-24T19:00:...|2076943052|  103953879|Rite Aid  arc id:...| 1910.8|1998549640|\n",
      "|2017-11-24T19:00:...|2076942340|  643354906|                BJ's|  372.7| 115209716|\n",
      "|2017-11-24T19:00:...|2076942282| 1445595477|Meijer    ccd id:...|  366.9|1717498102|\n",
      "|2017-11-24T19:00:...|2076942274| 2001148981|unkn    ppd id: 2...| 333.41| 559832710|\n",
      "|2017-11-24T19:00:...|2076942246|    9225731|AT&T Wireless  pp...|  396.9| 407629665|\n",
      "+--------------------+----------+-----------+--------------------+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.where(\"amount > 300\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7202569"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cache DataFrame (default storage level: MEMORY_AND_DISK)\n",
    "# NOTE: For Cachine, we need an *Action* to be executed and Count/Write are preferred since they scan the whole dataset.\n",
    "\n",
    "sales.cache().count()   # Caches data in de-serialised form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----------+--------------------+-------+----------+\n",
      "|       transacted_at|    trx_id|retailer_id|         description| amount|   city_id|\n",
      "+--------------------+----------+-----------+--------------------+-------+----------+\n",
      "|2017-11-24T19:00:...|1734117022|  847200066|Wal-Mart  ppd id:...|1737.26|1646415505|\n",
      "|2017-11-24T19:00:...|1734117030| 1953761884|Home Depot     pp...|  384.5| 287177635|\n",
      "|2017-11-24T19:00:...|1734117153|  847200066|unkn        Kings...|2907.57|1483931123|\n",
      "|2017-11-24T19:00:...|1734117241|  486576507|              iTunes|2912.67|1663872965|\n",
      "|2017-11-24T19:00:...|2076947146|  511877722|unkn     ccd id: ...|1915.35|1698762556|\n",
      "|2017-11-24T19:00:...|2076947113| 1996661856|AutoZone  arc id:...| 1523.6|1759612211|\n",
      "|2017-11-24T19:00:...|2076946994| 1898522855|Target    ppd id:...|2589.93|2074005445|\n",
      "|2017-11-24T19:00:...|2076946121|  562903918|unkn    ccd id: 5...| 315.86|1773943669|\n",
      "|2017-11-24T19:00:...|2076946063| 1070485878|Amazon.com   arc ...| 785.27|1126623009|\n",
      "|2017-11-24T19:00:...|2076944979| 1654681099|Delhaize America ...|  303.1|1243655802|\n",
      "|2017-11-24T19:00:...|2076944941| 1157343460|unkn    ppd id: 1...|2853.33|1141716004|\n",
      "|2017-11-24T19:00:...|2076944228| 1522061472|         YUM! Brands|1737.45| 592064091|\n",
      "|2017-11-24T19:00:...|2076944195| 1070485878|unkn   ppd id: 11...|2440.55|1525790470|\n",
      "|2017-11-24T19:00:...|2076944142|  847200066|Wal-Mart  ppd id:...| 331.63|1345953582|\n",
      "|2017-11-24T19:00:...|2076944073| 2077350195|Walgreen     arc ...|  396.9|2001708947|\n",
      "|2017-11-24T19:00:...|2076943052|  103953879|Rite Aid  arc id:...| 1910.8|1998549640|\n",
      "|2017-11-24T19:00:...|2076942340|  643354906|                BJ's|  372.7| 115209716|\n",
      "|2017-11-24T19:00:...|2076942282| 1445595477|Meijer    ccd id:...|  366.9|1717498102|\n",
      "|2017-11-24T19:00:...|2076942274| 2001148981|unkn    ppd id: 2...| 333.41| 559832710|\n",
      "|2017-11-24T19:00:...|2076942246|    9225731|AT&T Wireless  pp...|  396.9| 407629665|\n",
      "+--------------------+----------+-----------+--------------------+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.where(\"amount > 300\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[transacted_at: string, trx_id: string, retailer_id: string, description: string, amount: double, city_id: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove Cache\n",
    "\n",
    "sales.unpersist()\n",
    "# spark.catalog.clearCache()  # Clears all session cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use other caching storage levels, we use `persist`. (eg: MEMORY_ONLY, DISK_ONLY, etc.)\n",
    "import pyspark\n",
    "\n",
    "sales_persist = sales.persist(pyspark.StorageLevel.MEMORY_ONLY)\n",
    "sales_persist.write.format(\"noop\").mode(\"overwrite\").save()   # Caches data in Serialised form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed Shared Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://b29f78abb2bc:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://eb22766c6b88:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Distributed Shared Variables</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe93c149c00>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Distributed Shared Variables\")\n",
    "    .master(\"spark://eb22766c6b88:7077\")\n",
    "    .config(\"spark.cores.max\", 16)\n",
    "    .config(\"spark.executor.cores\", 4)\n",
    "    .config(\"spark.executor.memory\", \"512M\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "_schema = \"first_name string, last_name string, job_title string, dob string, email string, phone string, salary double, department_id int\"\n",
    "\n",
    "emp = spark.read.csv(\"/data/employee_records.csv\", header=True, schema=_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to match deparment IDs to department names.\n",
    "- Creating a new dataframe and using `Join` to assign the department names can Shuffle the data, if not optimised correctly, which is undesired.\n",
    "- Using UDF or a map operation. But when we create a variable, it will be Serialised along with the task. Hence, also de-serialised everytime the task is de-serialised which will create an unnecessary overhead.\n",
    "\n",
    "Solution: *Broadcast Variable*\n",
    "- The variable is sent and cached individually at each executor. Will prevent shuffling between themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Lookup) Variable\n",
    "\n",
    "dept_names = {\n",
    "    1: 'Department 1',\n",
    "    2: 'Department 2',\n",
    "    3: 'Department 3',\n",
    "    4: 'Department 4',\n",
    "    5: 'Department 5',\n",
    "    6: 'Department 6',\n",
    "    7: 'Department 7',\n",
    "    8: 'Department 8',\n",
    "    9: 'Department 9',\n",
    "    10: 'Department 10',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcast the Variable\n",
    "\n",
    "broadcastDeptNames = spark.sparkContext.broadcast(dept_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.broadcast.Broadcast"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(broadcastDeptNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'Department 1',\n",
       " 2: 'Department 2',\n",
       " 3: 'Department 3',\n",
       " 4: 'Department 4',\n",
       " 5: 'Department 5',\n",
       " 6: 'Department 6',\n",
       " 7: 'Department 7',\n",
       " 8: 'Department 8',\n",
       " 9: 'Department 9',\n",
       " 10: 'Department 10'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcastDeptNames.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create UDF to return Department name\n",
    "\n",
    "@udf\n",
    "def get_dept_name(dept_id):\n",
    "    return broadcastDeptNames.value.get(dept_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp.withColumn(\"deptName\", get_dept_name(col(\"department_id\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------------------+\n",
      "|department_id|CAST(sum(salary) AS BIGINT)|\n",
      "+-------------+---------------------------+\n",
      "|            6|                50294510721|\n",
      "+-------------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate total salary of Dept-6\n",
    "# Remember: The data is partitioned between all executors, hence any sort of \"aggregation\" would require collection and hence, shuffling.\n",
    "# The Accumulator helps avoid that.\n",
    "\n",
    "emp.where(\"department_id = 6\").groupBy(\"department_id\").agg(sum(\"salary\").cast(\"long\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_sal = spark.sparkContext.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use foreach (allows us to go through each record of that dataframe, row by row)\n",
    "\n",
    "def calculate_salary(dept_id, salary):\n",
    "    if dept_id == 6:\n",
    "        dept_salary.add(salary)\n",
    "\n",
    "emp.foreach(lambda row: calculate_salary(row.department_id, row.salary))    # Still not working\n",
    "dept_sal.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimising Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For joining 2 tables:\n",
    "1. Spark **reads** and distributes the data between its executors.\n",
    "2. Then **shuffles** the data to bring similar IDs into the same executor.\n",
    "3. Finally, performs **join** between the rows with similar IDs (or any other primary key).\n",
    "\n",
    "Join Strategies\n",
    "1. Shuffle Hash: Reliable when smaller dataset can fit in the memory.\n",
    "    - Shuffle the data.\n",
    "    - Hash the \"smaller\" dataset.\n",
    "    - Hashed dataset is matched with the \"bigger\" dataset.\n",
    "    - Join the datasets.\n",
    "\n",
    "2. Sort Merge: Useful when we need to join two BIG datasets\n",
    "    - Shuffle the datasets.\n",
    "    - Sort the primary keys\n",
    "    - Merge the datasets\n",
    "\n",
    "3. Broadcast Join: Most efficient for small dataset (default size of 10MB, can be incresed to 8G)\n",
    "    - Broadcast the smaller dataset to all executors.\n",
    "    - Join the datasets with the help of hashing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://b29f78abb2bc:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://eb22766c6b88:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Optimising Joins</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f9ebc05bb50>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Optimising Joins\")\n",
    "    .master(\"spark://eb22766c6b88:7077\")\n",
    "    .config(\"spark.cores.max\", 16)\n",
    "    .config(\"spark.executor.cores\", 4)\n",
    "    .config(\"spark.executor.memory\", \"512M\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable AQE and Broadcast Join\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", True)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join Big and Small table - SortMerge vs BroadCast Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "_schema = \"first_name string, last_name string, job_title string, dob string, email string, phone string, salary double, department_id int\"\n",
    "\n",
    "emp = spark.read.csv(\"/data/employee_records.csv\", header=True, schema=_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dept_schema = \"department_id int, department_name string, description string, city string, state string, country string\"\n",
    "\n",
    "dept = spark.read.csv(\"/data/department_data.csv\", header=True, schema=_dept_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined = emp.join(dept, on=emp.department_id==dept.department_id, how=\"left_outer\")\n",
    "df_joined.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(4) SortMergeJoin [department_id#135], [department_id#144], LeftOuter\n",
      ":- *(1) Sort [department_id#135 ASC NULLS FIRST], false, 0\n",
      ":  +- Exchange hashpartitioning(department_id#135, 200), ENSURE_REQUIREMENTS, [id=#123]\n",
      ":     +- FileScan csv [first_name#128,last_name#129,job_title#130,dob#131,email#132,phone#133,salary#134,department_id#135] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/data/employee_records.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<first_name:string,last_name:string,job_title:string,dob:string,email:string,phone:string,s...\n",
      "+- *(3) Sort [department_id#144 ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(department_id#144, 200), ENSURE_REQUIREMENTS, [id=#135]\n",
      "      +- *(2) Filter isnotnull(department_id#144)\n",
      "         +- FileScan csv [department_id#144,department_name#145,description#146,city#147,state#148,country#149] Batched: false, DataFilters: [isnotnull(department_id#144)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/data/department_data.csv], PartitionFilters: [], PushedFilters: [IsNotNull(department_id)], ReadSchema: struct<department_id:int,department_name:string,description:string,city:string,state:string,count...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join using broadcast. Saves the need to shuffle the data\n",
    "\n",
    "df_joined = emp.join(broadcast(dept), on=emp.department_id==dept.department_id, how=\"left_outer\")\n",
    "df_joined.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) BroadcastHashJoin [department_id#135], [department_id#144], LeftOuter, BuildRight, false\n",
      ":- FileScan csv [first_name#128,last_name#129,job_title#130,dob#131,email#132,phone#133,salary#134,department_id#135] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/data/employee_records.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<first_name:string,last_name:string,job_title:string,dob:string,email:string,phone:string,s...\n",
      "+- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#213]\n",
      "   +- *(1) Filter isnotnull(department_id#144)\n",
      "      +- FileScan csv [department_id#144,department_name#145,description#146,city#147,state#148,country#149] Batched: false, DataFilters: [isnotnull(department_id#144)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/data/department_data.csv], PartitionFilters: [], PushedFilters: [IsNotNull(department_id)], ReadSchema: struct<department_id:int,department_name:string,description:string,city:string,state:string,count...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join Big and Big tables - SortMerge without buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "_sales_schema = \"transacted_at string, trx_id string, retailer_id string, description string, amount double, city_id string\"\n",
    "\n",
    "sales = spark.read.csv(\"/data/new_sales.csv\", header=True, schema=_sales_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "_city_schema = \"city_id string, city string, state string, state_abv string, country string\"\n",
    "\n",
    "city = spark.read.csv(\"/data/cities.csv\", header=True, schema=_city_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_joined = sales.join(city, on=sales.city_id==city.city_id, how=\"left_outer\")\n",
    "sales_joined.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(4) SortMergeJoin [city_id#275], [city_id#282], LeftOuter\n",
      ":- *(1) Sort [city_id#275 ASC NULLS FIRST], false, 0\n",
      ":  +- Exchange hashpartitioning(city_id#275, 200), ENSURE_REQUIREMENTS, [id=#293]\n",
      ":     +- FileScan csv [transacted_at#270,trx_id#271,retailer_id#272,description#273,amount#274,city_id#275] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/data/new_sales.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<transacted_at:string,trx_id:string,retailer_id:string,description:string,amount:double,cit...\n",
      "+- *(3) Sort [city_id#282 ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(city_id#282, 200), ENSURE_REQUIREMENTS, [id=#305]\n",
      "      +- *(2) Filter isnotnull(city_id#282)\n",
      "         +- FileScan csv [city_id#282,city#283,state#284,state_abv#285,country#286] Batched: false, DataFilters: [isnotnull(city_id#282)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/data/cities.csv], PartitionFilters: [], PushedFilters: [IsNotNull(city_id)], ReadSchema: struct<city_id:string,city:string,state:string,state_abv:string,country:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_joined.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Big & Big join - with buckets\n",
    "\n",
    "- Bucketing utilises a hashing function to divide the data into a buckets. Based upon the joining-column, depositing the same \"ids\" in the same (separate set of) buckets for each dataset, and pass the same buckets to one exectuor.\n",
    "- No shuffling is required if the data is bucketed properly.\n",
    "- The number of buckets between the datasets should be the same.\n",
    "- Bucketing only works when we save the data as tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.write.format(\"csv\").mode(\"overwrite\").bucketBy(4, \"city_id\").option(\"header\", True).option(\"path\", \"/data/output/sales_bucket.csv\").saveAsTable(\"sales_bucket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "city.write.format(\"csv\").mode(\"overwrite\").bucketBy(4, \"city_id\").option(\"header\", True).option(\"path\", \"/data/output/city_bucket.csv\").saveAsTable(\"city_bucket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+-----------+\n",
      "|namespace|   tableName|isTemporary|\n",
      "+---------+------------+-----------+\n",
      "|  default| city_bucket|      false|\n",
      "|  default|sales_bucket|      false|\n",
      "+---------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables in default\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_bucket = spark.read.table(\"sales_bucket\")\n",
    "city_bucket = spark.read.table(\"city_bucket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets_joined = sales_bucket.join(city_bucket, on=sales_bucket.city_id==city_bucket.city_id, how=\"left_outer\")\n",
    "buckets_joined.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) SortMergeJoin [city_id#752], [city_id#759], LeftOuter\n",
      ":- *(1) Sort [city_id#752 ASC NULLS FIRST], false, 0\n",
      ":  +- FileScan csv default.sales_bucket[transacted_at#747,trx_id#748,retailer_id#749,description#750,amount#751,city_id#752] Batched: false, Bucketed: true, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/data/output/sales_bucket.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<transacted_at:string,trx_id:string,retailer_id:string,description:string,amount:double,cit..., SelectedBucketsCount: 4 out of 4\n",
      "+- *(2) Sort [city_id#759 ASC NULLS FIRST], false, 0\n",
      "   +- *(2) Filter isnotnull(city_id#759)\n",
      "      +- FileScan csv default.city_bucket[city_id#759,city#760,state#761,state_abv#762,country#763] Batched: false, Bucketed: true, DataFilters: [isnotnull(city_id#759)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/data/output/city_bucket.csv], PartitionFilters: [], PushedFilters: [IsNotNull(city_id)], ReadSchema: struct<city_id:string,city:string,state:string,state_abv:string,country:string>, SelectedBucketsCount: 4 out of 4\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "buckets_joined.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Points to note\n",
    "\n",
    "1. Joining Column different than Bucket Column, Same Bucket Size - Shuffle on Both table\n",
    "2. Joining Column Same, One table in Bucket - Shuffle on non Bucket table\n",
    "3. Joining Column Same, Different Bucket Size - Shuffle on Smaller Bucket Side\n",
    "4. Joining Column Same, Same Bucket Size - No Shuffle (Faster Join)\n",
    "\n",
    "***\n",
    "\n",
    "1. So its very importatant to choose correct Bucket column and Bucket Size\n",
    "2. Decide effectively on number of Buckets, as too mant buckets with not enough data can lead to Small file issue.\n",
    "3. Datasets are Small - you can prefer Shuffle Hash Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When compared to a similar functionality in DataBricks (\"scale up\"), pyspark adds & removes clusters within the designated worker nodes whereas DataBricks adds/removed the worker nodes themselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://b29f78abb2bc:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://eb22766c6b88:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Dynamic Allocation</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f056931f6d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Dynamic Allocation\")\n",
    "    .master(\"spark://eb22766c6b88:7077\")\n",
    "    .config(\"spark.executor.cores\", 2)\n",
    "    .config(\"spark.executor.memory\", \"512M\")\n",
    "    .config(\"spark.dynamicAllocation.enabled\", True)\n",
    "    .config(\"spark.dynamicAllocation.minExecutors\", 0)\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", 5)\n",
    "    .config(\"spark.dynamicAllocation.initialExecutors\", 1)\n",
    "    .config(\"spark.dynamicAllocation.shuffleTracking.enabled\", True)\n",
    "    .config(\"spark.dynamicAllocation.executorIdleTimeout\", \"60s\")\n",
    "    .config(\"spark.dynamicAllocation.cachedExecutorIdleTimeout\", \"60s\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_sales_schema = \"transacted_at string, trx_id string, retailer_id string, description string, amount double, city_id string\"\n",
    "\n",
    "sales = spark.read.csv(\"/data/new_sales.csv\", header=True, schema=_sales_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_city_schema = \"city_id string, city string, state string, state_abv string, country string\"\n",
    "\n",
    "city = spark.read.csv(\"/data/cities.csv\", header=True, schema=_city_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_joined = sales.join(city, on=sales.city_id==city.city_id, how=\"left_outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_joined.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skewness and Spillage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Memory spillage can happen if there exists skewed data in the dataset, ie, when the column used for joining has skew towards a particular value.\n",
    "- This can lead to more unserialized data being present in the memory than can be stored so the data needs be serialized and stored on the disk instead.\n",
    "- This increased the I/O overhead for the operations, leading to longer processing and execution time.\n",
    "- *Salting*: Technique to help repartition skewed data.\n",
    "    - \"Salting\" helps create new keys from exisiting ones by appending values to the joining key.\n",
    "    - This enables more equal distribution of data between the executors based upon the new keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://b29f78abb2bc:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://eb22766c6b88:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Skewness and Spillage</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f05685c7490>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Skewness and Spillage\")\n",
    "    .master(\"spark://eb22766c6b88:7077\")\n",
    "    .config(\"spark.cores.max\", 8)\n",
    "    .config(\"spark.executor.cores\", 4)\n",
    "    .config(\"spark.executor.memory\", \"512M\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disabling Spark features that help with Optimisation (Adaptive Query Engine)\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Employee data (Missing Skewed Dataset. Refer to the actual notebook & video)\n",
    "_schema = \"first_name string, last_name string, job_title string, dob string, email string, phone string, salary double, department_id int\"\n",
    "\n",
    "emp = spark.read.format(\"csv\").schema(_schema).option(\"header\", True).load(\"/data/employee_records.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read DEPT CSV data\n",
    "_dept_schema = \"department_id int, department_name string, description string, city string, state string, country string\"\n",
    "\n",
    "dept = spark.read.format(\"csv\").schema(_dept_schema).option(\"header\", True).load(\"/data/department_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined = emp.join(dept, on=emp.department_id==dept.department_id, how=\"left_outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+\n",
      "|partition_id|count(partition_id)|\n",
      "+------------+-------------------+\n",
      "|         103|             100417|\n",
      "|         122|              99780|\n",
      "|          43|              99451|\n",
      "|         107|              99805|\n",
      "|          49|              99706|\n",
      "|          51|             100248|\n",
      "|         102|             100214|\n",
      "|          66|             100210|\n",
      "|         174|             100155|\n",
      "|          89|             100014|\n",
      "+------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined.withColumn(\"partition_id\", spark_partition_id()).groupBy(\"partition_id\").agg(count(\"partition_id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Query Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://b29f78abb2bc:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://eb22766c6b88:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>AQE</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f054ea24c10>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"AQE\")\n",
    "    .master(\"spark://eb22766c6b88:7077\")\n",
    "    .config(\"spark.cores.max\", 8)\n",
    "    .config(\"spark.executor.cores\", 4)\n",
    "    .config(\"spark.executor.memory\", \"512M\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**POST-SHUFFLE**\n",
    "- *coalescePartitions*: Scales the Shuffle partitions to use only the necessary (and optimised) amount for a given dataset.\n",
    "- *skewed join optimizations*: Balances partition sizes, ie, join smaller partitions and split bigger partitions. Automatically avoids spillage.\n",
    "- *autoBroadcast*: Automatically converts SortMerge joins into Broadcast joins for smaller datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disabling Spark features that help with Optimisation (Adaptive Query Engine)\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False) # True\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False) # True\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1) # 10MB (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Catalog*: Stores the metadata of SQL objects. Catolog can be stored either in-memory (RAM) or hive (Disk).\n",
    "    - Use `.enableHiveSupport()` while creating SparkSession to persist the tables.\n",
    "- *Hints*: We can use hints in spark-sql queries to enforce a particular technique in the execution of task. (`/* <HINT(<TABLE>)> */`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://b29f78abb2bc:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark SQL</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f054dee2500>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Spark SQL\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.cores.max\", 8)\n",
    "    .config(\"spark.executor.cores\", 4)\n",
    "    .config(\"spark.executor.memory\", \"512M\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Employee data (Missing Skewed Dataset. Refer to the actual notebook & video)\n",
    "_schema = \"first_name string, last_name string, job_title string, dob string, email string, phone string, salary double, department_id int\"\n",
    "\n",
    "emp = spark.read.format(\"csv\").schema(_schema).option(\"header\", True).load(\"/data/employee_records.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read DEPT CSV data\n",
    "_dept_schema = \"department_id int, department_name string, description string, city string, state string, country string\"\n",
    "\n",
    "dept = spark.read.format(\"csv\").schema(_dept_schema).option(\"header\", True).load(\"/data/department_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in-memory'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.catalogImplementation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[namespace: string]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"show databases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "db = spark.sql(\"show databases\")\n",
    "db.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables in default\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp.createOrReplaceTempView(\"emp_temp\")\n",
    "dept.createOrReplaceTempView(\"dept_temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|         |dept_temp|       true|\n",
      "|         | emp_temp|       true|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables in default\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------------+----------+--------------------+--------------------+--------+-------------+\n",
      "|first_name| last_name|           job_title|       dob|               email|               phone|  salary|department_id|\n",
      "+----------+----------+--------------------+----------+--------------------+--------------------+--------+-------------+\n",
      "|   Richard|  Morrison|Public relations ...|1973-05-05|melissagarcia@exa...|       (699)525-4827|512653.0|            8|\n",
      "|     Bobby|  Mccarthy|   Barrister's clerk|1974-04-25|   llara@example.net|  (750)846-1602x7458|999836.0|            7|\n",
      "|    Dennis|    Norman|Land/geomatics su...|1990-06-24| jturner@example.net|    873.820.0518x825|131900.0|           10|\n",
      "|      John|    Monroe|        Retail buyer|1968-06-16|  erik33@example.net|    820-813-0557x624|485506.0|            1|\n",
      "|  Michelle|   Elliott|      Air cabin crew|1975-03-31|tiffanyjohnston@e...|       (705)900-5337|604738.0|            8|\n",
      "|    Ashley|   Montoya|        Cartographer|1976-01-16|patrickalexandra@...|        211.440.5466|483339.0|            6|\n",
      "| Nathaniel|     Smith|     Quality manager|1985-06-28|  lori44@example.net|        936-403-3179|419644.0|            7|\n",
      "|     Faith|  Cummings|Industrial/produc...|1978-07-01| ygordon@example.org|       (889)246-5588|205939.0|            7|\n",
      "|  Margaret|    Sutton|Administrator, ed...|1975-08-16| diana44@example.net|001-647-530-5036x...|671167.0|            8|\n",
      "|      Mary|    Sutton|   Freight forwarder|1979-12-28|  ryan36@example.com|   422.562.7254x3159|993829.0|            7|\n",
      "|      Jake|      King|       Lexicographer|1994-07-11|monica93@example.org|+1-535-652-9715x6...|702101.0|            4|\n",
      "|   Heather|     Haley|         Music tutor|1981-06-01|stephanie65@examp...|   (652)815-7973x298|570960.0|            6|\n",
      "|    Thomas|    Thomas|Chartered managem...|2001-07-17|pwilliams@example...|001-245-848-0028x...|339441.0|            6|\n",
      "|   Leonard|   Carlson|       Art therapist|1990-10-18|gabrielmurray@exa...|          9247590563|469728.0|            8|\n",
      "|      Mark|      Wood|   Market researcher|1963-10-13|nicholas76@exampl...|   311.439.1606x3342|582291.0|            4|\n",
      "|    Tracey|Washington|Travel agency man...|1986-05-07|  mark07@example.com|    001-912-206-6456|146456.0|            4|\n",
      "|   Rachael| Rodriguez|         Media buyer|1966-12-02|griffinmary@examp...| +1-791-344-7586x548|544732.0|            1|\n",
      "|      Tara|       Liu|   Financial adviser|1998-10-12|alexandraobrien@e...|        216.696.6061|399503.0|            3|\n",
      "|       Ana|    Joseph|      Retail manager|1995-01-10|  rmorse@example.org|  (726)363-7526x9965|761988.0|           10|\n",
      "|   Richard|      Hall|Engineer, civil (...|1967-03-02|brandoncardenas@e...| (964)451-9007x22496|660659.0|            4|\n",
      "+----------+----------+--------------------+----------+--------------------+--------------------+--------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select * from emp_temp\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+--------------------+----------+--------------------+--------------------+--------+-------------+\n",
      "| first_name|last_name|           job_title|       dob|               email|               phone|  salary|department_id|\n",
      "+-----------+---------+--------------------+----------+--------------------+--------------------+--------+-------------+\n",
      "|       John|   Monroe|        Retail buyer|1968-06-16|  erik33@example.net|    820-813-0557x624|485506.0|            1|\n",
      "|    Rachael|Rodriguez|         Media buyer|1966-12-02|griffinmary@examp...| +1-791-344-7586x548|544732.0|            1|\n",
      "|Christopher| Callahan| Exhibition designer|1966-10-23| qwalter@example.com|001-947-745-3939x...|251057.0|            1|\n",
      "|    Lindsey|   Huerta|Embryologist, cli...|1964-10-20|  psmith@example.net|   527.934.6665x1378|878257.0|            1|\n",
      "|      David|   Harris|   Company secretary|1990-04-13|     nli@example.com|001-959-766-1180x...|249553.0|            1|\n",
      "|      Brian|Hernandez|     Theatre manager|1978-05-05|  lisa93@example.com|        617-444-2378| 99894.0|            1|\n",
      "|      Peter|  Stevens|   Recycling officer|1983-06-10|jamesparker@examp...|   577-619-5082x8283|842980.0|            1|\n",
      "|       John|  Acevedo|Leisure centre ma...|1963-10-13|daniel21@example.org|  (247)878-5722x3342|385821.0|            1|\n",
      "|     Steven|      Lee|Adult guidance wo...|1969-06-01|wernergabriel@exa...|001-564-541-5402x...|336534.0|            1|\n",
      "|     Denise|     Dean|           Herbalist|1972-06-27|stephenwilliams@e...|        269.375.8072|656850.0|            1|\n",
      "|      Blake|       Wu| Marketing executive|1965-12-29|  gdoyle@example.net|+1-559-646-6295x3...|470459.0|            1|\n",
      "|      Billy|    Poole| Animal nutritionist|1969-05-31|maryhill@example.net|        713-637-6222|636619.0|            1|\n",
      "|       Eric|      Key|               Actor|1992-09-02|colemanshawn@exam...|   368.865.1746x2103|836623.0|            1|\n",
      "|     Amanda|     Ford|Health service ma...|1990-10-06|strongstephanie@e...|          5088050717|258843.0|            1|\n",
      "|      Jason|     Page|Historic building...|1964-02-22|woodsdebra@exampl...|001-327-358-1554x413| 76017.0|            1|\n",
      "|      Kelly|     Pugh|    Heritage manager|1972-07-17|paulmichelle@exam...|001-710-376-9474x...|332664.0|            1|\n",
      "|      Katie|    Rojas|    Paediatric nurse|1966-12-06|wilsoncurtis@exam...|+1-394-952-6247x7544|318915.0|            1|\n",
      "|    William|     Shaw|      Water engineer|1971-03-23|ngutierrez@exampl...|    674.916.1038x751|340283.0|            1|\n",
      "|      Linda|     Hill|Exercise physiolo...|1972-02-23|deckerdavid@examp...|   (931)974-8866x513|630520.0|            1|\n",
      "|  Christina| Martinez|Historic building...|1976-03-07| emily31@example.org|   277.954.3449x4345|383173.0|            1|\n",
      "+-----------+---------+--------------------+----------+--------------------+--------------------+--------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select * from emp_temp\n",
    "where department_id = 1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------------+----------+--------------------+--------------------+--------+-------------+--------+\n",
      "|first_name| last_name|           job_title|       dob|               email|               phone|  salary|department_id|dob_year|\n",
      "+----------+----------+--------------------+----------+--------------------+--------------------+--------+-------------+--------+\n",
      "|   Richard|  Morrison|Public relations ...|1973-05-05|melissagarcia@exa...|       (699)525-4827|512653.0|            8|    1973|\n",
      "|     Bobby|  Mccarthy|   Barrister's clerk|1974-04-25|   llara@example.net|  (750)846-1602x7458|999836.0|            7|    1974|\n",
      "|    Dennis|    Norman|Land/geomatics su...|1990-06-24| jturner@example.net|    873.820.0518x825|131900.0|           10|    1990|\n",
      "|      John|    Monroe|        Retail buyer|1968-06-16|  erik33@example.net|    820-813-0557x624|485506.0|            1|    1968|\n",
      "|  Michelle|   Elliott|      Air cabin crew|1975-03-31|tiffanyjohnston@e...|       (705)900-5337|604738.0|            8|    1975|\n",
      "|    Ashley|   Montoya|        Cartographer|1976-01-16|patrickalexandra@...|        211.440.5466|483339.0|            6|    1976|\n",
      "| Nathaniel|     Smith|     Quality manager|1985-06-28|  lori44@example.net|        936-403-3179|419644.0|            7|    1985|\n",
      "|     Faith|  Cummings|Industrial/produc...|1978-07-01| ygordon@example.org|       (889)246-5588|205939.0|            7|    1978|\n",
      "|  Margaret|    Sutton|Administrator, ed...|1975-08-16| diana44@example.net|001-647-530-5036x...|671167.0|            8|    1975|\n",
      "|      Mary|    Sutton|   Freight forwarder|1979-12-28|  ryan36@example.com|   422.562.7254x3159|993829.0|            7|    1979|\n",
      "|      Jake|      King|       Lexicographer|1994-07-11|monica93@example.org|+1-535-652-9715x6...|702101.0|            4|    1994|\n",
      "|   Heather|     Haley|         Music tutor|1981-06-01|stephanie65@examp...|   (652)815-7973x298|570960.0|            6|    1981|\n",
      "|    Thomas|    Thomas|Chartered managem...|2001-07-17|pwilliams@example...|001-245-848-0028x...|339441.0|            6|    2001|\n",
      "|   Leonard|   Carlson|       Art therapist|1990-10-18|gabrielmurray@exa...|          9247590563|469728.0|            8|    1990|\n",
      "|      Mark|      Wood|   Market researcher|1963-10-13|nicholas76@exampl...|   311.439.1606x3342|582291.0|            4|    1963|\n",
      "|    Tracey|Washington|Travel agency man...|1986-05-07|  mark07@example.com|    001-912-206-6456|146456.0|            4|    1986|\n",
      "|   Rachael| Rodriguez|         Media buyer|1966-12-02|griffinmary@examp...| +1-791-344-7586x548|544732.0|            1|    1966|\n",
      "|      Tara|       Liu|   Financial adviser|1998-10-12|alexandraobrien@e...|        216.696.6061|399503.0|            3|    1998|\n",
      "|       Ana|    Joseph|      Retail manager|1995-01-10|  rmorse@example.org|  (726)363-7526x9965|761988.0|           10|    1995|\n",
      "|   Richard|      Hall|Engineer, civil (...|1967-03-02|brandoncardenas@e...| (964)451-9007x22496|660659.0|            4|    1967|\n",
      "+----------+----------+--------------------+----------+--------------------+--------------------+--------+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select e.*, date_format(dob, 'yyyy') as dob_year from emp_temp e\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the data as table\n",
    "emp = spark.sql(\"\"\"\n",
    "select * from emp_temp\n",
    "where department_id = 1\n",
    "\"\"\")\n",
    "\n",
    "emp.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(\"emp_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|  default|emp_table|      false|\n",
      "|         |dept_temp|       true|\n",
      "|         | emp_temp|       true|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables in default\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------------+----------+--------------------+--------------------+--------+-------------+\n",
      "|first_name| last_name|           job_title|       dob|               email|               phone|  salary|department_id|\n",
      "+----------+----------+--------------------+----------+--------------------+--------------------+--------+-------------+\n",
      "|     Stacy|  Richmond|Chartered public ...|1963-06-22|  paul02@example.org|    705-270-1751x276|567073.0|            1|\n",
      "|   Zachary|    Torres|Recruitment consu...|1963-09-04|perkinslisa@examp...|        465.879.5063|180863.0|            1|\n",
      "|     Bryan|   Edwards|Engineer, biomedical|1983-03-05|   orush@example.com|    001-470-868-4085|815828.0|            1|\n",
      "|   Allison|   Hoffman|Therapist, hortic...|2001-01-04|mathewspatrick@ex...|   691-800-7757x7378|657399.0|            1|\n",
      "|    Travis|     Smith|Education officer...|1968-05-09|dawsonvictoria@ex...|    882-991-6093x704|464773.0|            1|\n",
      "|      Troy|     Weber|Geophysicist/fiel...|1965-02-26|lesliechen@exampl...|+1-957-581-9135x9089| 49902.0|            1|\n",
      "|    Joshua|  Benjamin|        Youth worker|1998-10-14|williamselizabeth...|    001-978-280-0230|640509.0|            1|\n",
      "|    Sierra|   Spencer|Chief Strategy Of...|1980-11-25|williamsjonathan@...|        300.647.4295|716954.0|            1|\n",
      "|     Jacob|   Swanson| Hospital pharmacist|1964-08-06|gloriarivera@exam...|       (378)584-2327|760968.0|            1|\n",
      "|     James|Washington|Insurance underwr...|1980-02-13| scott01@example.org|001-227-320-7643x...|137573.0|            1|\n",
      "|     Edwin| Rodriguez|Medical sales rep...|1995-03-13|traceynelson@exam...|        912.200.3307|860732.0|            1|\n",
      "|      Chad|      Luna|Adult guidance wo...|1971-02-14|tgutierrez@exampl...|       (820)620-4275|510931.0|            1|\n",
      "|     Shaun| Wilkinson|Conservator, muse...|1994-06-11|bennettconnie@exa...|        625-670-6607|581244.0|            1|\n",
      "|      Mark|     Jones|Optician, dispensing|1968-11-22|samantha48@exampl...|    396-680-0740x423|141882.0|            1|\n",
      "|    Robert|    Pierce|Corporate investm...|1968-09-26|  jack66@example.com|        493.378.5079|986052.0|            1|\n",
      "|   Kristin| Rodriguez|   Ceramics designer|1998-02-18|michael60@example...| +1-436-892-2857x097|860363.0|            1|\n",
      "|     Henry|   Johnson|  Personal assistant|1982-08-10|  anna62@example.net|  (790)435-6470x6997|263540.0|            1|\n",
      "|   Melissa|      Page|Publishing rights...|1971-01-06|arellanojessica@e...|        450.789.9702|535164.0|            1|\n",
      "|   Jessica|     Jones|  Petroleum engineer|1965-10-08|  john09@example.org|       (693)283-6920|227699.0|            1|\n",
      "|   Shannon|    Clarke|       Meteorologist|1983-01-06|brooke40@example.net|    001-870-599-5729|859033.0|            1|\n",
      "+----------+----------+--------------------+----------+--------------------+--------------------+--------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"emp_table\").show() # ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------------+----------+--------------------+--------------------+--------+-------------+\n",
      "|first_name| last_name|           job_title|       dob|               email|               phone|  salary|department_id|\n",
      "+----------+----------+--------------------+----------+--------------------+--------------------+--------+-------------+\n",
      "|     Stacy|  Richmond|Chartered public ...|1963-06-22|  paul02@example.org|    705-270-1751x276|567073.0|            1|\n",
      "|   Zachary|    Torres|Recruitment consu...|1963-09-04|perkinslisa@examp...|        465.879.5063|180863.0|            1|\n",
      "|     Bryan|   Edwards|Engineer, biomedical|1983-03-05|   orush@example.com|    001-470-868-4085|815828.0|            1|\n",
      "|   Allison|   Hoffman|Therapist, hortic...|2001-01-04|mathewspatrick@ex...|   691-800-7757x7378|657399.0|            1|\n",
      "|    Travis|     Smith|Education officer...|1968-05-09|dawsonvictoria@ex...|    882-991-6093x704|464773.0|            1|\n",
      "|      Troy|     Weber|Geophysicist/fiel...|1965-02-26|lesliechen@exampl...|+1-957-581-9135x9089| 49902.0|            1|\n",
      "|    Joshua|  Benjamin|        Youth worker|1998-10-14|williamselizabeth...|    001-978-280-0230|640509.0|            1|\n",
      "|    Sierra|   Spencer|Chief Strategy Of...|1980-11-25|williamsjonathan@...|        300.647.4295|716954.0|            1|\n",
      "|     Jacob|   Swanson| Hospital pharmacist|1964-08-06|gloriarivera@exam...|       (378)584-2327|760968.0|            1|\n",
      "|     James|Washington|Insurance underwr...|1980-02-13| scott01@example.org|001-227-320-7643x...|137573.0|            1|\n",
      "|     Edwin| Rodriguez|Medical sales rep...|1995-03-13|traceynelson@exam...|        912.200.3307|860732.0|            1|\n",
      "|      Chad|      Luna|Adult guidance wo...|1971-02-14|tgutierrez@exampl...|       (820)620-4275|510931.0|            1|\n",
      "|     Shaun| Wilkinson|Conservator, muse...|1994-06-11|bennettconnie@exa...|        625-670-6607|581244.0|            1|\n",
      "|      Mark|     Jones|Optician, dispensing|1968-11-22|samantha48@exampl...|    396-680-0740x423|141882.0|            1|\n",
      "|    Robert|    Pierce|Corporate investm...|1968-09-26|  jack66@example.com|        493.378.5079|986052.0|            1|\n",
      "|   Kristin| Rodriguez|   Ceramics designer|1998-02-18|michael60@example...| +1-436-892-2857x097|860363.0|            1|\n",
      "|     Henry|   Johnson|  Personal assistant|1982-08-10|  anna62@example.net|  (790)435-6470x6997|263540.0|            1|\n",
      "|   Melissa|      Page|Publishing rights...|1971-01-06|arellanojessica@e...|        450.789.9702|535164.0|            1|\n",
      "|   Jessica|     Jones|  Petroleum engineer|1965-10-08|  john09@example.org|       (693)283-6920|227699.0|            1|\n",
      "|   Shannon|    Clarke|       Meteorologist|1983-01-06|brooke40@example.net|    001-870-599-5729|859033.0|            1|\n",
      "+----------+----------+--------------------+----------+--------------------+--------------------+--------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from emp_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------------+----------+--------------------+--------------------+--------+-------------+\n",
      "|first_name| last_name|           job_title|       dob|               email|               phone|  salary|department_id|\n",
      "+----------+----------+--------------------+----------+--------------------+--------------------+--------+-------------+\n",
      "|   Richard|  Morrison|Public relations ...|1973-05-05|melissagarcia@exa...|       (699)525-4827|512653.0|            8|\n",
      "|     Bobby|  Mccarthy|   Barrister's clerk|1974-04-25|   llara@example.net|  (750)846-1602x7458|999836.0|            7|\n",
      "|    Dennis|    Norman|Land/geomatics su...|1990-06-24| jturner@example.net|    873.820.0518x825|131900.0|           10|\n",
      "|      John|    Monroe|        Retail buyer|1968-06-16|  erik33@example.net|    820-813-0557x624|485506.0|            1|\n",
      "|  Michelle|   Elliott|      Air cabin crew|1975-03-31|tiffanyjohnston@e...|       (705)900-5337|604738.0|            8|\n",
      "|    Ashley|   Montoya|        Cartographer|1976-01-16|patrickalexandra@...|        211.440.5466|483339.0|            6|\n",
      "| Nathaniel|     Smith|     Quality manager|1985-06-28|  lori44@example.net|        936-403-3179|419644.0|            7|\n",
      "|     Faith|  Cummings|Industrial/produc...|1978-07-01| ygordon@example.org|       (889)246-5588|205939.0|            7|\n",
      "|  Margaret|    Sutton|Administrator, ed...|1975-08-16| diana44@example.net|001-647-530-5036x...|671167.0|            8|\n",
      "|      Mary|    Sutton|   Freight forwarder|1979-12-28|  ryan36@example.com|   422.562.7254x3159|993829.0|            7|\n",
      "|      Jake|      King|       Lexicographer|1994-07-11|monica93@example.org|+1-535-652-9715x6...|702101.0|            4|\n",
      "|   Heather|     Haley|         Music tutor|1981-06-01|stephanie65@examp...|   (652)815-7973x298|570960.0|            6|\n",
      "|    Thomas|    Thomas|Chartered managem...|2001-07-17|pwilliams@example...|001-245-848-0028x...|339441.0|            6|\n",
      "|   Leonard|   Carlson|       Art therapist|1990-10-18|gabrielmurray@exa...|          9247590563|469728.0|            8|\n",
      "|      Mark|      Wood|   Market researcher|1963-10-13|nicholas76@exampl...|   311.439.1606x3342|582291.0|            4|\n",
      "|    Tracey|Washington|Travel agency man...|1986-05-07|  mark07@example.com|    001-912-206-6456|146456.0|            4|\n",
      "|   Rachael| Rodriguez|         Media buyer|1966-12-02|griffinmary@examp...| +1-791-344-7586x548|544732.0|            1|\n",
      "|      Tara|       Liu|   Financial adviser|1998-10-12|alexandraobrien@e...|        216.696.6061|399503.0|            3|\n",
      "|       Ana|    Joseph|      Retail manager|1995-01-10|  rmorse@example.org|  (726)363-7526x9965|761988.0|           10|\n",
      "|   Richard|      Hall|Engineer, civil (...|1967-03-02|brandoncardenas@e...| (964)451-9007x22496|660659.0|            4|\n",
      "+----------+----------+--------------------+----------+--------------------+--------------------+--------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from emp_temp\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+-------+\n",
      "|     col_name|data_type|comment|\n",
      "+-------------+---------+-------+\n",
      "|   first_name|   string|   null|\n",
      "|    last_name|   string|   null|\n",
      "|    job_title|   string|   null|\n",
      "|          dob|   string|   null|\n",
      "|        email|   string|   null|\n",
      "|        phone|   string|   null|\n",
      "|       salary|   double|   null|\n",
      "|department_id|      int|   null|\n",
      "+-------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show table metadata\n",
    "\n",
    "spark.sql(\"describe extended emp_temp\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online Implementations (Refer to Video)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
